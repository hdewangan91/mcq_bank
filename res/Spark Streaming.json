[{"q":"<p>In Spark Streaming, which of these statements about the following function are correct:</p>\n\n<p><strong>Function</strong></p>\n\n<pre class=\"prettyprint\"><code>groupByKeyAndWindow(windowDuration, slideDuration, [numTasks])</code></pre>\n\n<p><strong>Statement</strong></p>\n\n<ol>\n\t<li>When it is called on a DStream of (K, V) pairs, it returns a new DStream of (K, Seq[V]) pairs.</li>\n\t<li>The values for each key in the existing DStream pair are aggregated using a normal function.</li>\n\t<li>By default, it uses Spark's default number of parallel tasks to perform the grouping.</li>\n\t<li>You can pass only one numTasks argument to set a different number of tasks</li>\n</ol>\n\n<p> </p>","a":[{"id":1166913,"option":"1 and 2","correct":false},{"id":1166914,"option":"1 and 3","correct":true},{"id":1166915,"option":"1, 2, and 3","correct":false},{"id":1166916,"option":"2, 3, and 4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, you want to write data to an external system. You create a connection object in the Spark driver as displayed in the following code. You encounter an error. Which of these statements is a valid reason for this error:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>  dstream.foreachRDD(rdd =&gt; {\n      val connection = createNewConnection()  \n      rdd.foreach(record =&gt; {\n          connection.send(record) \n      })\n  })</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>The connection object must be parallelized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the worker to the driver. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>There is no error. The transferability of the connection object across machines is successful.</li>\n</ol>","a":[{"id":1167806,"option":"1","correct":false},{"id":1167807,"option":"2","correct":true},{"id":1167808,"option":"3","correct":false},{"id":1167809,"option":"4","correct":false}]},{"q":"<p>Consider that you are using the <strong>Azure Databricks</strong> interactive notebook to run the following code. If you want to send data to Slack to generate feedback from users through Slack, then which of these statements represents the task performed by the following code:</p>\n\n<p><strong>Code </strong></p>\n\n<pre class=\"prettyprint\"><code>val superSecretSlackToken = \"xoxb-...\" superSecretSlackToken: String = xoxb-... </code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>Defines the Slack Bot API token</li>\n\t<li>Defines the functions used by Slack</li>\n\t<li>Prepares the Kafka producer to send messages to Slack</li>\n\t<li>Initializes the Slack channel</li>\n</ol>","a":[{"id":1166933,"option":"1","correct":true},{"id":1166934,"option":"2","correct":false},{"id":1166935,"option":"3","correct":false},{"id":1166936,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, if you increase the number of topic-specific partitions in <strong>KafkaUtils.createStream()</strong>, then<strong> </strong>which of the following occurs:</p>\n\n<ol>\n\t<li>The number of threads using which the topics are consumed within multiple receivers increases.</li>\n\t<li>The number of threads using which the topics are consumed within a single receiver increases.</li>\n\t<li>The parallelism property of Spark does not increase while processing data.</li>\n\t<li>The parallelism property of Spark decreases significantly while processing data.</li>\n</ol>","a":[{"id":1166929,"option":"1 and 2","correct":false},{"id":1166930,"option":"2 and 3","correct":true},{"id":1166931,"option":"3 and 4","correct":false},{"id":1166932,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark Streaming, which of the following statements about transformations in <strong>DStream</strong> are correct:</p>\n\n<ol>\n\t<li>The stateless transformation includes common RDD transformations such as map(), filter(), reduceByKey(), etc.</li>\n\t<li>The stateful transformation uses the intermediate results from previous batches and then computes the result of the present batch.</li>\n\t<li>Stateless transformations can combine data from only one DStreams at an instance of time.</li>\n</ol>\n\n<p> </p>","a":[{"id":1166909,"option":"1 and 2","correct":false},{"id":1166910,"option":"2 and 3","correct":true},{"id":1166911,"option":"1 and 3","correct":false},{"id":1166912,"option":"All of these","correct":false}]},{"q":"<p>In Spark Streaming, the following new <strong>StreamingContext</strong> object is created from an existing <strong>SparkContext</strong> object and a context is defined. Which of the following statements about this scenario are correct:</p>\n\n<p><strong>Object</strong></p>\n\n<pre class=\"prettyprint\"><code>import org.apache.spark.streaming._\nval sc = ...                \nval ssc = new StreamingContext(sc, Seconds(5))</code></pre>\n\n<p><strong>Statements</strong> </p>\n\n<ol>\n\t<li>Once a Context is started, new streaming computations cannot be set up or added</li>\n\t<li>stop() on Context can only stop the SparkContext</li>\n\t<li>Multiple StreamingContext can be active in a JVM at the same time</li>\n\t<li>Once a Context is stopped, it cannot be restarted</li>\n</ol>","a":[{"id":1166905,"option":"1 and 2","correct":false},{"id":1166906,"option":"2 and 3","correct":false},{"id":1166907,"option":"3 and 4","correct":false},{"id":1166908,"option":"1 and 4","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these tasks is performed by the following artifact for Scala/Java applications using SBT/Maven project definitions:</p>\n\n<pre class=\"prettyprint\"><code>NameID = org.apache.spark \nitemId = spark-streaming-kafka-0-8_2.11 \nv = 2.2.0 </code></pre>\n\n<p><strong>Tasks</strong> </p>\n\n<ol>\n\t<li>It removes your Kafka streaming application.</li>\n\t<li>It links your Kafka streaming application.</li>\n\t<li>It starts your Kafka streaming application.</li>\n</ol>","a":[{"id":1166783,"option":"1","correct":false},{"id":1166784,"option":"2","correct":true},{"id":1166785,"option":"3","correct":false},{"id":1166786,"option":"None of these","correct":false}]},{"q":"<p>Which of the following statements about Spark Streaming are correct:</p>\n\n<ol>\n\t<li>It is an extension of the core Spark API.</li>\n\t<li>It enables high-throughput and fault-tolerant stream processing of live data streams.</li>\n</ol>","a":[{"id":1166719,"option":"1","correct":false},{"id":1166720,"option":"2","correct":false},{"id":1166721,"option":"All of these","correct":true},{"id":1166722,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming, you want to retrieve data from Kafka which is not present in the core API. Which of the following artifacts must be added to the dependencies to retrieve the required data in this scenario?</p>","a":[{"id":1166731,"option":"spark-streaming-flume_2.12","correct":false},{"id":1166732,"option":"spark-streaming-kinesis-asl_2.12","correct":false},{"id":1166733,"option":"spark-streaming-kafka-0-10_2.12","correct":true},{"id":1166734,"option":"spark-streaming-kafka-spark-streaming","correct":false}]},{"q":"<p>Which of the following statements represent the advantages of Spark Streaming:</p>\n\n<ol>\n\t<li>Quick recovery from failures</li>\n\t<li>Better load balancing</li>\n\t<li>Better resource usage</li>\n\t<li>Native integration with advanced processing libraries such as SQL, machine learning, and graph processing</li>\n</ol>","a":[{"id":1166727,"option":"1 and 2","correct":false},{"id":1166728,"option":"2, 3, and 4","correct":false},{"id":1166729,"option":"1, 2, and 3","correct":false},{"id":1166730,"option":"All of these","correct":true}]},{"q":"<p>You are given the following steps. Which of these sequences represent the internal working of Spark Streaming correctly:</p>\n\n<p><strong>Steps </strong></p>\n\n<ol>\n\t<li>Receives live input data streams</li>\n\t<li>Spark engine generates the final stream of results in batches</li>\n\t<li>Batches are then processed by the Spark engine</li>\n\t<li>Divides the data into batches</li>\n</ol>","a":[{"id":1166723,"option":"1=>3=>2=>4","correct":false},{"id":1166724,"option":"1=>3=>2=>4","correct":false},{"id":1166725,"option":"1=>4=>3=>2","correct":true},{"id":1166726,"option":"1=>2=>3=>4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following artifact is correct:</p>\n\n<p><strong>Artifact </strong></p>\n\n<pre class=\"prettyprint\"><code>ConsumerStrategies.Subscribe </code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>It allows you to subscribe to a fixed collection of topics.</li>\n\t<li>It allows you to use a regex to specify topics of interest.</li>\n\t<li>It allows you to distribute partitions evenly across available executors.</li>\n</ol>","a":[{"id":1167600,"option":"1","correct":true},{"id":1167601,"option":"2","correct":false},{"id":1167602,"option":"3","correct":false},{"id":1167603,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, what does the following code allow you to do:</p>\n\n<pre class=\"prettyprint\"><code>Ranges = []\n\n def storeRanges(rdd):\n     global Ranges\n     Ranges = rdd.Ranges()\n     return rdd\n\n def printRanges(rdd):\n     for o in Ranges:\n         print \"%s %s %s %s\" % (o.topic, o.partition, o.fromOffset, o.untilOffset)\n\n directKafkaStream \\\n     .transform(storeRanges) \\\n     .foreachRDD(printRanges)</code></pre>\n\n<p><strong>Tasks</strong></p>\n\n<ol>\n\t<li>Access the Kafka offsets consumed in each batch.</li>\n\t<li>If you want Zookeeper-based Kafka monitoring tools to show the progress of the streaming application then it updates Zookeeper.</li>\n\t<li>Create multiple-input Kafka streams and merge them.</li>\n\t<li>Increase the number of topic-specific partitions.</li>\n</ol>","a":[{"id":1167960,"option":"1 and 2","correct":true},{"id":1167961,"option":"2 and 3","correct":false},{"id":1167962,"option":"3 and 4","correct":false},{"id":1167963,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark streaming integrated with Kafka, which of the following artifacts is used to change this setting to handle more than (64 * number of executors) Kafka partitions?</p>","a":[{"id":1166925,"option":"spark.streaming.kafka.consumer.cache.maxCapacity","correct":true},{"id":1166926,"option":"spark-streaming-kafka-0-10_2.11","correct":false},{"id":1166927,"option":"LocationStrategies.PreferConsistent","correct":false},{"id":1166928,"option":"spark.streaming.kafka.consumer.cache.enabled","correct":false}]},{"q":"<p>You are required to set the following options if you want both batch and streaming queries for a Kafka source. Which statements about this scenario are correct:</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>assign</li>\n\t<li>subscribePattern</li>\n\t<li>subscribe</li>\n\t<li>kafka.bootstrap.servers</li>\n</ol>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>assign</strong>:<strong> </strong>Represents the specific TopicPartitions to consume</li>\n\t<li><strong>subscribePattern</strong>: Represents the topic list to subscribe</li>\n\t<li><strong>subscribe</strong>:<strong> </strong>Represents the list of subscribers </li>\n\t<li><strong>kafka.bootstrap.servers</strong>:<strong> </strong>Represents the pattern used to subscribe to topics</li>\n\t<li>All these options—assign, subscribe, and subscribePattern—can be specified for a Kafka source</li>\n\t<li>Only one of these options—assign, subscribe, or subscribePattern—can be specified for a Kafka source</li>\n</ol>","a":[{"id":1167987,"option":"1, 2, 4","correct":false},{"id":1167988,"option":"1, 4, 6","correct":true},{"id":1167989,"option":"3, 5, 6","correct":false},{"id":1167990,"option":"1, 3, 4, 6","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, your application uses the consumer group ID <strong>terran </strong>to read from the <strong>zerg.hydra </strong>Kafka topic, which contains 10 partitions. You have configured your application such that it can consume the topic with only one thread. However, during runtime, you increase the number of threads from 1 to 14.</p>\n\n<p>Which of the following statements about the consumption of the threads in this scenario is correct:</p>\n\n<ol>\n\t<li>All the 14 threads are consumed from a single partition—each consumed in two rebalancing events.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads stay idle.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads are consumed in the next rebalancing event.</li>\n\t<li>Once the rebalancing is complete, 7 of the 14 threads are consumed—each from a single partition—and the remaining 7 threads stay idle.</li>\n</ol>","a":[{"id":1167798,"option":"1","correct":false},{"id":1167799,"option":"2","correct":true},{"id":1167800,"option":"3","correct":false},{"id":1167801,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following code are correct:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>def function(): StreamingContext = {\n  val hack_ssc = new StreamingContext(...)   \n  val h_lines = hack_ssc.socketTextStream(...) \n  ...\n  hack_ssc.checkpoint(checkpointDirectory)   \n  hack_ssc\n}\nval hack_context = StreamingContext.getOrCreate(checkpointDirectory, function _)\nhack_context. ...\nhack_context.start()\nhack_context.awaitTermination()</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>If the program starts for the first time, the <strong>StreamingContext.getOrCreate()</strong> function allows you to create a new StreamingContext class, set up all the streams, and then call <strong>stop()</strong>.</li>\n\t<li>If the program restarts after failure, then the <strong>StreamingContext.getOrCreate()</strong> function allows you to replace a StreamingContext from the checkpoint data in the checkpoint directory.</li>\n\t<li>If the checkpointDirectory exists, then the context is replaced from the checkpoint data.</li>\n\t<li>If the checkpoint directory does not exist, that is, running for the first time, then the <strong>functionToCreateContext </strong>function is called to create a new context and disable the DStreams.</li>\n</ol>","a":[{"id":1167886,"option":"1, 2, 3","correct":false},{"id":1167887,"option":"2, 3, 4","correct":false},{"id":1167888,"option":"1, 3, 4","correct":false},{"id":1167889,"option":"None of these","correct":true}]},{"q":"<p>In Spark Streaming, the following new <strong>StreamingContext</strong> object is created from an existing <strong>SparkContext</strong> object. Which of these steps can be performed after a context is defined:</p>\n\n<p><strong>Object</strong></p>\n\n<pre class=\"prettyprint\"><code>import org.apache.spark.streaming._\nval sc = ...                \nval ssc = new StreamingContext(sc, Seconds(5))</code></pre>\n\n<p><strong>Steps </strong></p>\n\n<ol>\n\t<li>Defining the input sources by creating an input DStream</li>\n\t<li>Start receiving the data and then process it using streamingContext.start()</li>\n</ol>","a":[{"id":1166901,"option":"1","correct":false},{"id":1166902,"option":"2","correct":false},{"id":1166903,"option":"All of these","correct":true},{"id":1166904,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, how many input <strong>DStreams</strong> are created in the following code:</p>\n\n<pre class=\"prettyprint\"><code>val numInputDStreams = 5.5\nval kafkaDStreams = (2 to numInputDStreams).map { _ =&gt; KafkaUtils.createStream(...) }</code></pre>\n\n<p> </p>","a":[{"id":1166779,"option":"1","correct":false},{"id":1166780,"option":"3","correct":false},{"id":1166781,"option":"5","correct":false},{"id":1166782,"option":"None of these","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, RDD actions inside the DStream output operations force the processing of the data that is received. If you used the <strong>dstream.foreachRDD()</strong> artifact without any RDD actions inside them, then which of the following statements about the given scenario is correct:</p>\n\n<ol>\n\t<li>The system receives the data and discards it, and therefore, there is no execution.</li>\n\t<li>The system does not receive any data.</li>\n\t<li>The system receives and processes the data and the execution runs successfully.</li>\n\t<li>The system receives no data, and therefore, there is no execution.</li>\n</ol>","a":[{"id":1167810,"option":"1","correct":true},{"id":1167811,"option":"2","correct":false},{"id":1167812,"option":"3","correct":false},{"id":1167813,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements describe the task performed by the following code:</p>\n\n<pre class=\"prettyprint\"><code>createStream(StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map&lt;String,Object&gt; topics, StorageLevel storageLevel)</code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>createStream</strong> allows you to provide the input stream details, which include the systems where the port is created and the topic name.</li>\n\t<li>ssc represents the StreamingFile object.</li>\n\t<li>groupId represents the customer ID of this particular customer.</li>\n\t<li>Checkpoint operations are performed on a stream of data after it is received.</li>\n</ol>","a":[{"id":1167974,"option":"1 and 2","correct":false},{"id":1167975,"option":"2 and 3","correct":false},{"id":1167976,"option":"1 and 3","correct":true},{"id":1167977,"option":"All of these","correct":false}]},{"q":"<p>Which of the following classes is required to initialize a Spark Streaming program?</p>","a":[{"id":1166735,"option":"StreamingContext  ","correct":true},{"id":1166736,"option":"socketTextStream","correct":false},{"id":1166737,"option":" DStream","correct":false},{"id":1166738,"option":"None of these","correct":false}]},{"q":"<p>In Kafka, which of the following statements about the <strong>zookeeper.connect</strong> configuration are correct:</p>\n\n<ol>\n\t<li>It allows you to add a chroot path that makes all the Kafka data for this cluster appear under all the paths.</li>\n\t<li>It allows you to set up multiple Kafka clusters or other applications on the different ZooKeeper cluster at an instance of time. </li>\n\t<li>It specifies the ZooKeeper connection string in the port:hostname where hostname and port are the host and port for a node in the ZooKeeper clusterIn Kafka.</li>\n</ol>","a":[{"id":1166917,"option":"1","correct":false},{"id":1166918,"option":"2","correct":false},{"id":1166919,"option":"3","correct":false},{"id":1166920,"option":"None of these","correct":true}]}]