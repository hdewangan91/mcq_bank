[{"q":"<p>In data warehousing, you are given the following diagram that contains two figures, <strong>A</strong> and <strong>B</strong>.</p>\n\n<p>Which of these sets of statements regarding this diagram is correct:</p>\n\n<p><strong>Diagram</strong></p>\n\n<p> <img alt=\"\" height=\"183\" src=\"https://he-s3.s3.ap-southeast-1.amazonaws.com/media/uploads/66a96055-af71-47c5-9a88-b64ef70aed0f.png\" width=\"551\"></p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>Set 1</strong><br>\n\ti) Figure A represents a <em>nonidentifying</em><em> relationship </em>because the primary key of the child entity becomes a part of the primary key of the parent entity.<br>\n\tii) Figure B represents an <em>identifying relationship</em> because the primary key of the parent entity becomes a key attribute of the child entity.</li>\n\t<li><strong>Set 2</strong><br>\n\ti) Figure A represents an <em>identifying relationship </em>because the primary key of the parent entity becomes a part of the primary key of the child entity.<br>\n\tii) Figure B represents a <em>nonidentifying relationship</em> because the primary key of the parent entity becomes a non-key attribute of the child entity. </li>\n\t<li><strong>Set 3</strong><br>\n\ti) Figures A and B represent an <em>identifying relationship</em> because the primary key of the parent entity becomes a non-key attribute of the child entity.</li>\n\t<li><strong>Set 4</strong><br>\n\ti) Figures A and B represent a <em>nonidentifying relationship</em> because the primary key of the parent entity becomes a non-key attribute of the child entity.</li>\n</ol>","a":[{"id":1574166,"option":"Set 1","correct":false},{"id":1574167,"option":"Set 2","correct":true},{"id":1574168,"option":"Set 3","correct":false},{"id":1574169,"option":"Set 4","correct":false}]},{"q":"<p>You want to define the parts of summary management to use it, so you decide to check the schema first. You found that the dimensions should either be each in one table or we must normalize it among the tables, atleast partially, such that one parent is joined by every child row.<br>\nHow would you ensure that it happens?</p>","a":[{"id":1573262,"option":"By calling the VALIDATE_DIMENSION procedure of the DBMS_DIMENSION package","correct":false},{"id":1573263,"option":"By adding FOREIGN KEY and NOT NULL constraints on the child-side join keys and PRIMARY KEY constraints on the parent-side join keys","correct":true},{"id":1573264,"option":"By enabling the NOVALIDATE and RELY clauses","correct":false},{"id":1573265,"option":"By using Range/Partition on tables","correct":false}]},{"q":"<p>You want to use unique keys in a data warehouse such that the columns are not indexed in the data warehouse. The preceding ETL processing is unique, as many of the datawarehouses don’t run with queries. The environment is also using the 3NF schemas although the columns have some limitations.<br>\nWhat would you use in the Data Warehouse to perform the given operations?</p>","a":[{"id":1573294,"option":"B-Tree Indexes","correct":true},{"id":1573295,"option":"Index Compression","correct":false},{"id":1573296,"option":"Local Indexes","correct":false},{"id":1573297,"option":"Bitmap Join Index","correct":false}]},{"q":"<p>You are developing the application for a library. The database records the books issued by the library. But the only problem is that the library has merged data records of more than 2 years. You want to enable the user to get results based on the query based on the partitions.<br>\nWhat would you do to enhance the query search results in order to do the process faster?</p>","a":[{"id":1573290,"option":"Partition-wise joins","correct":false},{"id":1573291,"option":"Using Synchronized pruning","correct":false},{"id":1573292,"option":"Partitioning pruning","correct":true},{"id":1573293,"option":"Attribute Clustering","correct":false}]},{"q":"<p>You want to enhance manageability in Data Warehousing, hence, you decided to range the partitioning. The partitioning was done by dividing the ranges into equal lengths. However, you need to create the first partition manually such that further partitions get created automatically.<br>\nWhich of the following partitioning strategies would you use to perform the given operation?</p>","a":[{"id":1573286,"option":"Using Range","correct":false},{"id":1573287,"option":"Using Interval Partitioning","correct":true},{"id":1573288,"option":"Using Reference Partitioning","correct":false},{"id":1573289,"option":"Using Virtual column based partitioning","correct":false}]},{"q":"<p>You want the refresh for Materialized view to occur when a user executes a refresh procedure by itself, which is in the DBMS_MVIEW package.<br>\nWhich of the following refresh modes would you use to ensure that?</p>","a":[{"id":1573282,"option":"ON COMMIT","correct":false},{"id":1573283,"option":"ON DEMAND","correct":true},{"id":1573284,"option":"ON DELETE","correct":false},{"id":1573285,"option":"ON REFRESH","correct":false}]},{"q":"<p>You want the the highest materialistic view to be fresh with respect to the tables that contains the details in a Nested Materialized View. So, you kept a check that all the materialized views in the tree are refreshed in the order of dependency before the highest level is refreshed. What will happen if Nested Materialized View is not set to TRUE?</p>","a":[{"id":1573278,"option":"A complete refresh is performed even when a fast refresh is specified.","correct":false},{"id":1573279,"option":"All materialized views in a tree are refreshed.","correct":false},{"id":1573280,"option":"Materialized view is calculated relative to changes in the materialized view it directly references.","correct":false},{"id":1573281,"option":"Only refreshing the highest-level will succeed, where it refreshes only with respect to its underlying materialized view.","correct":true}]},{"q":"<p>You want to make the use of mediators and wrappers when a query is requested by a client with more than one heterogeneous databases.<br>\nWhich of the following approaches would you use for Data Warehousing?</p>","a":[{"id":1573274,"option":"Query-Driven","correct":true},{"id":1573275,"option":"Data-Driven","correct":false},{"id":1573276,"option":"Update-Driven","correct":false},{"id":1573277,"option":"Memory-Driven","correct":false}]},{"q":"<p>You want to redefine a table such that the data type of a column is changed from number to float along with adding attribute clustering and ensuring that the data is clustered while redefining it in Oracle Database Data Warehouse.<br>\nArrange the given steps in the correct order to perform the given operation:<br>\n1. Create a temporary table with the new table’s physical and logical attributes and define its schema.<br>\n2. Execute the DBMS_REDEFINITION.FINISH_REDEF_TABLE command.<br>\n3. Synchronize the temporary table optimally with the original table.<br>\n4. Execute the DBMS_REDEFINITON.START_REDEF_TABLE command.</p>","a":[{"id":1573270,"option":"1342","correct":false},{"id":1573271,"option":"1243","correct":false},{"id":1573272,"option":"2314","correct":false},{"id":1573273,"option":"1432","correct":true}]},{"q":"<p>You want to use a clustering technique with more than one dimension with the idea of Z-order curve fitting, such that multiple column attribute values can be mapped to a single one-dimensional value to preserve the multidimensional locality of column values. The tables can be both single and multiple. You need to ensure that the leading columns of the clusters is absent, and still the I/O pruning effects are achieved.<br>\nWhich of the following clustering techniques would you use to?</p>","a":[{"id":1573266,"option":"Attribute Clustering with Interleaved Ordering","correct":true},{"id":1573267,"option":"Attribute Clustering with Random Ordering","correct":false},{"id":1573268,"option":"Attribute Clustering with Linear Ordering","correct":false},{"id":1573269,"option":"Attribute Clustering with Linear Ordering with Joins","correct":false}]},{"q":"<p>Your database environment has large amount of ad hoc queries and data. However, the concurrent DML transaction is of low level.<br>\nYou want to achieve the following objectives in your data warehousing environment:<br>\n1. Large ad hoc queries(classes) response time must be significantly low.<br>\n2. Very less storage must be used relative to any other technique.<br>\n3. With even very less and efficient hardware capalibilities, the performance must be severely increased.<br>\n<br>\nWhat technique would you use in the Data warehouse?</p>","a":[{"id":1573258,"option":"Use cardinality index","correct":false},{"id":1573259,"option":"Use B-Tree Index","correct":false},{"id":1573260,"option":"Use 3NF Schemas","correct":false},{"id":1573261,"option":"Use Bitmap Indexes","correct":true}]},{"q":"<p>You use the DBMS_MVIEW.REFRESH procedure to refresh one or more materialized views. The required parameters to use this procedure has been mentioned in the options. Select the INCORRECT parameter statement.</p>","a":[{"id":1573254,"option":"The comma-delimitied list of materialized views to refresh","correct":false},{"id":1573255,"option":"Refresh after errors (TRUE or FALSE)","correct":false},{"id":1573256,"option":"Atomic refresh (TRUE or FALSE)","correct":false},{"id":1573257,"option":"The number of failures (this is an OUT variable)","correct":true}]},{"q":"<p>The transportation using transportable tablespaces is done by following 4 major steps which have been listed below. Select the correct order for these steps.<br>\n<br>\ni. Export the metadata<br>\nii. Copy the datafiles and export file to the target system<br>\niii. Place the data to be transported into its own tablespace<br>\niv. Import the metadata</p>","a":[{"id":1573250,"option":"i, iii, ii, iv","correct":false},{"id":1573251,"option":"iii, ii, i, iv","correct":true},{"id":1573252,"option":"ii, iii, iv, i","correct":false},{"id":1573253,"option":"iv, iii, ii, i","correct":false}]},{"q":"<p>You want to extract today's data from an orders table. Which of the following queries will you use?</p>","a":[{"id":1573246,"option":"SELECT * FROM orders \r\nWHERE TRUNC(CAST(order_date AS date),'dd') = \r\n TO_DATE(SYSDATE,'dd-mon-yyyy');","correct":true},{"id":1573247,"option":"SELECT * FROM orders \r\nWHERE TRUNC(CAST(order_date AS date),'dd') = \r\n TO_DATE('dd-mon-yyyy');","correct":false},{"id":1573248,"option":"SELECT * FROM orders \r\nWHERE TRUNC(order_date AS date,'dd') = \r\n TO_DATE(SYSDATE,'dd-mon-yyyy');","correct":false},{"id":1573249,"option":"SELECT * FROM orders \r\nWHERE TRUNC(order_date AS date,'dd') = \r\n TO_DATE(,'dd-mon-yyyy');\r\n\u0001\u0001\u0001\u0001","correct":false}]},{"q":"<p>To support Partition Change Tracking(PCT), a materialized view may not satisfy which of the following requirements?</p>","a":[{"id":1573242,"option":"At least one of the detail tables referenced by the materialized view must be partitioned.","correct":false},{"id":1573243,"option":"PCT is not supported for a materialized view that refers to views, remote tables, or outer joins.","correct":false},{"id":1573244,"option":"If you use a GROUP BY clause, the partition key column or the partition marker or ROWID or join dependent expression must be present in the GROUP BY clause.","correct":false},{"id":1573245,"option":"The COMPATIBILITY initialization parameter must be a maximum of 0.0.0.0.0.","correct":true}]},{"q":"<p>You have to decide whether to use fast refresh or not in your materialized view. Which of the following restrictions about fast refresh are correct?<br>\n<br>\n1) It cannot contain a SELECT list subquery.<br>\n2) It cannot contain nested queries that have ANY, ALL, or NOT EXISTS.<br>\n3) It cannot contain a [END WITH …] CONNECT BY clause.<br>\n4) On-commit materialized view cannot have remote detail tables.<br>\n5) The materialized view must not contain references to non-repeating expressions like SYSDATE and ROWNUM.</p>","a":[{"id":1573238,"option":"Only 1, 2, 3, and 4","correct":false},{"id":1573239,"option":"Only 3, 4, and 5","correct":false},{"id":1573240,"option":"Only 1 and 4","correct":false},{"id":1573241,"option":"Only 1, 2, 4, and 5","correct":true}]},{"q":"<p>All the Default physical attributes are initially specified when the CREATE INDEX statement creates a partitioned index. Since there is no segment corresponding to the partitioned index itself, these attributes are only used in derivation of physical attributes of member partitions. What will you use to modify the default physical attributes?</p>","a":[{"id":1573234,"option":"ALTER MODIFY INDEX DEFAULT ATTRIBUTES","correct":false},{"id":1573235,"option":"ALTER INDEX DEFAULT ATTRIBUTES","correct":false},{"id":1573236,"option":"ALTER INDEX MODIFY DEFAULT ATTRIBUTES","correct":true},{"id":1573237,"option":"ALTER DEFAULT INDEX ATTRIBUTES","correct":false}]},{"q":"<p>You want to implement partitioning in your data warehouse. Identify the correct statement about Partitioning from the options.</p>","a":[{"id":1573230,"option":"Very large tables are frequently scanned by a range predicate on a good partitioning column. Partitioning the table on that column will disable partition pruning.","correct":false},{"id":1573231,"option":"In hash partitioning, partition pruning uses only equality or IN-list predicates","correct":true},{"id":1573232,"option":"A partitioned table can have only partitioned and non-partitioned B-tree indexes.","correct":false},{"id":1573233,"option":"A non-partitioned table can have only partitioned indexes.","correct":false}]},{"q":"<p>When the transformation and computation is done outside the database and there is no need for flat file staging, what kind of loading should be preferred?</p>","a":[{"id":1573226,"option":"Loading a Data Warehouse with OCI and Direct-Path APIs","correct":true},{"id":1573227,"option":"Loading a Data Warehouse with Export/Import","correct":false},{"id":1573228,"option":"Loading a Data Warehouse with External Tables","correct":false},{"id":1573229,"option":"Loading a Data Warehouse with SQL*Loader","correct":false}]},{"q":"<p>Four parameters of the procedure of using the DBMS_DIMENSION.VALIDATE_DIMENSION for verifying the relationships to prevent incorrect results if the dimensions are incorrect, have been listed in the options. Choose the option which has incorrect parameter description.</p>","a":[{"id":1573222,"option":"dimension: the owner and name","correct":false},{"id":1573223,"option":"incremental: set to TRUE to check only the new rows for tables of this dimension","correct":false},{"id":1573224,"option":"check_nulls: set to TRUE to verify that all columns that are not in the levels containing a SKIP WHEN NULL clause are not null","correct":false},{"id":1573225,"option":"statement_id: a user-supplied unique identifier to check if input is correct","correct":true}]},{"q":"<p>What build function would you use to create the materialized view and then populate it with the data if the materialized view definition is added to the schema objects in the data dictionary, and then the fact or detail tables are scanned before the results are stored in the materialized view?</p>","a":[{"id":1573218,"option":"BUILD DEFERRED","correct":false},{"id":1573219,"option":"BUILD IMMEDIATE","correct":true},{"id":1573220,"option":"BUILD INFERRED","correct":false},{"id":1573221,"option":"BUILD CONSTANT","correct":false}]},{"q":"<p>The constraint must be in the ENABLE state which ensures that all data modifications upon a given table (or tables) satisfy the conditions of the constraints. Select the purpose that can be used for this.</p>","a":[{"id":1573214,"option":"Validation","correct":false},{"id":1573215,"option":"Belief","correct":false},{"id":1573216,"option":"Exception","correct":false},{"id":1573217,"option":"Enforcement","correct":true}]},{"q":"<p>X involves capturing the data on the source database by selecting the new and changed data from the source tables based on the value of a specific column. Identify X.</p>","a":[{"id":1573210,"option":"Table differencing","correct":false},{"id":1573211,"option":"Staging database","correct":false},{"id":1573212,"option":"Change-value selection","correct":true},{"id":1573213,"option":"Synchronous Change Data Capture","correct":false}]},{"q":"<p>A certain type of materialized view refresh operation uses Dimensions. But they are only mandatory if you use a GUI tool for materialized view and Index management. You have to use it without a workload to recommend which materialized views and indexes to create, drop or retain. Which GUI tool can be used for this?</p>","a":[{"id":1573206,"option":"SQL Access Advisor","correct":true},{"id":1573207,"option":"SQL Queue","correct":false},{"id":1573208,"option":"SQL Dimension Manager","correct":false},{"id":1573209,"option":"SQL GUI","correct":false}]},{"q":"<p>You are using bitmap indexes on a gender column, which has only two distinct values (male and female). Which of the following statements will be TRUE?</p>","a":[{"id":1573202,"option":"Degree of cardinality is small","correct":true},{"id":1573203,"option":"Degree of cardinality is large","correct":false},{"id":1573204,"option":"B-tree indexes should be used only for unique columns or other columns with very low cardinalities","correct":false},{"id":1573205,"option":"B-tree index on this column can outperform a bitmap index","correct":false}]},{"q":"<p>You want to develop data warehouses and dimensional data marts but you can not use Star schema for this. What alternative do you have considering that you are supposed to make more than one fact table referencing any number of dimension tables?</p>","a":[{"id":1573198,"option":"Snowflake schema","correct":false},{"id":1573199,"option":"Fact constellation","correct":true},{"id":1573200,"option":"Star-snowflake schema","correct":false},{"id":1573201,"option":"Star Constellation","correct":false}]},{"q":"<p>Consider the following fact table and choose the correct statement.</p>","a":[{"id":1573194,"option":"The fact table holds values for ‘price’ and ‘quantity’. These are called ‘measures’.","correct":true},{"id":1573195,"option":"The rest of the entries you see in this fact table are ‘foreign keys’ of dimensions that are independent on this fact table.","correct":false},{"id":1573196,"option":"All the dimensions associated with our fact tables are good, time, store, employee, Quantity.","correct":false},{"id":1573197,"option":"Not every dimension has a primary key.","correct":false}]},{"q":"<p>It is said, that professionals recommend designing your DynamoDB tables with a denormalized schema due to the following reason:<br>\nIn Amazon DynamoDB, when a table is created in the database, you don't need to define any attributes beforehand but only primary key attributes.<br>\n<br>\nWhat are those primary key attributes?</p>","a":[{"id":1573190,"option":"Sort key and Partition key","correct":true},{"id":1573191,"option":"Only Sort key","correct":false},{"id":1573192,"option":"Merge key and Partition key","correct":false},{"id":1573193,"option":"Only Merge key","correct":false}]},{"q":"<p>Let's have an example of a data warehouse where the users are from sales and marketing department. We can have security by top-to-down company view, with access centered on the different departments. If each department accesses different data, what should you do?</p>","a":[{"id":1573186,"option":"We should design the security access for each department separately.","correct":true},{"id":1573187,"option":"We should design a universal security access for all the departments.","correct":false},{"id":1573188,"option":"We can design the security access for one department and rest will be randomly defined.","correct":false},{"id":1573189,"option":"We should design the security access for the first few departments and then replicate it for others.","correct":false}]},{"q":"<p>The data is placed on the presentation server and the destination of the data can be atomic data mart or aggregated data mart, with each data target having its own details and syntax. What three strategies can you use to move forward?</p>","a":[{"id":1573182,"option":"Renewal, Withdrawal and Classical Update","correct":false},{"id":1573183,"option":"Pro-Logical, Gradual and DeepClean Update","correct":false},{"id":1573184,"option":"Slow, Gradual and Physical Update","correct":false},{"id":1573185,"option":"Renewal, Logical and Physical Update","correct":true}]},{"q":"<p>You have created the database and two tables (with five identical rows) and inserted the record. You have been asked to retrieve data from two tables, by returning all records from table1 and only the matched records from table2. You need to retrieve NULL values from table2 when there is no match between the two tables. What keyword would you use?</p>","a":[{"id":1573178,"option":"RIGHT JOIN T-SQL","correct":false},{"id":1573179,"option":"CENTRE JOIN T-SQL","correct":false},{"id":1573180,"option":"LEFT JOIN T-SQL","correct":true},{"id":1573181,"option":"SYS.COLUMNS","correct":false}]},{"q":"<p>You have a 3D Data Cube. You have been asked to perform the following operations:<br>\n<br>\ni) Disaggregate a dimension<br>\nii) Switch the view axis<br>\niii) Sort the members of a dimension according to some criteria<br>\niv) Restrict a value across a dimension<br>\n<br>\nWhat set of operations would you implement in OLAP(in the correct order)?</p>","a":[{"id":1573174,"option":"Drill-down, Roll-Up, Split, Nest","correct":false},{"id":1573175,"option":"Drill-across, Pivot, Split, Nest","correct":false},{"id":1573176,"option":"Drill-down, Pivot, Rank, Slice","correct":true},{"id":1573177,"option":"Drill-across, Push, Rank, Slice","correct":false}]},{"q":"<p>Identify X and Y from the syntax definitions given below.<br>\n<br>\ndefine X &lt; X_name &gt; [ &lt; dimension-list &gt; }: &lt; measure_list &gt;<br>\ndefine Y&lt; Y_name &gt; as ( &lt; attribute_or_dimension_list &gt; )</p>","a":[{"id":1573170,"option":"X=Dimension and Y=Cube","correct":false},{"id":1573171,"option":"X=Cube and Y=Dimension","correct":true},{"id":1573172,"option":"X=Star and Y=SnowFlake Schema","correct":false},{"id":1573173,"option":"X=Dimension and Y=Star","correct":false}]},{"q":"<p>Read the following steps and identify the type of merge process:<br>\n<br>\nFirst, merging the dictionaries of the differential buffer and main storage is performed.<br>\nThen, the value ids of main storage and write buffer are copied to a new main storage, so the changes need to applied to the new value ids.<br>\nInavalidated values of the original main storage are not copied and can be transferred to a history log.</p>","a":[{"id":1573166,"option":"Attribute merge","correct":true},{"id":1573167,"option":"Prepare merge","correct":false},{"id":1573168,"option":"Commit merge","correct":false},{"id":1573169,"option":"Hyrise merge","correct":false}]},{"q":"<p>The solutions of X most of the time manage relatively limited schemas with large cardinality in few entities, while data warehouses typically have lots of facts and dimensions (in a dimensional model) or lots of entities in a 3NF mode. Identify X.</p>","a":[{"id":1573162,"option":"NoSQL","correct":true},{"id":1573163,"option":"OLAP","correct":false},{"id":1573164,"option":"Database","correct":false},{"id":1573165,"option":"ROLAP","correct":false}]},{"q":"<p>Fill in the blank with the correct option.<br>\n<br>\nWhenever you try to insert data into the table, the code runs to check for enough space. If space is insufficient, then more space may have to be allotted to the tables. This very common approach is to insert data using _______.</p>","a":[{"id":1573158,"option":"SQL Layer","correct":true},{"id":1573159,"option":"Tape Silos","correct":false},{"id":1573160,"option":"Tape Stackers","correct":false},{"id":1573161,"option":"Tape Layers","correct":false}]},{"q":"<p>You want the facilities of optical work mapping, communication to every information source, and elastic consumption alternatives. What ETL tool would you use?</p>","a":[{"id":1573154,"option":"Apartar","correct":true},{"id":1573155,"option":"Jaspersoft ETL","correct":false},{"id":1573156,"option":"Talend ETL Open Source Tool","correct":false},{"id":1573157,"option":"Jedox ETL Open Source Tool","correct":false}]},{"q":"<p>For building a Data Warehouse, you take an approach where the highly summarized data is in one server, intermediate-level summarized data is in the second layer, and more detailed data is in the third server. It also has a faster response time. Identify the type of architecture.</p>","a":[{"id":1573150,"option":"Centralized architecture","correct":true},{"id":1573151,"option":"Federation architecture","correct":false},{"id":1573152,"option":"Layered architecture","correct":false},{"id":1573153,"option":"Single-tier architecture","correct":false}]},{"q":"<p>What kind of schema would you recommend, when queries are quite complex, number of joins are higher and dimension table is relatively big in size?</p>","a":[{"id":1573146,"option":"SnowFlake schema","correct":true},{"id":1573147,"option":"Star schema","correct":false},{"id":1573148,"option":"Constellation schema","correct":false},{"id":1573149,"option":"SQL schema","correct":false}]},{"q":"<p>You have a subset of organization-wide data that is valuable to specific people in the organization under a data mart. What will be the time required for measuring the implementation cycle of the data mart?</p>","a":[{"id":1573142,"option":"1 month","correct":false},{"id":1573143,"option":"6 months","correct":false},{"id":1573144,"option":"A year","correct":false},{"id":1573145,"option":"2 weeks","correct":true}]},{"q":"<p>A stage in the Data Warehouse Deployment lifecycle has the below mentioned characteristics:<br>\n<br>\nI) Defining the transformation rules<br>\nII) Performing data profiling<br>\nIII) Designing the ETL<br>\n<br>\nIdentify the stage.</p>","a":[{"id":1573138,"option":"Warehouse Schema Design","correct":false},{"id":1573139,"option":"OLTP to data warehouse mapping","correct":true},{"id":1573140,"option":"Deployment","correct":false},{"id":1573141,"option":"Enhancement","correct":false}]},{"q":"<p>DataWarehousing testing involves a huge volume of Data. You have been given some resources that are presumed to be the required resources for Data Warehouse testing. Later you are informed that one of them is not necessary and can be dropped considering the budget. Which among the following resources is most likely to get dropped?</p>","a":[{"id":1573134,"option":"Business Analysts who create requirements","correct":false},{"id":1573135,"option":"DBAs who test for performance stress","correct":false},{"id":1573136,"option":"Testers to solve conflict and redundancy","correct":true},{"id":1573137,"option":"Architects who organize test environments","correct":false}]},{"q":"<p>You observe that:<br>\n1) The activity appears in the immediately near areas.<br>\n2) Scope is finite.<br>\n3) Any intersection of data between them is circumstantial.<br>\n<br>\nIdentify the correct term for the situation described above.</p>","a":[{"id":1573130,"option":"Virtual DataWarehousing","correct":false},{"id":1573131,"option":"Local DataWarehousing","correct":true},{"id":1573132,"option":"Stationary DataWarehousing","correct":false},{"id":1573133,"option":"MultiStage DataWarehousing","correct":false}]},{"q":"<p>You are working on an unknown architecture and want to deduce it from its components. You see a central server and a database server. There is a front-end tool which sends information requests to the central server. The result after Metadata request process is sent back to the front-end tool. Identify the type of architechture.</p>","a":[{"id":1573126,"option":"ROLAP","correct":true},{"id":1573127,"option":"Multidimensional OLAP","correct":false},{"id":1573128,"option":"OLAP","correct":false},{"id":1573129,"option":"Schemas","correct":false}]},{"q":"<p>You have installed a system to get the feed of real time data as it flows through different databases within your company's environment which are shown in the form 'Scorecard'. Which of the following describes this the best?</p>","a":[{"id":1573122,"option":"Data visualization","correct":false},{"id":1573123,"option":"Dashboard","correct":true},{"id":1573124,"option":"Anomaly detection","correct":false},{"id":1573125,"option":"Data analysis","correct":false}]},{"q":"<p>You have to choose between ETL and ELT while performing datawarehousing. Which of the following is a shortcoming of ETL when compared with ELT on the same parameter?</p>","a":[{"id":1573118,"option":"Development Time","correct":false},{"id":1573119,"option":"Targetting of data","correct":false},{"id":1573120,"option":"Availability of tools","correct":false},{"id":1573121,"option":"Flexibility","correct":true}]},{"q":"<p>The sample database schemas provide a common platform for examples in each release of the Oracle Database. The sample schemas are a set of interlinked database schemas. You need to choose the sample schemas from the following list which are both originally defined and correctly defined.<br>\n<br>\n1. Schema Human Resources (HR) is useful for introducing basic topics.<br>\n2. Schema Order Entry (OE) is useful for dealing with matters of intermediate complexity. Many data types are available in this schema, including nonscalar data types.<br>\n3. Schema Product Media (PM) is dedicated to print the variables.<br>\n4. Schema Order check(COC) deals with inspection related to the data provided.<br>\n5. Schema Customer Orders (CO) is a modern schema useful for demos of e-commerce transactions. It allows the storage of semi-structured data using JSON.</p>","a":[{"id":1573114,"option":"Only 1, 3, and 5","correct":true},{"id":1573115,"option":"Only 1, 2, and 4","correct":false},{"id":1573116,"option":"Only 2, 4, and 5","correct":false},{"id":1573117,"option":"Only 1, 2, and 5","correct":false}]},{"q":"<p>You are studying the ETL(Extract, Transform, Load) process. If you are given the final data and you notice the poor primary data quality, then what possible logical conclusion can you draw?</p>","a":[{"id":1573110,"option":"The process of extraction failed","correct":true},{"id":1573111,"option":"The cleansing stage is skipped","correct":false},{"id":1573112,"option":"The source systems might be complicated and poorly documented","correct":false},{"id":1573113,"option":"Loading process was not performed properly","correct":false}]},{"q":"<p>You have infrequently accessed aged data to post-partition. The traits of the set are :<br>\n1) The detailed information remains available online.<br>\n2) The number of Physical tables are small to reduce operating cost.<br>\n3) The technique is suitable where a mix of data dipping recent history.<br>\nIdentify the type of partition used in this case.</p>","a":[{"id":1573106,"option":"Partition by time into equal segments","correct":false},{"id":1573107,"option":"Partition by time with unequal sized segments","correct":true},{"id":1573108,"option":"Partition by Size of Table","correct":false},{"id":1573109,"option":"Partition on a different dimension","correct":false}]},{"q":"<p>Assume a base of 70,000 customers in 300 cities, 29 states and 5 regions. Up till the city-level, multidimensional storage would resolve queries to raise sales totals. However, if it were necessary to query a customer's total sales, the relational database would respond much faster to the request. This situation is typical for indicating the HOLAP architecture. Which of the following is NOT TRUE here considering HOLAP architecture?</p>","a":[{"id":1573102,"option":"Dimensional cubes only store information synthesis.","correct":false},{"id":1573103,"option":"The details of the information are stored in an operational database.","correct":false},{"id":1573104,"option":"This model presents the highest acquisition and maintenance costs.","correct":false},{"id":1573105,"option":"It can combine the capabilities and scalability of ROLAP tools with the superior performance of multidimensional databases.","correct":true}]}]