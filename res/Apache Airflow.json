[{"q":"<p>If the Apache Airflow framework returns a value from the PythonOperator’s <strong>python_callable</strong> function, then which of the following statements is correct?</p>","a":[{"id":787586,"option":"An XCom containing that value is automatically pushed.","correct":true},{"id":787587,"option":"The value returned by the task is automatically pulled by all the existing XComs.","correct":false},{"id":787588,"option":"The DAG generates a separate task to pull the value returned via a XCom.","correct":false},{"id":787589,"option":"None of these","correct":false}]},{"q":"<p>If you are integrating the Kerberos protocol with the Apache Airflow framework, then which of the following limitations is valid</p>\n\n<ol>\n\t<li>All hooks that are available in Airflow cannot be used in such an implementation.</li>\n\t<li>You must rely on network-level security since the web interface does not integrate with Kerberos.</li>\n</ol>","a":[{"id":787582,"option":"1","correct":false},{"id":787583,"option":"2","correct":false},{"id":787584,"option":"Both of these","correct":true},{"id":787585,"option":"None of these","correct":false}]},{"q":"<p>You are using the <strong>Active Directory</strong> while working with Apache Airflow. However, you have not explicitly specified an OU that your users are in. Which of the following search scopes you must set for the implementation of an <strong>LDAP</strong> authentication?</p>","a":[{"id":787578,"option":"TREE","correct":false},{"id":787579,"option":"SUBTREE","correct":true},{"id":787580,"option":"ROOT","correct":false},{"id":787581,"option":"None of these","correct":false}]},{"q":"<p>You are required to specify the <strong>authMechanism</strong> technique for the following Apache Airflow module:</p>\n\n<pre class=\"prettyprint\"><code>airflow.hooks.base_hook.BaseHook</code></pre>\n\n<p>Which of these parts of the connection allows you to specify the additional configuration that performs the same operation?</p>","a":[{"id":787574,"option":"Auth","correct":false},{"id":787575,"option":"Extra","correct":true},{"id":787576,"option":"Additional","correct":false},{"id":787577,"option":"Route","correct":false}]},{"q":"<p>In Apache Airflow, which of these will you use to override the following parameter that you have passed while working with Apache Airflow:</p>\n\n<pre class=\"prettyprint\"><code>{\"hive_cli_params\": \"-hiveconf mapred.job.tracker=some.jobtracker:444\"}</code></pre>\n\n<p> </p>","a":[{"id":787566,"option":"hive_conf in run_cli","correct":true},{"id":787567,"option":"beeline","correct":false},{"id":787568,"option":"auth","correct":false},{"id":787569,"option":"_prepare_cli","correct":false}]},{"q":"<p>In Apache Airflow, which of the following can be used to keep the record of SLA failures and avoid double triggering of alert emails?</p>","a":[{"id":787558,"option":"airflow.models.base","correct":true},{"id":787559,"option":"airflow.models.KubeWorkerIdentifier","correct":false},{"id":787560,"option":"airflow.models.KubeResourceVersion","correct":false},{"id":787561,"option":"airflow.models.vector","correct":false}]},{"q":"<p>Which of these statements about the implementation of the following object in the Apache Airflow platform is correct:</p>\n\n<pre class=\"prettyprint\"><code>airflow.models.DagPickle(dag)[source]</code></pre>\n\n<p> </p>","a":[{"id":787554,"option":"This object represents a version of a DAG","correct":false},{"id":787555,"option":"This object becomes a source of truth for a BackfillJob execution","correct":false},{"id":787556,"option":"Both of these","correct":true},{"id":787557,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, you called the Bash script by using the <strong>bash_command</strong> argument (<strong>bash_command=\"/home/batcher/test.sh</strong>) while using the <strong>BashOperator</strong> operator in the Bash shell. This execution failed and the '<strong>Jinja template not found</strong>' exception got raised. Which of the following techniques must be used to resolve this issue?</p>\n\n<p> </p>","a":[{"id":787546,"option":"Modify the directory (bash_command=\"/home/script_file/test.sh \"","correct":false},{"id":787547,"option":"Modify the directory (bash_command=\"/etc/script_file/test.sh \"","correct":false},{"id":787548,"option":"Add a space after the script name (bash_command=\"/home/batcher/test.sh \"","correct":true},{"id":787549,"option":"Modify the directory by adding the script_file folder (bash_command=\"/home/script_file/script_file/test.sh \"","correct":false}]},{"q":"<p>In Apache Airflow, which of the following sequences of steps is used to add module dependencies to your DAG?</p>","a":[{"id":787542,"option":"1. mkdir zip_dag_contents                   cd zip_dag_contents                     \r\n2. pip install --install-option=\"--install-lib=$PWD\" my_useful_package               cp ~/my_dag.py .           \r\n3. zip -r zip_dag.zip *","correct":false},{"id":787543,"option":"1. virtualenv zip_dag                source zip_dag/bin/activate                          \r\n2. mkdir zip_dag_contents                   cd zip_dag_contents                     \r\n3. pip install --install-option=\"--install-lib=$PWD\" my_useful_package               cp ~/my_dag.py .           \r\n4. zip -r zip_dag.zip *","correct":true},{"id":787544,"option":"1. mkdir zip_dag_contents                   cd zip_dag_contents                    \r\n2. zip -r zip_dag.zip *","correct":false},{"id":787545,"option":"1. virtualenv zip_dag                source zip_dag/bin/deactivate                         \r\n2. mkdir zip_dag_contents                   cd zip_dag_contents                     \r\n3. pip install --install-option=\"--install-lib=$PWD\" my_useful_package .           \r\n4. zip -r zip_dag.zip *","correct":false}]},{"q":"<p>You have terminated an Apache Airflow process externally but its status is still running the database. Which of the following techniques allows you to terminate this process permanently?</p>","a":[{"id":787534,"option":"By deleting rows in the task instances","correct":false},{"id":787535,"option":"By verifying the state of the process as part of the heartbeat routine","correct":false},{"id":787536,"option":"Process  of termination is performed periodically by the scheduler’s process","correct":true},{"id":787537,"option":"By changing its end_time schedule to 'now'","correct":false}]},{"q":"<p>In Apache Airflow, the start date of your DAG scheduler is set to '<strong>2018-04-21</strong>' and you turn on the scheduling toggle at '<strong>2018-04-22T00:00:00</strong>'. Your DAG is configured to run on an hourly basis. Which of the following conditions can occur in this scenario?</p>","a":[{"id":787530,"option":"Scheduler backfills 24 DAG runs and then start running on the scheduled interval","correct":true},{"id":787531,"option":"Scheduler instantly set the start_date parameter at '2018-04-22T00:00:00' and continue previous running on scheduled interval","correct":false},{"id":787532,"option":"Scheduler continues to run on the scheduled interval","correct":false},{"id":787533,"option":"Scheduler backfills one DAG run and then start running on the scheduled interval","correct":false}]},{"q":"<p>In Apache Airflow, you have used a CLI '<strong>airflow clear -h</strong>' command for clearing a task instance. What is the function of this instance:</p>\n\n<ol>\n\t<li>Update the max_tries property </li>\n\t<li>Delete the task instance record</li>\n\t<li>Set the current task instance state to be <strong>None</strong></li>\n\t<li>Remove the task instance that is marked as <strong>Success</strong></li>\n\t<li>Remove the task instance that is marked as <strong>Failed</strong></li>\n</ol>","a":[{"id":787159,"option":"2","correct":false},{"id":787160,"option":"1 and 3","correct":true},{"id":787161,"option":"2, 3, and 5","correct":false},{"id":787162,"option":"1, 3, and 5","correct":false}]},{"q":"<p>In Apache Airflow, your DAG is written in such a way that it can handle its own catchup. Your task is to turn off the catchup on the DAG itself. Which of the configurations will you use to complete this task?</p>","a":[{"id":787155,"option":"dag_default_catchup = False","correct":false},{"id":787156,"option":"default_dag_catchup = False","correct":false},{"id":787157,"option":"catchup_by_default = False","correct":false},{"id":787158,"option":"dag.catch= False","correct":true}]},{"q":"<p>In Apache Airflow, which of the following arguments will you use to stop the scheduling process of your DAG?</p>","a":[{"id":787151,"option":"schedule_interval='None'","correct":false},{"id":787152,"option":"dag_schedule='None'","correct":false},{"id":787153,"option":"schedule_interval=None","correct":true},{"id":787154,"option":"dag_schedule=None","correct":false}]},{"q":"<p>In Apache Airflow, if processes are integrated with the <strong>upstart</strong> file, then which of the following will you use to manually start a process, stop a process, and view the status of a process?</p>","a":[{"id":787143,"option":"airflow airflow-webserver status","correct":false},{"id":787144,"option":"initctl airflow-webserver status","correct":true},{"id":787145,"option":"airflow airflow-process status","correct":false},{"id":787146,"option":"airflow airflow-webprocess status","correct":false}]},{"q":"<p>In Apache Airflow, which of the following techniques is used to automatically start the system boot by using the <strong>upstart</strong> file?</p>","a":[{"id":787139,"option":"A corresponding *.conf file in /etc/conf","correct":false},{"id":787140,"option":"Define configuration at AIRFLOW_CONFIG","correct":false},{"id":787141,"option":"Define configuration at AIRFLOW_HOME","correct":false},{"id":787142,"option":"A corresponding *.conf file in /etc/init","correct":true}]},{"q":"<p>In Apache Airflow, if you set the value of <strong>depends_on_set</strong> to <strong>true</strong> while branching, then which of the following conditions can occur?</p>","a":[{"id":787135,"option":"Only one path at each DagRun is available","correct":false},{"id":787136,"option":"DAG gets completely locked","correct":true},{"id":787137,"option":"Apache Airflow crashes","correct":false},{"id":787138,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, you are given two XComs that contain similar keys. Which of the following conditions will you allow you to determine which XCom gets retrieved first?</p>","a":[{"id":787131,"option":"The XCOM having the most recent execution_date gets retrieved first by default. ","correct":true},{"id":787132,"option":"The XCOM having the least execution_date gets retrieved first by default. ","correct":false},{"id":787133,"option":"You cannot have two XCOMs with the same keys. ","correct":false},{"id":787134,"option":"It cannot be predicted. ","correct":false}]},{"q":"<p>Which of the following is the correct location to store your plugins so that they can be used by Apache Airflow?</p>","a":[{"id":787127,"option":"In the $AIRFLOW_HOME/plugins folder ","correct":true},{"id":787128,"option":"In the $AlRFLOW_HOME/addons folder","correct":false},{"id":787129,"option":"In the $AlRFLOW_HOME/config folder","correct":false},{"id":787130,"option":"In the $AIRFLOW_HOME/Setup folder ","correct":false}]},{"q":"<p>In Apache Airflow, if you add a new container to an existing multi-container application, then what is the purpose of the <strong>--network</strong> option in the <strong>docker-compose</strong> container?</p>","a":[{"id":787123,"option":"The --network option allows the new container to create a new network ","correct":false},{"id":787124,"option":"The --network option allows the new container to be connected with the host network ","correct":false},{"id":787125,"option":"The --network option allows the new container to be connected with an existing network","correct":true},{"id":787126,"option":"The --network option can be used to create an isolated container ","correct":false}]},{"q":"<p>In Apache Airflow, which of the following statements is the description of the <strong>sensor</strong> operator?</p>","a":[{"id":787119,"option":"It is a long running task that is waiting for no operation to perform. ","correct":false},{"id":787120,"option":"It is a task that allows to transfer data from a source system to a destination system.","correct":false},{"id":787121,"option":"It is a long running task that is waiting for an event to happen. A poke function is called every n seconds to check if the criteria are met.","correct":true},{"id":787122,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, you are required to modify the configuration value of the dags_folder parameter by using an environment variable. Which of the following is the correct technique to perform this task?</p>","a":[{"id":787115,"option":"By setting the environment variable AIRFLOW DAGS_FOLDER=\"your/path/dags\" ","correct":false},{"id":787116,"option":"By setting the environment variable AIRFLOW CORE_DAGS_FOLDER=\"your/path/dags\" ","correct":true},{"id":787117,"option":"By setting the environment variable DAGS_FOLDER=\"your/path/dags\"","correct":false},{"id":787118,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, which of the following operations represents the dependency into a DAG while performing a task?</p>","a":[{"id":787107,"option":"Using the set_upstream and set_downstream functions ","correct":false},{"id":787108,"option":"Using bit shift operators such as >> and << ","correct":false},{"id":787109,"option":"Using the set_upstream and set_downstream functions and bit shift operators such as >> and <<","correct":true},{"id":787110,"option":"None of these","correct":false}]},{"q":"<p>You are required to set up the Apache Airflow platform for a reverse proxy. Which of the following settings will you set up in your <strong>airflow.cfg</strong> file to perform this task?</p>","a":[{"id":787103,"option":"sql_alchemy_conn_cmd = bash_command_to_run","correct":false},{"id":787104,"option":"base_url = https://lab.mycompany.com/myorg/airflow/","correct":false},{"id":787105,"option":"base_url = http://my_host/myorg/airflow","correct":true},{"id":787106,"option":"base_url = /myorg/flower","correct":false}]},{"q":"<p>The Apache Airflow platform is throwing an exception due to the non-availability of arguments in tasks. Which of the following arguments must be included in the respective task to avoid this scenario:</p>\n\n<ol>\n\t<li>Owner</li>\n\t<li>Dag_id</li>\n\t<li>Task_id</li>\n\t<li>Schedule_interval</li>\n</ol>","a":[{"id":786995,"option":"1 and 2","correct":false},{"id":786996,"option":"1 and 3","correct":true},{"id":786997,"option":"1 and 4","correct":false},{"id":786998,"option":"2 and 3","correct":false}]},{"q":"<p>In Airflow, tasks can remove XComs at any time to make it available for other tasks. Which of the following techniques can be used by a task for pushing an XCom:</p>\n\n<ol>\n\t<li>By calling the <strong>xcom_push()</strong> method</li>\n\t<li>By returning a value from the <strong>execute(</strong>) method that available in operators</li>\n\t<li>By returning a value from the PythonOperator’s <strong>python_callable</strong> function<br>\n\t </li>\n</ol>","a":[{"id":786991,"option":"1","correct":false},{"id":786992,"option":"1 and 2","correct":false},{"id":786993,"option":"2 and 3","correct":false},{"id":786994,"option":"All of these","correct":true}]},{"q":"<p>You are given a function. This function contains two DAGs, <strong>dag_1</strong> and <strong>dag_2</strong>. Which of the following DAG objects is stored by the Apache Airflow platform:</p>\n\n<pre class=\"prettyprint\"><code>dag_1 = DAG('this_dag_will_be_discovered')\n\ndef my_function( ):\n    dag_2 = DAG('this_dag_will_not_be_discovered')\n\nmy_function( )</code></pre>\n\n<p> </p>","a":[{"id":786983,"option":"dag_1","correct":true},{"id":786984,"option":"dag_2","correct":false},{"id":786985,"option":"Both of these","correct":false},{"id":786986,"option":"None of these","correct":false}]},{"q":"<p>While searching for DAGs in the Apache Airflow platform, the platform only considers the Python files that contain the strings '<strong>airflow</strong>' and '<strong>DAG</strong>'. Assume that you are required to consider all the Python files. Which of the following configuration flags must be disabled to perform this task?<br>\n </p>","a":[{"id":786979,"option":"DAG_FOLDER","correct":false},{"id":786980,"option":"AIRFLOW_HOME","correct":false},{"id":786981,"option":"AIRFLOW_CONFIG","correct":false},{"id":786982,"option":"DAG_DISCOVERY_SAFE_MODE","correct":true}]},{"q":"<p>Which of the following order of precedence is correct to perform tasks in Apache Airflow:</p>\n\n<ol>\n\t<li>Explicitly passed arguments</li>\n\t<li>Operator’s default value</li>\n\t<li>Values that exist in the <strong>default_args</strong> dictionary</li>\n</ol>","a":[{"id":786975,"option":"1 -> 2 -> 3","correct":false},{"id":786976,"option":"2 -> 3 -> 1","correct":false},{"id":786977,"option":"2 -> 1 -> 3","correct":false},{"id":786978,"option":"1 -> 3 -> 2","correct":true}]},{"q":"<p>In Apache Airflow, if you want to prevent overloading users with logging messages about skipped files while configuring the Airflow settings. Therefore, a file is skipped only once per <strong>DagBag</strong> file. Which of the following will you use to perform this task? </p>","a":[{"id":786967,"option":"dag_folder (unicode) parameter of dagbag","correct":false},{"id":786968,"option":"has_logged parameter of dagbag","correct":true},{"id":786969,"option":"executor parameter of dagbag","correct":false},{"id":786970,"option":"include_examples (bool) parameter of dagbag","correct":false}]},{"q":"<p>In Apache Airflow, which of the following storage locations is used to store the DAG directory?</p>","a":[{"id":786959,"option":"[AIRFLOW_ENV]/dags","correct":false},{"id":786960,"option":"[AIRFLOW_HOME]/dags","correct":true},{"id":786961,"option":"[AIRFLOW_HOME]/conf/dags","correct":false},{"id":786962,"option":"[AIRFLOW_CONF]/dags","correct":false}]},{"q":"<p>You are required to perform a task in isolation, without affecting the metadata database or being concerned about task dependencies. Which of the following commands will you use while developing the Apache Airflow platform to perform this task?</p>","a":[{"id":786955,"option":"airflow backfill DAG_ID TASK_ID -s START_DATE -e END_DATE","correct":false},{"id":786956,"option":"airflow resetdb","correct":false},{"id":786957,"option":"airflow clear DAG_ID","correct":false},{"id":786958,"option":"airflow test DAG_ID TASK_ID EXECUTION_DATE","correct":true}]},{"q":"<p>Which of the following tasks is performed by hooks in Apache Airflow?</p>","a":[{"id":786951,"option":"Limits the execution parallelism on arbitrary sets of tasks","correct":false},{"id":786952,"option":"Implements a common interface when possible and act as a building block for operators","correct":true},{"id":786953,"option":"Allows tasks to exchange messages implementing more nuanced forms of control and shared state","correct":false},{"id":786954,"option":"Stores and retrieves arbitrary content or settings as a simple key value store within Airflow","correct":false}]},{"q":"<p>In Apache Airflow, which of the following statements about operators is correct?</p>","a":[{"id":786939,"option":"It is a task that is assigned to a DAG","correct":false},{"id":786940,"option":"It is a parameterized instance of an operator","correct":false},{"id":786941,"option":"It is a class that acts as a template for performing a task","correct":true},{"id":786942,"option":"It is a description of the order in which task must be performed","correct":false}]},{"q":"<p>Which of the following conditions makes the status of an Apache Airflow instance component unhealthy?</p>","a":[{"id":786935,"option":"When the last heartbeat was received more than 30 seconds earlier than the current time","correct":true},{"id":786936,"option":"When the last heartbeat was received less than 30 seconds earlier than the current time","correct":false},{"id":786937,"option":"When the last heartbeat was received more than 60 seconds earlier than the current time","correct":false},{"id":786938,"option":"When the last heartbeat was received less than 20 seconds earlier than the current time","correct":false}]},{"q":"<p>In Apache Airflow, you are required to enable the Kerberos protocol. To complete this task, you must generate a (service) key tab. Which of the following storage locations allows you to store the file so that Airflow users can read it?</p>","a":[{"id":786927,"option":"/etc/airflow/sources/airflow.keytab","correct":false},{"id":786928,"option":"/etc/airflow/docs/airflow.keytab","correct":false},{"id":786929,"option":"/etc/airflow/airflow.keytab","correct":true},{"id":786930,"option":"/etc/airflow/airflow.keytab","correct":false}]},{"q":"<p>In Apache Airflow, if you want to deserialize a JSON object from a variable, then which of the following statements will you use to perform this task?</p>","a":[{"id":786923,"option":"echo {{ var.value.<variable_name> }}","correct":false},{"id":786924,"option":"variable_name = Variable.get(\"variable_name\", default_var=None)","correct":false},{"id":786925,"option":"variable_name = Variable.get(\"variable_name\")","correct":false},{"id":786926,"option":"echo {{ var.json.<variable_name> }}","correct":true}]},{"q":"<p>In Apache Airflow, which of the following components is used to create a view into the Airflow UI?</p>","a":[{"id":786919,"option":"Blueprint ","correct":false},{"id":786920,"option":"Admin View","correct":false},{"id":786921,"option":"Template ","correct":false},{"id":786922,"option":"All of these","correct":true}]},{"q":"<p>You find that the hive table used to store your data is not an efficient serialization format. In the given context which of the following can be used to store data in a more efficient manner before working with hooks in apache airflow?</p>","a":[{"id":787570,"option":"HiveOperator","correct":true},{"id":787571,"option":"kwargs","correct":false},{"id":787572,"option":"field_dict","correct":false},{"id":787573,"option":"tblproperties","correct":false}]},{"q":"<p>You are migrating your apache airflow installation to a new server. You require to import the pool from JSON file to do so. In the given context which of these arguments would you utilize to do so?</p>","a":[{"id":787562,"option":"-g","correct":false},{"id":787563,"option":"-x","correct":false},{"id":787564,"option":"-i","correct":true},{"id":787565,"option":"-u","correct":false}]},{"q":"<p>In Airflow, a task instance represents a task that has been assigned to a DAG and has a state associated with a specific run of the DAG.<br>\n<br>\nWhich of the following could be the indicative state of a task instance?</p>\n\n<ol>\n\t<li>failed</li>\n\t<li>success</li>\n\t<li>running</li>\n\t<li>skipped</li>\n</ol>","a":[{"id":786971,"option":"1 and 2","correct":false},{"id":786972,"option":"3 and 4","correct":false},{"id":786973,"option":"1,2 and 3","correct":false},{"id":786974,"option":"All of these","correct":true}]},{"q":"<p>Which of these sequences of operators relationship is functionally equivalent to the following bit-wise statement:</p>\n\n<blockquote>\n<p>op1 &gt;&gt; op2 &gt;&gt; op3 &lt;&lt; op4</p>\n</blockquote>","a":[{"id":786987,"option":"op1.set_downstream(op2)\r\nop2.set_upstream(op3)\r\nop3.set_downstream(op4)","correct":false},{"id":786988,"option":"op1.set_downstream(op2)\r\nop2.set_downstream(op3)\r\nop3.set_upstream(op4)","correct":true},{"id":786989,"option":"op1.set_upstream(op2)\r\nop2.set_downstream(op3)\r\nop3.set_upstream(op4)","correct":false},{"id":786990,"option":"op1.set_upstream(op2)\r\nop2.set_downstream(op3)\r\nop3.set_downstream(op4)","correct":false}]},{"q":"<p>You have a requirement wherein you need to share a SQLite database with two docker containers.<br>\nWhich of the following approach should be adopted for doing so?</p>","a":[{"id":787111,"option":"Nothing special to do, each container has its own SQLite database file and it works ","correct":false},{"id":787112,"option":"You have to create a common volume for both containers where the metadatabase will be shared. ","correct":true},{"id":787113,"option":"You need to create a common persistent volume claim for both the dockers","correct":false},{"id":787114,"option":"You can't because SQLite doesn't allow for concurrent writing.","correct":false}]},{"q":"<p>In Apache Airflow, you are running a DAG on a scheduled interval that starts at [2019-08-01T11:59] and ends at [2019-08-01T23:59]. When will the scheduler run your job instance?</p>","a":[{"id":787147,"option":"At 2019-08-01T11:59","correct":false},{"id":787148,"option":"Before 2019-08-01T11:59","correct":false},{"id":787149,"option":"At 2019-08-01T23:59","correct":true},{"id":787150,"option":"At 2019-08-01T05:59","correct":false}]},{"q":"<p>In Airflow, the tasks call the <strong>xcom_pull()</strong> method to retrieve XComs. These tasks apply filters based on some criteria. Which of the following criterion is filtered by the<strong> xcom.pull( )</strong> method as a default?<br>\n </p>","a":[{"id":787099,"option":"key","correct":true},{"id":787100,"option":"conn_id","correct":false},{"id":787101,"option":"Source's task_id","correct":false},{"id":787102,"option":"Source's dag_id","correct":false}]},{"q":"<p>In Apache Airflow, which of the following statements about the <strong>sensor</strong> operator is correct?</p>","a":[{"id":786947,"option":"It sends an HTTP request","correct":false},{"id":786948,"option":"It executes a bash command","correct":false},{"id":786949,"option":"It calls an arbitrary Python function","correct":false},{"id":786950,"option":"It waits for a certain time until a certain criterion is met","correct":true}]},{"q":"<p>If you are debugging an Apache Airflow operator, then which of the following arguments will you not use for the Airflow test command:</p>\n\n<ol>\n\t<li>Name of DAG</li>\n\t<li>Execute method</li>\n\t<li>Name of a task</li>\n\t<li>Date associated with a specific DAG Run</li>\n\t<li>Environment variable</li>\n</ol>","a":[{"id":786931,"option":"1, 3, and 4","correct":false},{"id":786932,"option":"2 and 5","correct":true},{"id":786933,"option":"2, 3, and 4","correct":false},{"id":786934,"option":"4 and 5","correct":false}]},{"q":"<p>Suppose you want to execute a SQL command in Airflow. You need to use an appropriate operator for doing so.<br>\nWhich of the following operators cannot be used for the required purpose?<br>\n </p>","a":[{"id":786943,"option":"JdbcOperator","correct":false},{"id":786944,"option":"BashOperator","correct":true},{"id":786945,"option":"OracleOperator","correct":false},{"id":786946,"option":"PostgresOperator","correct":false}]},{"q":"<p>Your local Apache Airflow settings file has defined a policy function that could apply a specific queue property when using a specific operator or enforce a task timeout policy. This policy restricts tasks to run for more than 18 hours. Which of the following configuration statements must be set up within the <strong>airflow_settings.py</strong> file?</p>","a":[{"id":787538,"option":"def policy(task): if task.__class__.__name__ == 'HivePartitionSensor': task.queue = \"sensor_queue\" task.timeout = timedelta(hours=18)","correct":false},{"id":787539,"option":"def policy(task): if class == 'HivePartitionSensor': task.queue = \"sensor_queue\" task.timeout = timedelta(hours=18)","correct":false},{"id":787540,"option":"def policy(task): if task.timeout > timedelta(hours=18): task.timeout = timedelta(hours=18)","correct":false},{"id":787541,"option":"def policy(task): if task.__class__.__name__ == 'HivePartitionSensor': task.queue = \"sensor_queue\" if task.timeout > timedelta(hours=18): task.timeout = timedelta(hours=18)","correct":true}]},{"q":"<p>Which of the following is the correct universal order of precedence for all the configuration options that are available in Apache Airflow:</p>\n\n<ol>\n\t<li>Set up in the <strong>airflow.cfg</strong> file</li>\n\t<li>Built-in defaults that are available in Airflow</li>\n\t<li>Set an environment variable</li>\n\t<li>Command in the <strong>airflow.cfg</strong> file</li>\n</ol>","a":[{"id":786963,"option":"1 -> 2 -> 3 -> 4","correct":false},{"id":786964,"option":"2 -> 1 -> 4 -> 3","correct":false},{"id":786965,"option":"3 -> 1 -> 4 -> 2","correct":true},{"id":786966,"option":"4 -> 1 -> 2 -> 3","correct":false}]}]