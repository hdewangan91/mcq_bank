[{"q":"<p>John is working on Hadoop for his clients. Then which of the following options is used to&nbsp;produce a RecordReader, by enumerating InputSplits, which turns work units into physical input records?</p>","a":[{"id":745442,"option":"Block","correct":false},{"id":745443,"option":"InputFormat","correct":true},{"id":745444,"option":"Mapper","correct":false},{"id":745445,"option":"Reducer","correct":false}]},{"q":"<p>Which of these&nbsp;is the correct sequence of steps that are followed to debug a Hadoop code:</p>\n\n<p><strong>Steps:</strong></p>\n\n<ol>\n\t<li>Examine the Node Manager log. The majority of errors come from user-level&nbsp;logs related&nbsp;map-reduce job.</li>\n\t<li>Login to that node and run \"ps –ef | grep –iNodeManager\" command.</li>\n\t<li>Execute “ps –ef | grep –I ResourceManager” command and look for the&nbsp;log directory in the displayed result.&nbsp;Find out the job-id from the displayed list and also, check if there is an&nbsp;error message associated with that job.</li>\n\t<li>On the basis of RM logs,&nbsp;identify the worker node that is&nbsp;involved in the execution of a&nbsp;task.</li>\n</ol>","a":[{"id":745479,"option":"1->2->3->4","correct":false},{"id":745480,"option":"3->2->4->1","correct":false},{"id":745481,"option":"4->3->2->1","correct":false},{"id":745482,"option":"3->4->2->1","correct":true}]},{"q":"<p>&nbsp;John is working in Hadoop. Then what is the correct sequence of steps taken for the recovery process of the NameNode?</p>\n\n<p><strong>Steps:</strong></p>\n\n<ol>\n\t<li>The clients and DataNodes are configured to acknowledge the new NameNode.</li>\n\t<li>Once the new NameNode completes loading of the last checkpoint called FsImage and receives block reports from the DataNodes, the new NameNode starts to serve the client.</li>\n\t<li>Utilize the file system metadata replica (FsImage) in order to start a new NameNode,.</li>\n</ol>","a":[{"id":745458,"option":"1->2->3","correct":false},{"id":745459,"option":"3->2->1","correct":false},{"id":745460,"option":"1->3->2","correct":false},{"id":745461,"option":"3->1->2","correct":true}]},{"q":"<p>John is using Hadoop framework for his clients. Then which of the following options should he consider such that a single reducer gathers and processes all the output from all the mappers</p>","a":[{"id":172104,"option":"Create a single node cluster","correct":false},{"id":172105,"option":"Set number of reducers as 1","correct":true},{"id":172106,"option":"Use capacity scheduler instead of First In First Out","correct":false},{"id":172107,"option":"All of these","correct":false}]},{"q":"<p>Mike is using Hadoop framework for his clients. He is using replica to ensure availability of data. If he made 6 file blocks then determine the number of blocks made for backup purpose if replica factor is assumed to be unchanged.</p>","a":[{"id":171889,"option":"12","correct":false},{"id":171890,"option":"6","correct":false},{"id":171891,"option":"3","correct":false},{"id":171892,"option":"18","correct":true}]},{"q":"<p>John is using Hadoop framework for his clients. Then which of the following is an invalid mode for Hadoop to run?</p>","a":[{"id":171779,"option":"Standalone mode","correct":false},{"id":171780,"option":"Pseudo-distributed mode","correct":true},{"id":171781,"option":"Partially-distributed mode","correct":false},{"id":171782,"option":"Fully-distributed mode","correct":false}]},{"q":"<p>John is using Hadoop framework for his clients. Then which of the following require data replication to occur?</p>","a":[{"id":171679,"option":"The replication factor is changed","correct":false},{"id":171680,"option":"DataNode goes down","correct":false},{"id":171681,"option":"The data blocks get corrupted","correct":false},{"id":171682,"option":"All of these","correct":true}]},{"q":"<p>John is using Hadoop framework for his clients. He is using MapReduce framework to achieve parallel processing. Then which of the following pair of phases occur parallely?</p>","a":[{"id":171524,"option":"Shuffle and sort","correct":true},{"id":171525,"option":"Reduce and sort","correct":false},{"id":171526,"option":"Shuffle and map","correct":false},{"id":171527,"option":"Combiner and Sort","correct":false}]},{"q":"<p>John is using Hadoop framework for his clients. He has to perform data transfer between different storage systems. Then which of the following is used to validate the contents of the HDFS files?</p>","a":[{"id":171834,"option":"metastore","correct":false},{"id":171835,"option":"parity","correct":false},{"id":171836,"option":"checksum","correct":true},{"id":171837,"option":"check","correct":false}]},{"q":"<p>Mike started using Hadoop framework in his organisation. Then which of the following would have improved in the organisation due to use of the framework?</p>","a":[{"id":170879,"option":"Security ","correct":false},{"id":170880,"option":"Workload management ","correct":false},{"id":170881,"option":"SQL support","correct":false},{"id":170882,"option":"All of these","correct":true}]},{"q":"<p>John is using Hadoop framework for his clients and wants to implement most of the features the framework supports. Then which of the following feature he cannot use as it is not characteristic of Hadoop?</p>","a":[{"id":170854,"option":"Open source","correct":false},{"id":170855,"option":"Real-time","correct":true},{"id":170856,"option":"Java-based","correct":false},{"id":170857,"option":"Handles large amount of data","correct":false}]},{"q":"<p>Mike is using Hadoop framework for his clients and trying to understand more about MapReduce model. Then which of the following represents the correct logical flow of the model?</p>","a":[{"id":183511,"option":"Input -> Map -> Reduce -> Combine -> Shuffle and Sort -> Split -> Output","correct":false},{"id":183512,"option":"Input -> Split -> Map -> Combine -> Shuffle and sort -> Reduce -> Output","correct":true},{"id":183513,"option":"Input -> Split -> Combine -> Map -> Shuffle and sort -> Reduce -> Output","correct":false},{"id":183514,"option":"Input -> Split -> Combine -> Shuffle and sort -> Reduce -> Output","correct":false}]},{"q":"<p>Mike is using Hadoop framework for his clients. Then which version should he use to avoid the issue of single point failure by having multiple name node?</p>","a":[{"id":183696,"option":"Hadoop 1","correct":false},{"id":183697,"option":"Hadoop 2","correct":false},{"id":183698,"option":"Hadoop 3","correct":false},{"id":183699,"option":"Isuue is still unresolved.","correct":true}]},{"q":"<p>Mike is working on Hadoop for user data storage. Then which of these stores the user data in the form of independent data block?</p>","a":[{"id":171684,"option":"DataNode","correct":true},{"id":171685,"option":"NameNode","correct":false},{"id":171686,"option":"MemoryNode","correct":false},{"id":171687,"option":"Replication","correct":false}]},{"q":"<p>In Hadoop, you are working on HDFS architecture. If you are required to perform the following actions, then which of the following elements is used to perform these actions in this scenario:</p>\n\n<p><strong>Actions</strong></p>\n\n<ol>\n\t<li>Manage the file system namespace.</li>\n\t<li>Regulate the client’s access to files.</li>\n\t<li>Execute file system operations such as renaming, closing, and opening files and directories</li>\n</ol>","a":[{"id":1586414,"option":"Datanode","correct":false},{"id":1586415,"option":"Namenode","correct":true},{"id":1586416,"option":"Mapreduce cluster","correct":false},{"id":1586417,"option":"None of these","correct":false}]},{"q":"<p>How do Hive and Hadoop communicate with the Hive S3 tables while working with <strong>HiveAwsEmr</strong>?</p>","a":[{"id":1176791,"option":"They communicate automatically by using provided credentials.","correct":true},{"id":1176792,"option":"They communicate manually after providing credentials.","correct":false},{"id":1176793,"option":"They communicate manually before providing credentials.","correct":false},{"id":1176794,"option":"They communicate automatically after providing credentials.","correct":false}]},{"q":"<p>You are using Amazon Elastic MapReduce to run your Hive cluster using multiple clients. Which of the following can be used to launch the Hive cluster:</p>\n\n<ol>\n\t<li>AWS Management Console</li>\n\t<li>Amazon Elastic MapReduce Ruby Client</li>\n\t<li>AWS Java SDK</li>\n</ol>","a":[{"id":1176787,"option":"1 and 2","correct":false},{"id":1176788,"option":"2 and 3","correct":false},{"id":1176789,"option":"1 and 3","correct":false},{"id":1176790,"option":"All of these","correct":true}]},{"q":"<p>What does the Hive counter <strong>RECORDS_IN[_%context]</strong> do?</p>","a":[{"id":1176783,"option":"It reads the input records.","correct":true},{"id":1176784,"option":"It writes the input records.","correct":false},{"id":1176785,"option":"It read and writes the input records.","correct":false},{"id":1176786,"option":"It reads the intermediate records.","correct":false}]},{"q":"<p>While handlings errors in Hive HCatalog Streaming API, the <strong>StreamingException</strong> exception causes the client to perform exponential back off before starting a new batch. What is the significance of this information?</p>","a":[{"id":1176779,"option":"It stabilizes the cluster.","correct":true},{"id":1176780,"option":"It handles HDFS overload failures.","correct":false},{"id":1176781,"option":"It handles the tuples that are not parsed.","correct":false},{"id":1176782,"option":"It handles HDFS overload failures and the tuples that are not parsed.","correct":false}]},{"q":"<p>Which of the following statements about concurrency in Hive HCatalog Streaming API is correct:</p>\n\n<ol>\n\t<li>I/O can be performed on multiple TransactionBatches concurrently.</li>\n\t<li>The transactions within a transaction batch must be consumed sequentially.</li>\n</ol>","a":[{"id":1176775,"option":"1","correct":false},{"id":1176776,"option":"2","correct":false},{"id":1176777,"option":"Both of these","correct":true},{"id":1176778,"option":"None of these","correct":false}]},{"q":"<p>Which of the following is used to provide support for other input formats while working with Hive Streaming API?</p>","a":[{"id":1176771,"option":"Implement the RecordWriter interface","correct":true},{"id":1176772,"option":"Implement the StreamWriter interface","correct":false},{"id":1176773,"option":"Implement the DataWriter interface","correct":false},{"id":1176774,"option":"None of these","correct":false}]},{"q":"<p>While using the LEAD windowing function in Hive QL, you have not specified the number of rows to lead. Which of the following is the lead in this context?</p>","a":[{"id":1176755,"option":"Null","correct":false},{"id":1176756,"option":"One row","correct":true},{"id":1176757,"option":"Two rows","correct":false},{"id":1176758,"option":"None of these","correct":false}]},{"q":"<p>When a user connects to a Hive Metastore Server, the Metastore determines some information of the connecting user. What is the usage of this information?</p>","a":[{"id":1176751,"option":"It is used to determine if the user must have access to the metadata.","correct":true},{"id":1176752,"option":"It is used to determine if the user must have access to groups associated to the metadata.","correct":false},{"id":1176753,"option":"It is used to determine if the user has access to groups associated to the metadata.","correct":false},{"id":1176754,"option":"It is used to determine if the user has access to the metadata.","correct":false}]},{"q":"<p>An admin wants to create a new role in Hive. Which of the following role names are reserved and cannot be used by the admin:</p>\n\n<ol>\n\t<li>ALL</li>\n\t<li>DEFAULT</li>\n\t<li>NONE</li>\n</ol>","a":[{"id":1176747,"option":"1 and 2","correct":false},{"id":1176748,"option":"2 and 3","correct":false},{"id":1176749,"option":"1 and 3","correct":false},{"id":1176750,"option":"All of these","correct":true}]},{"q":"<p>Which of these Hive configuration parameters when initialized, turns on metastore-side security?</p>","a":[{"id":1176739,"option":"hive.metastore.pre.event.listeners","correct":true},{"id":1176740,"option":"hive.security.metastore.authorization.manager","correct":false},{"id":1176741,"option":"hive.security.metastore.authenticator.manager","correct":false},{"id":1176742,"option":"hive.security.metastore.authorization.auth.reads","correct":false}]},{"q":"<p>Which of the following patch formats can be used in Hive:</p>\n\n<ol>\n\t<li>git diff</li>\n\t<li>git diff --no-prefix</li>\n\t<li>svn diff</li>\n</ol>","a":[{"id":1176727,"option":"1 and 2","correct":false},{"id":1176728,"option":"2 and 3","correct":false},{"id":1176729,"option":"1 and 3","correct":false},{"id":1176730,"option":"All of these","correct":true}]},{"q":"<p>In Hive, which of the following is the primary function of the RecordWriter interface:</p>\n\n<ol>\n\t<li>Modify input records</li>\n\t<li>Encode modified record</li>\n\t<li>Identify the bucket to which the record belongs</li>\n</ol>","a":[{"id":1176723,"option":"1 and 2","correct":false},{"id":1176724,"option":"2 and 3","correct":false},{"id":1176725,"option":"1 and 3","correct":false},{"id":1176726,"option":"All of these","correct":true}]},{"q":"<p>Which of the following is used by the HIve compiler to transform the parse tree to an internal query representation that is still block based?</p>","a":[{"id":1176719,"option":"Parser","correct":false},{"id":1176720,"option":"Semantic Analyser ","correct":true},{"id":1176721,"option":"Logical Plan Generator ","correct":false},{"id":1176722,"option":"Query Plan Generator ","correct":false}]},{"q":"<p>Which of the following Hive APIs will you use to apply large sets of mutations to a data set in an atomic fashion?</p>","a":[{"id":1176711,"option":"Streaming API","correct":false},{"id":1176712,"option":"Mutation API","correct":true},{"id":1176713,"option":"Batch API","correct":false},{"id":1176714,"option":"None of these","correct":false}]},{"q":"<p>You want to create a custom class that allows you to have an extension point without changing the <strong>ObjectInspector</strong> class for the <strong>rowId</strong> column. Which of the following extension points will you use?</p>","a":[{"id":1176707,"option":"accumulo.composite.rowid.factory","correct":true},{"id":1176708,"option":"accumulo.composite.column","correct":false},{"id":1176709,"option":"accumulo.composite.rowid","correct":false},{"id":1176710,"option":"None of these","correct":false}]},{"q":"<p>Which of these Hive components can be used to perform Hive metadata operations using an HTTP (REST style) interface?</p>","a":[{"id":1176691,"option":"WebHCat ","correct":true},{"id":1176692,"option":"HCatalog","correct":false},{"id":1176693,"option":"Yetus","correct":false},{"id":1176694,"option":"MiniDrive","correct":false}]},{"q":"<p>Which of the following Hive configuration variables decides the disk storage location for Metadata?</p>","a":[{"id":1176687,"option":"javax.jdo.option.ConnectionURL","correct":true},{"id":1176688,"option":"javax.jdo.option.Store","correct":false},{"id":1176689,"option":"javax.jdo.option.Config","correct":false},{"id":1176690,"option":"javax.jdo.option.Data","correct":false}]},{"q":"<p>You want to ensure that a map-reduce job in a query can be locally run in Hive. In this scenario, the total number of reduce tasks required must not be greater than which of these values?</p>","a":[{"id":1176679,"option":"1","correct":true},{"id":1176680,"option":"2","correct":false},{"id":1176681,"option":"3","correct":false},{"id":1176682,"option":"5","correct":false}]},{"q":"<p>Which of the following features of a data warehouse are provided by the Metastore in Hive?</p>","a":[{"id":1176667,"option":"Data abstraction","correct":false},{"id":1176668,"option":"Data discovery","correct":false},{"id":1176669,"option":"Both of these","correct":true},{"id":1176670,"option":"None of these","correct":false}]},{"q":"<p>In Hive, which of the following transaction configuration parameters can be used to control the number of transaction streaming agents open simultaneously?</p>","a":[{"id":1176663,"option":"hive.compactor.history.reaper.interval","correct":false},{"id":1176664,"option":"hive.txn.max.open.batch","correct":true},{"id":1176665,"option":"hive.compactor.max.num.delta","correct":false},{"id":1176666,"option":"hive.compactor.job.queue","correct":false}]},{"q":"<p>If you want to enable transaction support in Hive, then which of these configuration parameters will you require to set on the client side:</p>\n\n<ol>\n\t<li>hive.support.concurrency</li>\n\t<li>hive.exec.dynamic.partition.mode</li>\n\t<li>hive.txn.manager</li>\n</ol>","a":[{"id":1176659,"option":"1 and 2","correct":false},{"id":1176660,"option":"2 and 3","correct":false},{"id":1176661,"option":"1 and 3","correct":false},{"id":1176662,"option":"All of these","correct":true}]},{"q":"<p>You want to obtain the performance metrics through <strong>PerfLogger</strong> while working in Hive. Which of the following logging leves must be set for the <strong>PerfLogger</strong> class?</p>","a":[{"id":1176627,"option":"INFO","correct":false},{"id":1176628,"option":"WARNING","correct":true},{"id":1176629,"option":"DEBUG","correct":false},{"id":1176630,"option":"INSPECT","correct":false}]},{"q":"<p>You want to configure a different log location for Hive. Which of the following sticky bit set must be used for <strong>hive.log.dir</strong> while performing this task?</p>","a":[{"id":1176623,"option":"chmod 1777 <dir>","correct":true},{"id":1176624,"option":"chmod 999 <dir>","correct":false},{"id":1176625,"option":"chmod 666 <dir>","correct":false},{"id":1176626,"option":"chmod 939 <dir>","correct":false}]},{"q":"<p>You have created <strong>hive.metastore.warehouse.dir</strong> while configuring Hive. What are the <strong>chmod</strong> permissions that must be assigned to it in order to create a table in Hive?</p>","a":[{"id":1176615,"option":"chmod g+w ","correct":true},{"id":1176616,"option":"chmod r+w","correct":false},{"id":1176617,"option":"chmod 666","correct":false},{"id":1176618,"option":"chmod g+r","correct":false}]},{"q":"<p>The code given alongside is used to generate an adapter class called TrialAdapter in Hive. Analyse the code carefully and choose the valid deductions that can be made?<br>\nD1: The name of the adapter can be changed with the optional adapterName parameter of @HiveType.<br>\nD2: Each field has a unique number which should not be changed once your class is in use.</p>\n\n<pre class=\"prettyprint\"><code>import 'package:hive/hive.dart';\n\npart 'trial.g.dart';\n\n@HiveType()\nclass Trial {\n  @HiveField(0)\n  String name;\n\n  @HiveField(1)\n  int age;\n\n  @HiveField(2)\n  List&lt;Trial&gt; friends;\n}</code></pre>\n\n<p> </p>","a":[{"id":1179225,"option":"Only D1 ","correct":false},{"id":1179226,"option":"Only D2","correct":false},{"id":1179227,"option":"Both D1 and D2","correct":true},{"id":1179228,"option":"Neither D1 nor D2","correct":false}]},{"q":"<p>You want to cogroup the rows from the tables video and comments on the uid column and send them to the 'reduce_script' custom reducer. In order to do so, you are using the query given alongside.<br>\n<br>\nWhat can be used in place of XXX to complete the code?</p>\n\n<pre class=\"prettyprint\"><code>FROM (\n     FROM (\n             FROM video av\n             SELECT av.uid AS uid, av.id AS id, av.date AS date\n \n            UNION ALL\n \n             FROM comment ac\n             SELECT ac.uid AS uid, ac.id AS id, ac.date AS date\n     ) union_actions\n\n    XXX\n \n INSERT OVERWRITE TABLE actions_reduced\n     SELECT TRANSFORM(map.uid, map.id, map.date) USING 'reduce_script' AS (uid, id, reduced_val);</code></pre>\n\n<p> </p>","a":[{"id":1179209,"option":" XXX: SELECT union_actions.uid, union_actions.id, union_actions.date\r\n     CLUSTER BY union_actions.uid) map","correct":true},{"id":1179210,"option":" XXX: CLUSTER BY union_actions.uid) map","correct":false},{"id":1179211,"option":" XXX: SELECT union_actions.uid, union_actions.id, union_actions.date) map\r\n  ","correct":false},{"id":1179212,"option":" XXX: CLUSTER BY union_actions.uid) map\r\nSELECT union_actions.uid, union_actions.id, union_actions.date\r\n     ","correct":false}]},{"q":"<p>You want to create a table in Hive such that the table is clustered by a hash function of userid into 32 buckets. Within each bucket the data is sorted in increasing order of viewTime. In order to achieve this, you use the code snippet given alongside.<br>\n<br>\nAnalyse the given scenario and determine which of the following can be achieved by doing this?<br>\n1. Allows the user to do efficient sampling on the clustered column—n this case userid<br>\n2. Allows internal operators to take advantage of the better-known data structure while evaluating queries with greater efficiency.</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE page_view(viewTime INT, userid BIGINT,\n                page_url STRING, referrer_url STRING,\n                ip STRING COMMENT 'IP Address of the User')\nCOMMENT 'This is the page view table'\nPARTITIONED BY(dt STRING, country STRING)\nCLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\nROW FORMAT DELIMITED\n        FIELDS TERMINATED BY '1'\n        COLLECTION ITEMS TERMINATED BY '2'\n        MAP KEYS TERMINATED BY '3'\nSTORED AS SEQUENCEFILE;</code></pre>\n\n<p> </p>","a":[{"id":1179205,"option":"Only 1","correct":false},{"id":1179206,"option":"Only 2","correct":false},{"id":1179207,"option":"Both1 and 2","correct":true},{"id":1179208,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You have a base table as shown alongside. What will the output produced by the given query be?<br>\n<br>\nSELECT Col1, col2 FROM baseTable<br>\nLATERAL VIEW explode(col1) Table1 AS Col1;</p>\n\n<table border=\"1\" style=\"width: 223px;\">\n\t<tbody>\n\t\t<tr>\n\t\t\t<td style=\"width: 106px;\">Array&lt;int&gt;col1</td>\n\t\t\t<td style=\"width: 99px;\">Array&lt;str&gt; col2</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"text-align: center; width: 106px;\">[5,6]</td>\n\t\t\t<td style=\"text-align: center; width: 99px;\">[p\",\"q\",\"r]</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"text-align: center; width: 106px;\">[3,4]</td>\n\t\t\t<td style=\"text-align: center; width: 99px;\">[a\",\"b\",\"c]</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n<p> </p>","a":[{"id":1179165,"option":"5    [p\", \"q\", \"r]\r\n6    [p\", \"q\", \"r]\r\n 3    [a\", \"b\", \"c]\r\n 4     [a\", \"b\", \"c]","correct":true},{"id":1179166,"option":"5    [p\", \"q\", \"r]\r\n3    [a\", \"b\", \"c]\r\n6    [p\", \"q\", \"r]\r\n 4     [a\", \"b\", \"c]","correct":false},{"id":1179167,"option":"3    [a\", \"b\", \"c]\r\n 4    [a\", \"b\", \"c]\r\n5    [p\", \"q\", \"r]\r\n6    [p\", \"q\", \"r]","correct":false},{"id":1179168,"option":" 3    [a\", \"b\", \"c]\r\n5    [p\", \"q\", \"r]\r\n 4     [a\", \"b\", \"c]\r\n6    [p\", \"q\", \"r]\r\n","correct":false}]},{"q":"<p>You have a base table named Ads which has two columns pid and ad_list.<br>\n1. The column pid is of type STRING and consists the name of the page.<br>\n2. The column ad_list is of type Array and consists array of ads appearing on the page.<br>\n<br>\nAssume that the table contains the following rows. In the given context, which of the following queries can be used to get the output as shown alongside?<br>\n<br>\n    pid ad_list<br>\nfront_page [1, 2, 3]<br>\ncontact_pag [3, 4, 5]</p>\n\n<pre class=\"prettyprint\"><code>Output:\n\n1     1\n2     1\n3     2\n4     1\n5     1</code></pre>\n\n<p> </p>","a":[{"id":1179161,"option":"SELECT adid, count(1)\r\nFROM Ads LATERAL VIEW explode(adid_list) adTable AS adid\r\nGROUP BY adid;","correct":true},{"id":1179162,"option":"SELECT adid, count(1)\r\nFROM Ads LATERAL VIEW explode(adid_list) adTable AS adid;","correct":false},{"id":1179163,"option":"SELECT adid, count(1)\r\nFROM Ads VIEW (adid_list) adTable AS adid\r\nGROUP BY adid;","correct":false},{"id":1179164,"option":"None of these","correct":false}]},{"q":"<p>You are using the TABLESAMPLE clause given alongside to write queries for samples of the data instead of the whole table.<br>\n<br>\nWhich of the following is true with respect to the given scenario?</p>\n\n<pre class=\"prettyprint\"><code>table_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname])</code></pre>\n\n<p> </p>","a":[{"id":1179137,"option":"The TABLESAMPLE clause can be added to any table in the FROM clause","correct":false},{"id":1179138,"option":"The colname cannot be one of the non-partition columns in the table","correct":false},{"id":1179139,"option":"The rows of the table are 'bucketed' on the colname randomly into y buckets numbered 1 through y","correct":false},{"id":1179140,"option":"Both 1 and 3","correct":true}]},{"q":"<p>The monolithic query given alongside has a very broad set of responsibilities which cannot be easily verified in isolation.<br>\n<br>\nWhat can you say about the deductions with respect to the given scenario?<br>\n<br>\nD1: Each of the subqueries into separate components so that they can be tested independently.<br>\nD2: Perform variable substitution of query fragments for modularization.</p>\n\n<pre class=\"prettyprint\"><code>SELECT ... FROM (                 \n  SELECT ... FROM (             \n    SELECT ... FROM (            \n      SELECT ... FROM a WHERE ... \n    ) A LEFT JOIN (              \n      SELECT ... FROM b          \n    ) B ON (...)                \n  ) ab FULL OUTER JOIN (          \n    SELECT ... FROM c WHERE ...  \n  ) C ON (...)                   \n) abc LEFT JOIN (                 \n  SELECT ... FROM d WHERE ...   \n) D ON (...)                      \nGROUP BY ...;  </code></pre>\n\n<p> </p>","a":[{"id":1178869,"option":"D2 is an approach used to achieve D1 and perform effectively unit testing","correct":true},{"id":1178870,"option":"D2 and D1 are ways to achieve effect unit testing ","correct":false},{"id":1178871,"option":"D1 is an approach used to achieve D2 and perform effective unit testing","correct":false},{"id":1178872,"option":"D1 and D2 are not related","correct":false}]},{"q":"<p>You are creating a new HBase table using the query given alongside<br>\n<br>\nHow can the query be altered in order to give Hive access to an existing HBase table?</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE hbase_table_1(key int, value string) \n</code></pre>\n\n<p> </p>","a":[{"id":1178845,"option":"CREATE EXISTING TABLE hbase_table_1(key int, value string) \r\n","correct":true},{"id":1178846,"option":"CREATE EXTERNAL TABLE hbase_table_1(key int, value string) \r\n","correct":false},{"id":1178847,"option":"CREATE EXT TABLE hbase_table_1(key int, value string) \r\n","correct":false},{"id":1178848,"option":"None of these","correct":false}]},{"q":"<p>You are creating a new HBase table which is to be managed by Hive using the query given alongside.<br>\n<br>\nWhat should be used in place of XXX to achieve this?</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE hbase_table_1(key int, value string) \nXXX\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf1:val\")\nTBLPROPERTIES (\"hbase.table.name\" = \"xyz\", \"hbase.mapred.output.outputtable\" = \"xyz\");</code></pre>\n\n<p> </p>","a":[{"id":1178837,"option":"STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'","correct":true},{"id":1178838,"option":"STORE USING 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'","correct":false},{"id":1178839,"option":"STORED BY 'org.apache.HBaseStorageHandler'","correct":false},{"id":1178840,"option":"STORE 'org.apache.HBaseStorageHandler'","correct":false}]},{"q":"<p>You are working with the COPY FROM FTP HPL/SQL statement given alongside. Assume that you haven't specified the DIR option to get the files.<br>\n<br>\nWhat happens in the given scenario?</p>\n\n<pre class=\"prettyprint\"><code>COPY FROM FTP host [USER user [PWD password]] [DIR directory] [FILES files_wildcard] \n  [TO [LOCAL] target_directory] [options]</code></pre>\n\n<p> </p>","a":[{"id":1178797,"option":"The current working FTP directory is used","correct":true},{"id":1178798,"option":"A prompt asking for DIR info is asked","correct":false},{"id":1178799,"option":"A new option is built using the ETL process","correct":false},{"id":1178800,"option":"Both 1 and 2","correct":false}]},{"q":"<p>When initializing Hive with no arguments, you have defined ANT_LIB as an environment variable is shown alongside.<br>\n<br>\nWhich of the following code snippets can be used to demonize in a production environment?</p>\n\n<pre class=\"prettyprint\"><code>export ANT_LIB=/opt/ant/lib\nbin/hive --service hwi</code></pre>\n\n<p><br>\n </p>","a":[{"id":1178769,"option":"nohup bin/hive --service hwi > /dev/null 2> /dev/null &","correct":true},{"id":1178770,"option":"nohup bin/hive --service hwi > /dev/null 2 &","correct":false},{"id":1178771,"option":"nohup bin/hive --service hwi > /dev/null &","correct":false},{"id":1178772,"option":"nohup bin/hive --service hwi > /dev/null > /dev/null 2 &","correct":false}]},{"q":"<p>Assume that you want to upgrade Hive from a Lower Version of CDH 5. You are currently running Hive under MRv1Y and you have removed the property given alongside before proceeding.<br>\n<br>\nWhat happens when you do so?</p>\n\n<pre class=\"prettyprint\"><code>&lt;property&gt;\n &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;</code></pre>\n\n<p> </p>","a":[{"id":1178781,"option":"Hive queries spawned from MapReduce\r\njobs do not fail with a null pointer exception (NPE)","correct":true},{"id":1178782,"option":"Hive queries spawned from MapReduce\r\njobs fail with a null pointer exception (NPE)","correct":false},{"id":1178783,"option":"Hive queries spawned from MapReduce\r\njobs fail with an exception","correct":false},{"id":1178784,"option":"None of these","correct":false}]},{"q":"When working with dynamic partition insert statement in Hive, a set of non-empty partitions that exists for the dynamic partition columns are already there. What happens to the dynamic partition in the following cases?<br>1. The dynamic partition insert saw the same value in the input data<br>2. The partition value does not appear in the input data","a":[{"id":1179217,"option":"1. The existing partition will not be overwritten\n2. The existing partition will be overwritten","correct":false},{"id":1179218,"option":"The existing partition will not be overwritten in both the cases","correct":false},{"id":1179219,"option":"The existing partitions will be overwritten in both the cases","correct":false},{"id":1179220,"option":"1. The existing partition will be overwritten\n2. The existing partition will not be overwritten\n ","correct":true}]},{"q":"A user has created tables with decimal columns on Hive 0.12.0. Assume that the user has upgraded to Hive 0.13.0 and is asked to migrate the tables. To do so, the user has determined the precision to set for the decimal column in the table.<br><br>How can the user update the column definition to the desired precision for each decimal column in the table?","a":[{"id":1179153,"option":"ALTER TABLE foo CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);","correct":true},{"id":1179154,"option":"SET TABLE foo CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18);","correct":false},{"id":1179155,"option":"SET TABLE foo ALTER COLUMN dec_column_name dec_column_name DECIMAL(38,18);","correct":false},{"id":1179156,"option":"ALTER TABLE foo SET COLUMN dec_column_name dec_column_name DECIMAL(38,18);","correct":false}]},{"q":"Which of the following Relational operators takes all primitive types?<br>1. X <= Y<br>2. X RLIKE Y<br>3. X REGEXP Y","a":[{"id":1179133,"option":"Only 1 and 2","correct":false},{"id":1179134,"option":"Only 2 and 3","correct":false},{"id":1179135,"option":"Only 1 ","correct":true},{"id":1179136,"option":"Only 2","correct":false}]},{"q":"You are using the TABLESAMPLE clause to scan an entire table and fetch the sample. When doing so, you observe that it is not very efficient.<br><br>What can be used alternatively to deal with this problem?","a":[{"id":1179141,"option":"Create the table using the CLUSTERED BY clause","correct":true},{"id":1179142,"option":"Create the table using the CLUSTERTABLE clause","correct":false},{"id":1179143,"option":"Create the table using the CLUSTER TABLESAMPLE clause","correct":false},{"id":1179144,"option":"Create the table using the JOINCLUSTER clause","correct":false}]},{"q":"Which of the following Interval Descriptions can be interpreted as follows?<br>INTERVAL '1' DAY+<br>INTERVAL '2' HOUR +<br>INTERVAL '3' MINUTE +<br>INTERVAL '4' SECOND","a":[{"id":1179129,"option":"INTERVAL '-1 2:3:4' DAY","correct":false},{"id":1179130,"option":"INTERVAL '1-2:3:4' DAY","correct":false},{"id":1179131,"option":"INTERVAL '1 2:3:4' DAY","correct":true},{"id":1179132,"option":"INTERVAL '1+2:3:4' DAY","correct":false}]},{"q":"Which of the following XPath expressions can you use in Hive to get a list of node values?","a":[{"id":1179113,"option":"select xpath('<a><b>b1</b><b>b2</b></a>','a/*/text()') from src limit 1 ;","correct":true},{"id":1179114,"option":"select xpath('<a><b>b1</b><b>b2</b></a>','a/*') from src limit 1 ;","correct":false},{"id":1179115,"option":"select xpath('<a><b>\"b1\"</b><b>\"b2\"</b></a>','a/*') from src limit 1 ;","correct":false},{"id":1179116,"option":"None of these","correct":false}]},{"q":"When working with Accumulo-backed Hive tables, you are using a string encoded table. <br><br>How are the indexed field value encoded for numeric types in the given scenario?","a":[{"id":1178865,"option":"Using the Accumulo Lexicoder methods","correct":true},{"id":1178866,"option":"Using native binary encoding","correct":false},{"id":1178867,"option":"Using the Accumulo outside of Hive","correct":false},{"id":1178868,"option":"Using indexes outside of Hive","correct":false}]},{"q":"When registering a Native SerDes in Hive, you are using the STORED AS AVRO syntax to store it.<br><br>Which of the following is this equivalent to with respect to the ROW FORMAT SERDE?","a":[{"id":1178829,"option":"org.apache.hadoop.hive.serde2.avro.AvroSerDe'","correct":true},{"id":1178830,"option":"org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'","correct":false},{"id":1178831,"option":"org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'","correct":false},{"id":1178832,"option":"org.apache.hadoop.hive.serde2.avro.AvroSerDeRowFormat'","correct":false}]},{"q":"You observe that inserting large amounts of data in the HBase is very slow due to WAL overhead and you disable WAL.<br><br>When does disabling WAL lead to dataloss?","a":[{"id":1178841,"option":"If an HBase failure occurs","correct":true},{"id":1178842,"option":"When other recovery strategies are used","correct":false},{"id":1178843,"option":"When other recovery strategies are not used","correct":false},{"id":1178844,"option":"If Hive does have access to the HBase","correct":false}]},{"q":"When working in Advanced mode, you want to compile against a custom version of the Hadoop tree to run Ivy. To do so,  you are specifying the root of the Hadoop source tree to be used.<br><br>Which of the following commands is the correct way to implement this?","a":[{"id":1178833,"option":"ant -Dhadoop.version=0.17.1 package","correct":false},{"id":1178834,"option":"ant -Dtarget.dir=<my-install-dir> package","correct":false},{"id":1178835,"option":"ant -Dhadoop.root=~/src/hadoop-19/build/hadoop-0.19.2-dev -Dhadoop.version=0.19.2-dev","correct":true},{"id":1178836,"option":"ant -Dhadoop.root=~/src/hadoop-19/hadoop-0.19.2-dev -Dhadoop.version=0.19.2-dev/build","correct":false}]},{"q":"You want to raise a condition if the number of rows in a table is not equal to 1.<br><br>Which of the following procedural SQL queries is a valid way to achieve this?","a":[{"id":1178793,"option":"DECLARE cnt INT DEFAULT 0; \nDECLARE wrong_cnt_condition CONDITION;\n \nDECLARE EXIT HANDLER FOR wrong_cnt_condition\n  PRINT 'Wrong number of rows';  \n \nSELECT COUNT(*) INTO cnt FROM TABLE (VALUES (1,2));\n \nIF cnt <> 1 THEN\n  SIGNAL wrong_cnt_condition;\nEND IF;","correct":true},{"id":1178794,"option":"DECLARE cnt INT 1; \nDECLARE wrong_cnt_condition CONDITION;\n \nDECLARE EXIT HANDLER FOR wrong_cnt_condition\n  PRINT 'Wrong number of rows';  \n \nSELECT COUNT(*) INTO cnt FROM TABLE (VALUES (1,2));\n \n  SIGNAL wrong_cnt_condition;\nEND IF;","correct":false},{"id":1178795,"option":"DECLARE cnt INT DEFAULT 0; \nDECLARE wrong_cnt_condition CONDITION;\n \nDECLARE EXIT HANDLER FOR wrong_cnt_condition\n  PRINT 'Wrong number of rows';  \n \nSELECT COUNT(*) INTO cnt FROM TABLE (VALUES (1,2));\n \nIF cnt <> 1 THEN\n  SIGNAL wrong_cnt_condition;\n","correct":false},{"id":1178796,"option":"DECLARE cnt INT DEFAULT 1; \nDECLARE wrong_cnt_condition CONDITION;\n \nDECLARE EXIT HANDLER FOR wrong_cnt_condition\n  PRINT 'Wrong number of rows';  \n \nSELECT COUNT(*) INTO cnt FROM TABLE (VALUES (1,2));\n \nIF cnt <> 1 THEN\n  SIGNAL wrong_cnt_condition;\n","correct":false}]},{"q":"When working with HPL/SQL, you want to create a regular table in the database and automatically drop it at the end of the session.<br><br>What should you do to achieve this?","a":[{"id":1178813,"option":"Set hplsql.temp.tables to true","correct":false},{"id":1178814,"option":"Set hplsql.temp.tables to 1","correct":false},{"id":1178815,"option":"Set hplsql.temp.tables to managed","correct":true},{"id":1178816,"option":"Set hplsql.temp.tables to 0","correct":false}]},{"q":"You are using the hplsql.onerror configuration option to define how HPL/SQL handles errors in the current session. Assume that you have specified the Seterror value.<br><br>What does HPL/SQL do in the given scenario?","a":[{"id":1178789,"option":"It raises an exception when an error occurs","correct":false},{"id":1178790,"option":"It sets the error code to SQLCODE and ERRORCODE variables","correct":false},{"id":1178791,"option":"It raises exception when an error occurs and sets the error code to ERRORCODE variable","correct":false},{"id":1178792,"option":" It sets the error code to SQLCODE, ERRORCODE and HOSTCODE variables","correct":true}]},{"q":"You are using the DECLARE HANDLER statement to define one or more PL/HQL statements to execute when a condition occurs.<br><br>What is the control returned to when you use CONTINUE in the given scenario?<br>","a":[{"id":1178817,"option":"The PL/HQL statement following the statement that raised the condition","correct":true},{"id":1178818,"option":"The end of the block that declared the handler","correct":false},{"id":1178819,"option":"The PL/HQL statement(s) to execute when the specified condition occurs","correct":false},{"id":1178820,"option":"The PL/HQL statement that raised the condition","correct":false}]},{"q":"Assume that you are using a cursor when working with HPL/SQL. In the given context, which of the following statements is true?<br>S1: Each FETCH statement increments ACTIVITY_COUNT by 1.<br>S2: Each FETCH statement does not increment the ACTIVITY_COUNT.","a":[{"id":1178809,"option":"Only S1","correct":true},{"id":1178810,"option":"Only S2","correct":false},{"id":1178811,"option":"Both S1 and S2","correct":false},{"id":1178812,"option":"Neither S1 nor S2","correct":false}]},{"q":"You are using the SequenceFileOutputFormat FileFormat class to read and write HDFS files in Hive. <br><br>In the given context, what format are the two classes read/write data in?","a":[{"id":1178777,"option":" Hadoop SequenceFile format.","correct":true},{"id":1178778,"option":"Plain text file format.","correct":false},{"id":1178779,"option":" Hive IgnoreKey format.","correct":false},{"id":1178780,"option":" Hive IgnoreKeyText format.","correct":false}]},{"q":"You decide to use the following approaches to achieve efficient unit testing. Which of these approaches takes longer to run and generates more I/O?<br><br>1. Intermediate table solutions<br>2. Views<br>3. Restrict query optimization opportunities","a":[{"id":1178757,"option":"Only 1","correct":true},{"id":1178758,"option":"Only 2","correct":false},{"id":1178759,"option":"Only 3","correct":false},{"id":1178760,"option":"All 1, 2 and 3","correct":false}]},{"q":"&nbsp;&nbsp;&nbsp;&nbsp;What does Hive do when the query given alongside is executed?","a":[{"id":1178725,"option":"It converts the join into a single map/reduce job","correct":true},{"id":1178726,"option":"It converts the join into a two map/reduce job","correct":false},{"id":1178727,"option":"Either 1 or 2","correct":false},{"id":1178728,"option":"None of these","correct":false}]},{"q":"When working in Hive, you want to union the string type and a date type. What should be used in place of XXX in the query given alongside to successfully achieve this?","a":[{"id":1178733,"option":"XXX: cast('2001-01-01' as date)","correct":true},{"id":1178734,"option":"XXX: externalcast('2001-01-01' as date)","correct":false},{"id":1178735,"option":"XXX: internalcast('2001-01-01' as date)","correct":false},{"id":1178736,"option":"XXX: 2001-01-01'  castas date","correct":false}]},{"q":"You have two different tables in Hive that track which user has published a post and which user has published a comment. You want to create a single annotated stream for all the post publishing and comment publishing events.<br><br>Which of the following query can be used to achieve this?","a":[{"id":1178745,"option":"SELECT u.id, actions.date\nFROM (\n    SELECT av.uid AS uid\n    FROM action_video av\n    WHERE av.date = '2008-06-03'\n    UNION ALL\n    SELECT ac.uid AS uid\n    FROM action_comment ac\n    WHERE ac.date = '2008-06-03'\n ) ","correct":false},{"id":1178746,"option":"SELECT u.id, actions.date\nFROM (\n    SELECT av.uid AS uid\n    FROM action_video av\n    WHERE av.date = '2008-06-03'\n    UNION ALL\n    SELECT ac.uid AS uid\n    FROM action_comment ac\n    WHERE ac.date = '2008-06-03'\n ) actions JOIN users u ","correct":false},{"id":1178747,"option":"SELECT u.id, actions.date\nFROM (\n    SELECT av.uid AS uid\n    FROM action_video av\n    WHERE av.date = '2008-06-03'\n    UNION ALL\n    SELECT ac.uid AS uid\n    FROM action_comment ac\n    WHERE ac.date = '2008-06-03'\n ) actions JOIN users u ON (u.id = actions.uid)","correct":true},{"id":1178748,"option":"SELECT u.id, actions.date\nFROM (\n    SELECT av.uid AS uid\n    FROM action_video av\n    WHERE av.date = '2008-06-03'\n    UNION\n    SELECT ac.uid AS uid\n    FROM action_comment ac\n    WHERE ac.date = '2008-06-03'\n )JOIN users u ON (u.id = actions.uid)","correct":false}]},{"q":"The query given alongside does a dynamic partition insert in Hive. How is the country partition created in the given scenario?","a":[{"id":1178705,"option":"It will be dynamically","correct":false},{"id":1178706,"option":"It will be dynamically created by pvs.ip","correct":false},{"id":1178707,"option":"It will be dynamically created by pvs.cnt","correct":true},{"id":1178708,"option":"It will be created by pvs.cnt","correct":false}]},{"q":"Assume that the ZooKeeper Service Discovery Is enabled when working with HiveServer2. You want to provide additional runtime parameters in the JDBC connection URL for querying.<br><br>How can you do so?","a":[{"id":1178685,"option":"Append the parameter as ?<option> to the URL","correct":true},{"id":1178686,"option":"Append the parameter as $option to the URL","correct":false},{"id":1178687,"option":"Include it in the <zookeeper quorum>","correct":false},{"id":1178688,"option":"Include it in the hive.zookeeper.quorum configuration parameter","correct":false}]},{"q":"When working with Hive 2.1.0, you notice that Beeline reconnects to the last successfully connected URL even after !close has been run. <br><br>What happens when the user does a !save?","a":[{"id":1178689,"option":"Allows !reconnect to connect to this saved last-connected-to URL in the current Beeline session","correct":false},{"id":1178690,"option":"Allows !reconnect to connect to this saved last-connected-to URL across multiple Beeline sessions","correct":false},{"id":1178691,"option":"Allows the use of  beeline -r  from the command line to do a reconnect on startup.","correct":false},{"id":1178692,"option":"Both 2 and 3","correct":true}]},{"q":"You want to use JDBC to access data stored in a relational database when working with Hive. You are using the code snippet given alongside to submit SQL to the database. <br><br>Which of the following code snippets can be used in place of XXX in the given context?","a":[{"id":1178693,"option":"XXX:  stmt.executeQuery","correct":true},{"id":1178694,"option":"XXX:  stmt.queryExecute","correct":false},{"id":1178695,"option":"XXX:  stmt.ExecuteQuery","correct":false},{"id":1178696,"option":"XXX:  stmt.execute","correct":false}]},{"q":"You are only making a JDBC connection as a privileged user from a single blessed machine. <br><br>How can you pass the user you need to impersonate in the JDBC URL?","a":[{"id":1178697,"option":"Use direct proxy access method","correct":false},{"id":1178698,"option":"Impersonate in the JDBC URL by using the hive.server2.proxy.user=<user> parameter.","correct":true},{"id":1178699,"option":"Use the hive.server2.proxy.user=<user> parameter","correct":false},{"id":1178700,"option":"None of these","correct":false}]},{"q":"When working with Hive, you want to write data into the filesystem from queries. What will Hive do if the scheme or authority is not specified?","a":[{"id":1178713,"option":"It will use the scheme and authority from the hadoop configuration variable","correct":true},{"id":1178714,"option":"It will use the scheme and authority from the Namenode URI","correct":false},{"id":1178715,"option":"It will prompt asking for a scheme and authority ","correct":false},{"id":1178716,"option":"None of these","correct":false}]},{"q":"You notice that Hive CLI includes some commands that are not available in the HCatalog CLI. <br><br>In the given context, how does Hive and hcat use the -p flag?","a":[{"id":1178661,"option":"hcat uses the flag for permissions but hive uses it to specify a port number.","correct":true},{"id":1178662,"option":"hcat uses the flag to specify a port number but hive uses it for permissions","correct":false},{"id":1178663,"option":"hcat uses the flag to define key=value pairs but hive uses it for permissions","correct":false},{"id":1178664,"option":"hcat uses the flag to specify a port number but hive uses it to define key=value pairs","correct":false}]},{"q":"The JDBC connection URL for HiveServer2 when SSL is enabled is given alongside.<br><br>In the given context, where does the client's truststore file live?","a":[{"id":1178681,"option":"In <trust_store_path> ","correct":true},{"id":1178682,"option":"In sslTrustStore","correct":false},{"id":1178683,"option":"In the host","correct":false},{"id":1178684,"option":"In the trust_store","correct":false}]},{"q":"When working with HiveServer2, the JDBC connection URL format for a remote server is as shown alongside. <br><br>What can be said about the default port for HiveServer2 in the given scenario?","a":[{"id":1178677,"option":"There is no default port","correct":false},{"id":1178678,"option":"The default port is 10000","correct":true},{"id":1178679,"option":"The default port is 1000","correct":false},{"id":1178680,"option":"The default port is 100","correct":false}]},{"q":"You have created a table with a CLUSTERED BY clause when working with HCatalog DDL.When doing so, you realise that you are not able to write to the table with Pig or MapReduce.<br><br>What could be the reason for this?","a":[{"id":1178665,"option":" Because writing using Pig and MapReduce is not supported","correct":false},{"id":1178666,"option":" Because they do not know how to write without data corruption","correct":false},{"id":1178667,"option":" Because they do not understand how to partition the table","correct":true},{"id":1178668,"option":" Because they do not understand how to write in the partitions of the table","correct":false}]},{"q":"When working with Hive, you want to describe a table whose database is specified. <br><br>Which of the following queries is the right way to do so?","a":[{"id":1178645,"option":"DESCRIBE [EXTENDED|FORMATTED] \n  [db_name.]table_name[.col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ];","correct":false},{"id":1178646,"option":"DESCRIBE [EXTENDED|FORMATTED] \n  [db_name.]table_name[ col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ];","correct":true},{"id":1178647,"option":"DESCRIBE [EXTENDED|FORMATTED] \n  table_name[ col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ];","correct":false},{"id":1178648,"option":"DESCRIBE [EXTENDED|FORMATTED] \n  table_name[*col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ];","correct":false}]},{"q":"Assume the you have mentioned the following regular expressions in SHOW COLUMNS when working with Hive. <br><br>Which of these regular expressions matches with the column name 'game'?<br>1. gam*<br>2. *h|gam*","a":[{"id":1178653,"option":"Only 1","correct":false},{"id":1178654,"option":"Only 2","correct":false},{"id":1178655,"option":"Both 1 and 2","correct":true},{"id":1178656,"option":"Neither 1 nor 2","correct":false}]},{"q":"In a JDBC driver, no fetch size value is explicitly set on the JDBC driver's statement and the driver's default value is used.<br><br>What is the driver's default value used in the following cases?<br>1. The fetch size value is specified within the JDBC connection string.<br>2. The fetch size value is absent from the JDBC connection string.","a":[{"id":1178657,"option":"1. The fetch size value\n2. The value fetched by the server","correct":false},{"id":1178658,"option":"1. The fetch size value\n2. The server's preferred fetch size","correct":true},{"id":1178659,"option":"1. The server's preferred fetch size\n2. The fetch size value","correct":false},{"id":1178660,"option":"1. The value fetched by the server\n2. The fetch size value\n ","correct":false}]},{"q":"<p>You want to implement query results caching while working with Hive. Which of the following types of tables will you use to allow you to do so?</p>","a":[{"id":1178549,"option":"Managed table","correct":true},{"id":1178550,"option":"External table","correct":false},{"id":1178551,"option":"Both 1,2","correct":false},{"id":1178552,"option":"None of these","correct":false}]},{"q":"<p>You want to debug Hive code without going through Ant. Which of the following can be used to do so?</p>","a":[{"id":1178537,"option":" HADOOP_HOME","correct":true},{"id":1178538,"option":" HADOOP_OPTS","correct":false},{"id":1178539,"option":" HADOOP_CONFIG","correct":false},{"id":1178540,"option":" HADOOP_ENV","correct":false}]},{"q":"<p>You encounter an error while using AvroSerde during a MapReduce in Hive. Which of these can be used to collect more information on this issue from the failed task log?</p>","a":[{"id":1178533,"option":"JobTracker's web interface.","correct":true},{"id":1178534,"option":"TaskTracker's web interface. ","correct":false},{"id":1178535,"option":"Scheduler web interface. ","correct":false},{"id":1178536,"option":"Config Web Interface","correct":false}]},{"q":"<p>You get error-error-error-error-error-error-error and a message to check avro.schema.literal and avro.schema.url while describing a table in Hive using AvroSerDe.<br>\n<br>\n    Which of the following are valid reasons for the same?</p>","a":[{"id":1178529,"option":" AvroSerde has trouble finding or parsing the schema provided by either the avro.schema.literal or avro.avro.schema.url value.","correct":true},{"id":1178530,"option":" AvroSerde path is not added to the hadoop config","correct":false},{"id":1178531,"option":" AvroSerde is not added to Hadoop Lib","correct":false},{"id":1178532,"option":"None of these","correct":false}]},{"q":"<p>You are setting the executor memory size while configuring spark with Hive. Which of these considerations need to be kept in mind while doing so?<br>\n1.More executor memory means it can enable mapjoin optimization for more queries.<br>\n2.More executor memory, on the other hand, becomes unwieldy from GC perspective.<br>\n3. HDFS client doesn’t handle concurrent writers well, so it may face race condition if executor cores are too many.</p>","a":[{"id":1178513,"option":"Only 1,2","correct":false},{"id":1178514,"option":"Only 2,3","correct":false},{"id":1178515,"option":"Only 1,3","correct":false},{"id":1178516,"option":"All 1,2,3","correct":true}]},{"q":"<p>You are constructing IteratorSetting in Hive. Which of these classes will you be using while doing so?</p>","a":[{"id":1178509,"option":"PushdownTuple","correct":true},{"id":1178510,"option":"PrimitiveComparisonFilter","correct":false},{"id":1178511,"option":"AccumuloRangeGenerator","correct":false},{"id":1178512,"option":"None of these","correct":false}]},{"q":"<p>Which of these formats can be used in Hive to specify a custom distribution directory while using Hive?</p>","a":[{"id":1178505,"option":"ant -Dtarget.dir=<my-install-dir> package","correct":true},{"id":1178506,"option":"ant test -Dtestcase=TestCliDriver","correct":false},{"id":1178507,"option":"ant test -Dtestcase=TestCliDriver -Dqfile=groupby1.q","correct":false},{"id":1178508,"option":"None of these","correct":false}]},{"q":"<p>Which of these classes handles access to the actual metadata that is stored in the SQL store while working on Hive?</p>","a":[{"id":1178501,"option":"Metastore Server","correct":false},{"id":1178502,"option":"Object Store ","correct":true},{"id":1178503,"option":"Metastore Client","correct":false},{"id":1178504,"option":"Data store","correct":false}]},{"q":"<p>Which of these are valid limitations to be taken into consideration if HBase Schema bulk upload needs to be performed?</p>","a":[{"id":1178493,"option":"you can't bulk load into an existing table","correct":false},{"id":1178494,"option":"The target table can only have a single column family","correct":false},{"id":1178495,"option":"The target table cannot be sparse","correct":false},{"id":1178496,"option":"All of these","correct":true}]},{"q":"<p>Data appended to tables or partitions using the HCatalogWriters is not replicated automatically. Which of these can be considered as valid reasons for the same?</p>","a":[{"id":1178489,"option":"Because metastore notifications are not generated ","correct":true},{"id":1178490,"option":"Because metastore does not support table events","correct":false},{"id":1178491,"option":"Because partitions in metastore are fixed","correct":false},{"id":1178492,"option":"None of these","correct":false}]},{"q":"<p>If you want to source events from the metastore database for replication which of the following need to be used?</p>","a":[{"id":1178485,"option":"DbNotificationListener","correct":true},{"id":1178486,"option":" MetaStoreEventListener","correct":false},{"id":1178487,"option":" EventListener","correct":false},{"id":1178488,"option":" MetaStoreCallListener","correct":false}]},{"q":"<p>Which of these configuration variables in WebHCat contain the secret used to sign the HTTP cookie value?</p>","a":[{"id":1178477,"option":"templeton.kerberos.secret","correct":true},{"id":1178478,"option":"WebHCat.kerberos.secret","correct":false},{"id":1178479,"option":"WebHCat.cookie.secret","correct":false},{"id":1178480,"option":"templeton.auth.secret","correct":false}]},{"q":"<p>Which of the following Hive DDL are not supported in HCatalog?</p>","a":[{"id":1178473,"option":"ALTER INDEX ","correct":false},{"id":1178474,"option":"ALTER TABLE ","correct":false},{"id":1178475,"option":"ANALYZE TABLE","correct":false},{"id":1178476,"option":"All of these","correct":true}]},{"q":"<p>Which of the following configurations need to be made to run Metastore in standalone mode in Hive 3.0?</p>","a":[{"id":1178465,"option":"metastore.expression.proxy","correct":false},{"id":1178466,"option":"metastore.task.threads.always","correct":false},{"id":1178467,"option":"metastore.task","correct":false},{"id":1178468,"option":"Both Choice 1,2","correct":true}]},{"q":"<p>Which of these problems can arise from using Embedded mode in Hive?</p>","a":[{"id":1178461,"option":" having many clients will put a burden on the backing RDBMS ","correct":false},{"id":1178462,"option":"It is hard to properly secure the RDBMS.  ","correct":false},{"id":1178463,"option":"Both Choice 1,2","correct":true},{"id":1178464,"option":"None of these","correct":false}]},{"q":"<p>You want to set your Unit tests in Hive to run faster. Which of these configuration parameters in Hive can be set to true to do so?</p>","a":[{"id":1178457,"option":"hive.exec.submit.local.task.via.child ","correct":true},{"id":1178458,"option":"hive.test.mode.nosamplelist","correct":false},{"id":1178459,"option":"hive.test.mode.samplefreq","correct":false},{"id":1178460,"option":"hive.test.mode","correct":false}]},{"q":"<p>You are configuring which hosts the test should run on while setting up test nodes in Hive. You should put<br>\nyour configuration file in which of these files to do so?</p>","a":[{"id":1178453,"option":" \"~/.hive_ptest.conf\".","correct":true},{"id":1178454,"option":" \"~/.hive_config.conf\".","correct":false},{"id":1178455,"option":" \"~/.hive_test.log\".","correct":false},{"id":1178456,"option":" \"~/.hive_ssh.conf\".","correct":false}]},{"q":"<p>You are configuring the Hive metastore. Which of these configuration parameters will be used to specify the URI of the default location for native tables?</p>","a":[{"id":1178449,"option":"hive.metastore.warehouse.dir","correct":true},{"id":1178450,"option":"hive.metastore.local","correct":false},{"id":1178451,"option":"hive.metastore.uris","correct":false},{"id":1178452,"option":"None of these","correct":false}]},{"q":"<p>Which of these commands can be used to display the default list for the current release in Hive 0.13?</p>","a":[{"id":1178441,"option":"set hive.security.authorization.sqlstd.confwhitelist","correct":true},{"id":1178442,"option":"set hive.security.authorization.sqlstd.config'","correct":false},{"id":1178443,"option":"set hive.security.authorization.sqlstd.list'","correct":false},{"id":1178444,"option":"set hive.security.authorization.sqlstd.current'","correct":false}]},{"q":"<p>You want to perform custom parsing of the rowid column into a LazyObject. Which of these extension points will be used to do so?</p>","a":[{"id":1178437,"option":"accumulo.composite.rowid","correct":true},{"id":1178438,"option":"accumulo.table.name","correct":false},{"id":1178439,"option":"accumulo.visibility.label","correct":false},{"id":1178440,"option":"accumulo.iterator.pushdown","correct":false}]},{"q":"<p>The AccumuloStorageHandler can be converted via which of the following in Hive?<br>\n1.SERDEPROPERTIES<br>\n2.TBLPROPERTIES</p>","a":[{"id":1178433,"option":"Only 1","correct":false},{"id":1178434,"option":"Only 2","correct":false},{"id":1178435,"option":"Both 1,2","correct":true},{"id":1178436,"option":"None of these","correct":false}]},{"q":"<p>You get the error shown alongside while running a Hive query. Assume that you have integrated spark to Hive already and are working in Mac.</p>\n\n<pre class=\"prettyprint\"><code>FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask</code></pre>\n\n<p><br>\nWhich of these commands can be run to resolve this issue?</p>\n\n<p> </p>","a":[{"id":1178525,"option":"export HADOOP_OPTS=\"-Dorg.xerial.snappy.tempdir=/tmp -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib $HADOOP_OPTS","correct":true},{"id":1178526,"option":"config HADOOP_OPTS=\"-Dorg.xerial.task.tempdir=/tmp -Dorg.xerial.task.lib.name=libsnappyjava.jnilib $HADOOP_OPTS","correct":false},{"id":1178527,"option":"config HADOOP_ENV=\"-Dorg.xerial.task.tempdir=/tmp -Dorg.xerial.task.lib.name=libsnappyjava.jnilib $HADOOP_ENV","correct":false},{"id":1178528,"option":"None of these","correct":false}]},{"q":"<p>You encounter the error shown alongside while integrating spark with Hive. Which of the following are valid fixes for this issue?</p>\n\n<pre class=\"prettyprint\"><code>Error: Could not find or load main class org.apache.spark.deploy.SparkSubmit</code></pre>\n\n<p> </p>","a":[{"id":1178517,"option":"Add Spark dependency to Hive","correct":true},{"id":1178518,"option":"configure spark.executor.instances ","correct":false},{"id":1178519,"option":"Increase yarn.nodemanager.resource.cpu-vcores value","correct":false},{"id":1178520,"option":"None of these","correct":false}]},{"q":"<p>You receive the error given alongside while trying to perform a sort operation in Hive. Which of these can be valid reasons for the same?</p>\n\n<pre class=\"prettyprint\"><code>\"Wrong number of partitions in keyset\" error.</code></pre>\n\n<p> </p>","a":[{"id":1178497,"option":"Number of reduce tasks is incorrectly configured","correct":true},{"id":1178498,"option":"Compression is not set","correct":false},{"id":1178499,"option":"HBase schema is not loaded correctly","correct":false},{"id":1178500,"option":"None of these","correct":false}]},{"q":"<p>Assume that the configuration variable shown alongside is not set while working with WebHCat in Hive. What is likely to occur in this scenario?</p>\n\n<pre class=\"prettyprint\"><code>templeton.mapper.memory.mb</code></pre>\n\n<p> </p>","a":[{"id":1178481,"option":"configuration in mapred-site.xml will be used.","correct":true},{"id":1178482,"option":"configuration in mapred-config.xml will be used.","correct":false},{"id":1178483,"option":"configuration in data.xml will be used.","correct":false},{"id":1178484,"option":"configuration in index-site.xml will be used.","correct":false}]},{"q":"<p>You are using the set = command in a HiveQL script to set the value of a particular configuration variable (key).<br>\n<br>\nWhich of the following is true with respect to the given scenario?</p>","a":[{"id":1176931,"option":"If the variable name is misspelled then the CLI will not show an error.","correct":true},{"id":1176932,"option":"If the variable name is misspelled then the CLI will not show an error.","correct":false},{"id":1176933,"option":"If the variable name is misspelled then the CLI automatically corrects it. ","correct":false},{"id":1176934,"option":"None of these","correct":false}]},{"q":"<p>A table is created in Hive without the EXTERNAL clause. What can be said about such a table?</p>","a":[{"id":1176919,"option":"The table is called a managed table","correct":true},{"id":1176920,"option":"The table is called a extended table","correct":false},{"id":1176921,"option":"The table is called a describe table","correct":false},{"id":1176922,"option":"Both 1 and 2`","correct":false}]},{"q":"<p>You want to run the command $HIVE_HOME/bin/hive such that it executes the SQL commands in batch mode.<br>\n<br>\nHow should you run the batch mode command in order to achieve this?</p>","a":[{"id":1176915,"option":"Run the command with a -e option","correct":false},{"id":1176916,"option":"Run the command with a -f option","correct":false},{"id":1176917,"option":"Run the command with a -g option","correct":false},{"id":1176918,"option":"Either 1 or 2","correct":true}]},{"q":"<p>Which of these Hive shell commands can be used to list the resources that are already added to the distributed cache?<br>\n1. list FILE[S]<br>\n2. list FILE[S] *<br>\n3. list FILE[S] *</p>","a":[{"id":1176907,"option":"Only 1 ","correct":true},{"id":1176908,"option":"Only 2","correct":false},{"id":1176909,"option":"Only 3","correct":false},{"id":1176910,"option":"Both 1 and 2","correct":false}]},{"q":"<p>Which of these parameters passed to the ADD command in Hive takes the value true or false?</p>","a":[{"id":1176903,"option":"exclude","correct":false},{"id":1176904,"option":"transitive","correct":true},{"id":1176905,"option":"ext","correct":false},{"id":1176906,"option":"classifier","correct":false}]},{"q":"<p>You notice that when using the Load operation in Hive 3.0, the target to be loaded is a table that is partitioned.<br>\n<br>\nWhat should you specify in the given scenario?</p>","a":[{"id":1176899,"option":"A specific partition of the table must be specified","correct":true},{"id":1176900,"option":"The specific values of the table must be specified","correct":false},{"id":1176901,"option":"The specific partitioning columns of the table must be specified","correct":false},{"id":1176902,"option":"The specific values for all the partitioning columns must be specified","correct":false}]},{"q":"<p>Which of the following tasks are part of the Map tasks in a Hive MAPJOIN?<br>\n1. Write hashtable to local disk<br>\n2. Read hashtable from local disk<br>\n3. Upload hashtable to dfs</p>","a":[{"id":1176895,"option":"Only 1 ","correct":false},{"id":1176896,"option":"Only 2","correct":true},{"id":1176897,"option":"Only 3","correct":false},{"id":1176898,"option":"All 1, 2 and 3","correct":false}]},{"q":"<p>What is stored in the target directory when the EXPORT command is used in Hive?</p>","a":[{"id":1176891,"option":"Exported metadata","correct":true},{"id":1176892,"option":"Data files","correct":false},{"id":1176893,"option":"Table properties","correct":false},{"id":1176894,"option":"Table partitions","correct":false}]},{"q":"<p>Which of the following can be used as an alternative to Indexing when working with Data Definition Statements in Hive 3.0?</p>","a":[{"id":1176887,"option":"Materialized views with manual rewriting ","correct":false},{"id":1176888,"option":"Columnar file formats","correct":false},{"id":1176889,"option":"Materialized views with automatic rewriting ","correct":false},{"id":1176890,"option":"Both 2 and 3","correct":true}]},{"q":"<p>Which of the following is NOT true about Hive HPL/SQL?<br>\n1. HPL/SQL can efficiently implement ETL processes in Hadoop.<br>\n2. HPL/SQL is a homogenous language that understands syntaxes and semantics of any procedural SQL dialect.</p>","a":[{"id":1176863,"option":"Only 1","correct":false},{"id":1176864,"option":"Only 2","correct":true},{"id":1176865,"option":"Both 1 and 2","correct":false},{"id":1176866,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>A XPath expression string is compiled and cached in Hive. In which of these scenarios is this expression reused?</p>","a":[{"id":1176859,"option":"The XPath expression in the next input row matches the previous expression","correct":false},{"id":1176860,"option":"The XPath expression in the next input row does not match the previous expression","correct":false},{"id":1176861,"option":"The XPath expression in the next input row matches all the previous expressions","correct":true},{"id":1176862,"option":"The XPath expression in the next input row does not match all the previous expression","correct":false}]},{"q":"<p>When working in Hive, a user wants to set HBase as the implementation of temporary statistics storage.<br>\n<br>\nWhich of these commands should the user issue to achieve this?</p>","a":[{"id":1176855,"option":"set hive.stats.dbclass=hbase;","correct":true},{"id":1176856,"option":"set hive.stats.autogather=hbase;","correct":false},{"id":1176857,"option":"set hive.stats.class=hbase;","correct":false},{"id":1176858,"option":"None of these","correct":false}]},{"q":"<p>In which of following statements can Unions be used when working in Hive?<br>\n1. Views<br>\n2. Inserts<br>\n3. Create table as select</p>","a":[{"id":1176851,"option":"Only 1 and 2","correct":false},{"id":1176852,"option":"Only 1 and 3","correct":false},{"id":1176853,"option":"Only 2 and 3","correct":false},{"id":1176854,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>What will the integral type for the given integral literal be?<br>\n1. 200Y<br>\n2. 200S</p>","a":[{"id":1176839,"option":"1. TINYINT\r\n2. SMALLINT","correct":true},{"id":1176840,"option":"1. BIGINT\r\n2. SMALLINT","correct":false},{"id":1176841,"option":"1. SMALLINT\r\n2. TINYINT","correct":false},{"id":1176842,"option":"1. BIGINT\r\n2. TINYINT","correct":false}]},{"q":"<p>When working with the load operation in Hive 3.0, the keyword LOCAL is not given.<br>\n<br>\nIn the given context, what must the filepath refer to?</p>","a":[{"id":1176835,"option":" Files within the same filesystem as the table's or partition's directory","correct":false},{"id":1176836,"option":" Files within the same filesystem as the partition's location.","correct":false},{"id":1176837,"option":" Files within the same filesystem as the table's location.","correct":true},{"id":1176838,"option":"None of these","correct":false}]},{"q":"<p>A relative path is specified when performing a load operation in Hive 3.0.<br>\n<br>\nHow will this path be interpreted?</p>","a":[{"id":1176831,"option":"Relative to the user's current working directory.","correct":true},{"id":1176832,"option":"Relative to the current working directory.","correct":false},{"id":1176833,"option":"Relative to the user's recently used working directory","correct":false},{"id":1176834,"option":"Relative to the user's frequently used directory.","correct":false}]},{"q":"<p>You want to restrict the output of a join in Hive. What can you do to achieve this?</p>","a":[{"id":1176827,"option":"The requirement should be in the WHERE clause","correct":false},{"id":1176828,"option":"The requirement should be in the JOIN clause","correct":false},{"id":1176829,"option":"The requirement should be in both WHERE and JOIN clause","correct":false},{"id":1176830,"option":"The requirement should be in WHERE or JOIN clause","correct":true}]},{"q":"<p>Which of the following are valid when using the Hive joins?<br>\n1. More than 2 tables can be joined in the same query.<br>\n2. Only 2 tables can be joined in the same query.<br>\n3. LEFT, RIGHT, and FULL OUTER joins exist</p>","a":[{"id":1176823,"option":"All 1, 2 and 3","correct":false},{"id":1176824,"option":"Only 1 and 3","correct":true},{"id":1176825,"option":"Only 1 and 2","correct":false},{"id":1176826,"option":"Only 2 and 3","correct":false}]},{"q":"<p>    You are using the LIMIT clause in Hive to constrain the number of rows returned by the SELECT statement.<br>\n<br>\nHow many arguments does the LIMIT clause take?</p>","a":[{"id":1176819,"option":"One numeric argument","correct":false},{"id":1176820,"option":"Two numeric arguments","correct":false},{"id":1176821,"option":"Three numeric arguments","correct":true},{"id":1176822,"option":"Either 1 or 2","correct":false}]},{"q":"<p>It is given that quoting is disabled for the csv and tsv output formats in Beeline.<br>\n<br>\nWhat can be said about quoting in the given context?</p>","a":[{"id":1176811,"option":"The embedded single quotes applied before disabling the quotes are not removed","correct":false},{"id":1176812,"option":"The embedded single quotes are removed from the output format","correct":false},{"id":1176813,"option":"The output formats don't escape the embedded single quotes.","correct":true},{"id":1176814,"option":"None of these","correct":false}]},{"q":"<p>How can quoting be disabled in the separated-value output formats in Beeline?</p>","a":[{"id":1176807,"option":"Set the disable.quoting.for.sv system variable to true.","correct":true},{"id":1176808,"option":"Set the disable.quoting.sv system variable to true.","correct":false},{"id":1176809,"option":"Set the enable.quoting.for.sv system variable to false.","correct":false},{"id":1176810,"option":"Set the enable.quoting.sv system variable to false.","correct":false}]},{"q":"<p>What should be the value set to HiveServer2 Web UI in order to disable the HiveServer2 Web UI feature?</p>","a":[{"id":1176675,"option":"0","correct":false},{"id":1176676,"option":"-1","correct":false},{"id":1176677,"option":"1","correct":false},{"id":1176678,"option":"Both Choice 1 and Choice 2","correct":true}]},{"q":"<p>You want to test parallel execution on Hive using python modules. Which of these python modules would you use to do so?</p>","a":[{"id":1176655,"option":"argparse","correct":false},{"id":1176656,"option":"mako","correct":false},{"id":1176657,"option":"Synchronize","correct":false},{"id":1176658,"option":"Both Choice 1 and Choice 2","correct":true}]},{"q":"<p>In Hadoop, <strong>MapReduce</strong> is a program model that is based on Java and is used for distributed computing. You are executing the <strong>MapReduce </strong>program. If the <strong>mapper </strong>function processes data and creates several small chunks of data, then which of the following stages of this program is being represented in this scenario?</p>","a":[{"id":1110039,"option":"Shuffle stage","correct":false},{"id":1110040,"option":"Reduce stage","correct":false},{"id":1110041,"option":"Map stage","correct":true},{"id":1110042,"option":"Payload Map stage","correct":false}]},{"q":"<p>In Hadoop, you are working on the <strong>HBaseAdmin</strong> class. Which of the following methods will you use to get an instance of the admin?</p>","a":[{"id":1110031,"option":"hadoop.Connection.getAdmin(HTableDescriptor, desc)","correct":false},{"id":1110032,"option":"Connection.getAdmin(HTableDescriptor, desc)","correct":false},{"id":1110033,"option":"Connection.getAdmin()","correct":true},{"id":1110034,"option":"hadoop.Connection.getAdmin()","correct":false}]},{"q":"<p>Which of the following options is a core component of Big Data?</p>","a":[{"id":170964,"option":"YARN","correct":false},{"id":170965,"option":"HBase","correct":false},{"id":170966,"option":"Chukwa","correct":false},{"id":170967,"option":"All of these","correct":true}]},{"q":"<p>Which of the following is a data-storage component?</p>","a":[{"id":170974,"option":"HBase","correct":true},{"id":170975,"option":"Sqoop","correct":false},{"id":170976,"option":"Chukwa","correct":false},{"id":170977,"option":"Apache Flume","correct":false}]},{"q":"<p>In Hadoop, which of these HDFS files is also a directory on the file system in which the secondary NameNode stores temporary images of edit logs where EditLogs and the FsImage will merge for backup?</p>","a":[{"id":745402,"option":"dfs.name.dir","correct":false},{"id":745403,"option":"dfs.data.dir","correct":false},{"id":745404,"option":"fs.checkpoint.dir","correct":true},{"id":745405,"option":"hdfs.logs.dir","correct":false}]},{"q":"<p>Which of these statements about the <strong>speculative execution</strong> in Hadoop is correct? </p>","a":[{"id":745358,"option":"If a node appears to be executing a task slower, then the master node can randomly execute another instance of the same task on another node. The task which finishes first will be accepted and the other one is killed.","correct":true},{"id":745359,"option":"If a node appears to be executing a task slower, then the slave node can randomly execute another instance of the same task on another node. The task which finishes last will be accepted and the other one is killed. ","correct":false},{"id":745360,"option":"If a node appears to be executing a task slower, the slave node can randomly execute another instance of the same task on another node. The task which finishes first will be accepted and the other one is killed.","correct":false},{"id":745361,"option":"If a node appears to be executing a task faster, the master node can randomly execute another instance of the same task on another node. The task which finishes last will be accepted and the first one gets killed. ","correct":false}]},{"q":"<p>Which of these core methods of Reducer is used for configuring various parameters like input data size, distributed cache.</p>","a":[{"id":745370,"option":"setup()","correct":true},{"id":745371,"option":"reduce()","correct":false},{"id":745372,"option":"cleanup()","correct":false},{"id":745373,"option":"configure()","correct":false}]},{"q":"<p>Which of these file systems are supported by Spark:</p>\n\n<ol>\n\t<li>Hadoop Distributed File System (HDFS)</li>\n\t<li>Local file system</li>\n\t<li>Amazon S3</li>\n</ol>","a":[{"id":730738,"option":"Only 1","correct":false},{"id":730739,"option":"1 and 2","correct":false},{"id":730740,"option":"2 and 3","correct":false},{"id":730741,"option":"All of these","correct":true}]},{"q":"<p>In Hadoop, which of these cluster managers in Spark is responsible for resource management?</p>","a":[{"id":730730,"option":"Standalone ","correct":false},{"id":730731,"option":"Apache Mesos","correct":false},{"id":730732,"option":"YARN","correct":true},{"id":730733,"option":"Akka","correct":false}]},{"q":"<p>In Spark SQL, which of these data sources are available?</p>","a":[{"id":730726,"option":"Parquet file","correct":false},{"id":730727,"option":"JSON datasets","correct":false},{"id":730728,"option":"Hive tables","correct":false},{"id":730729,"option":"All of these","correct":true}]},{"q":"<p>In Hadoop, if a field type is not specified, which of the following is the default datatype in<strong> Pig</strong>?</p>","a":[{"id":183531,"option":"Chararray","correct":false},{"id":183532,"option":"byteArray","correct":true},{"id":183533,"option":"int","correct":false},{"id":183534,"option":"long","correct":false}]},{"q":"<p>Which of the following architectures is used in the Hadoop Distributed File System?</p>","a":[{"id":183708,"option":"Worker-slave","correct":false},{"id":183709,"option":"Master-master","correct":false},{"id":183710,"option":"Peer to peer","correct":false},{"id":183711,"option":"Master-slave","correct":true}]},{"q":"<p>Which of the following platforms is HBase is built on?</p>","a":[{"id":183507,"option":"RDBMS ","correct":false},{"id":183508,"option":"Pig ","correct":false},{"id":183509,"option":"Giraph ","correct":false},{"id":183510,"option":"HDFS","correct":true}]},{"q":"<p>In Hadoop, which of the following is a collection of about 40 nodes that are physically stored close together and are connected to the same network switch?</p>","a":[{"id":260577,"option":"A cluster","correct":false},{"id":260578,"option":"A rack","correct":true},{"id":260579,"option":"An input splitter","correct":false},{"id":260580,"option":"A combiner","correct":false}]},{"q":"<p>Which of the following is the default block size of HDFS?</p>","a":[{"id":183555,"option":"256MB","correct":false},{"id":183556,"option":"16MB","correct":false},{"id":183557,"option":"64MB","correct":true},{"id":183558,"option":"20MB","correct":false}]},{"q":"<p>In Hadoop, which of the following commands is used to display the version?</p>","a":[{"id":183684,"option":"hadoop ver","correct":false},{"id":183685,"option":"hadoop version","correct":true},{"id":183686,"option":"version","correct":false},{"id":280715,"option":"hadoop -v","correct":false}]},{"q":"<p>Which of the following is the workflow scheduler and the manager of Hadoop?</p>","a":[{"id":183926,"option":"HBase","correct":false},{"id":183927,"option":"Zookeeper","correct":false},{"id":183928,"option":"Oozie","correct":true},{"id":183929,"option":"Hive","correct":false}]},{"q":"<p>In Hadoop, which of the following sends <strong>heartbeat</strong> signals in HDFS?</p>","a":[{"id":260589,"option":"DataNode and TaskTracker send heartbeat signals to NameNode only","correct":false},{"id":260590,"option":"DataNode and TaskTracker send heartbeat signals to NameNode and JobTracker respectively","correct":true},{"id":260591,"option":"DataNode and TaskTracker send heartbeat signals to JobTracker only","correct":false},{"id":260592,"option":"DataNode and TaskTracker send heartbeat signals to JobTracker and NameNode respectively","correct":false}]},{"q":"<p>In Hadoop, which of the following is the <strong>default partitioner</strong> used to partition keys of the intermediate mapper output?</p>","a":[{"id":171494,"option":"Hash","correct":false},{"id":171495,"option":"Partitioner","correct":false},{"id":171496,"option":"HashPartitioner","correct":true},{"id":171497,"option":"All of these","correct":false}]},{"q":"<p>John is using Hadoop framework for his clients. He is using MapReduce to achieve parallel pragramming. Then which of the following failures is considered fatal for MapReduce jobs to stop executing?</p>","a":[{"id":172144,"option":"JobTracker failure","correct":true},{"id":172145,"option":"Child-task failure","correct":false},{"id":172146,"option":"TaskScheduler failure","correct":false},{"id":172147,"option":"TaskTracker failure","correct":false}]},{"q":"<p>Mike is using Hadoop framework for his clients. Then the global counter defined by the applications is of which type?</p>","a":[{"id":45448,"option":"Struct","correct":false},{"id":45449,"option":"Union","correct":false},{"id":45450,"option":"Enum","correct":false},{"id":45451,"option":"Integer","correct":true}]},{"q":"<p>Mary is using Hadoop framework for his applications. Then which of the following options can he use for his application progress?</p>","a":[{"id":172094,"option":"InputSplit","correct":false},{"id":172095,"option":"OutputSplit","correct":false},{"id":172096,"option":"Reporter","correct":true},{"id":172097,"option":"Tracker","correct":false}]},{"q":"Assume that you are working with Transform/Mapreduce in Hive. How does the standard output of a user script treat the columns and cells with the following information?<br>&nbsp;&nbsp;&nbsp;&nbsp;1. Columns with values<br>2. Cells with only \\N","a":[{"id":1179117,"option":"Both 1 and 2 are reinterpreted as NULL","correct":false},{"id":1179118,"option":"Both 1 and 2 are reinterpreted as TAB-separated STRING columns\n","correct":false},{"id":1179119,"option":"1. TAB-separated STRING columns\n2. NULL","correct":true},{"id":1179120,"option":"1. NULL\n2. TAB-separated STRING columns","correct":false}]},{"q":"<p>Read the statement carefully and choose the correct option.<br>\nS1: The ALTER DATABASE ... SET LOCATION statement does not move the contents of the database's current directory to the newly specified location.<br>\nS2: The ALTER DATABASE ... SET MANAGEDLOCATION statement moves the contents of the database's managed tables directories to the newly specified location.</p>","a":[{"id":1176935,"option":"S1 is true and S2 is true","correct":false},{"id":1176936,"option":"S1 is true and S2 is false","correct":true},{"id":1176937,"option":"S1 is false and S2 is true","correct":false},{"id":1176938,"option":"S1 is false and S2 is false","correct":false}]},{"q":"<p>You want to set the current database for all subsequent HiveQL statements.<br>\n<br>\nWhich of the following statements can you use to do so?</p>","a":[{"id":1176927,"option":"USE database_name;","correct":true},{"id":1176928,"option":"USE default;","correct":false},{"id":1176929,"option":"USE;","correct":false},{"id":1176930,"option":"USE default_database;","correct":false}]},{"q":"<p>Read the given statements carefully and choose the correct option.<br>\nS1: String literals can be expressed with single quotes (') in Hive.<br>\nS2: String literals can be expressed with double quotes (\") in Hive.</p>","a":[{"id":1176867,"option":"S1 is true and S2 is false","correct":false},{"id":1176868,"option":"S1 is false and S2 is true","correct":false},{"id":1176869,"option":"S1 is true and S2 is true","correct":true},{"id":1176870,"option":"S1 is false and S2 is false","correct":false}]},{"q":"<p>In Hadoop, which of the following statements about the <strong>ZooKeeper</strong> open source project are correct:</p>\n\n<ol>\n\t<li><strong>ZooKeeper </strong>contains ephemeral nodes representing different region servers.</li>\n\t<li><strong>ZooKeeper’s</strong> nodes are used to track server failures or network partitions.</li>\n\t<li>Clients communicate with region servers through <strong>MasterServer</strong> of <strong>ZooKeeper</strong>.</li>\n</ol>","a":[{"id":1110035,"option":"1 and 2","correct":true},{"id":1110036,"option":"2 and 3","correct":false},{"id":1110037,"option":"1 and 3","correct":false},{"id":1110038,"option":"All of these","correct":false}]},{"q":"<p>In Hadoop, which of the following is used to classify data?</p>","a":[{"id":170869,"option":"MapReduce, Hive, and HBase","correct":true},{"id":170870,"option":"NoSQL, MySQL, and Apps","correct":false},{"id":170871,"option":"MapReduce, Hummer, and Iguana","correct":false},{"id":170872,"option":"MapReduce, Heron, and Trumpet","correct":false}]},{"q":"<p>Which of the following are the components of Hadoop Big Data?</p>","a":[{"id":183878,"option":"MapReduce, Hive, and HBase","correct":true},{"id":183879,"option":"NoSQL, MySQL, and Apps\r\n","correct":false},{"id":183880,"option":"MapReduce, Hummer, and Iguana\r\n","correct":false},{"id":183881,"option":"MapReduce, Heron, and Trumpet","correct":false}]},{"q":"<p>Which of the following statements about <strong>HBase </strong> is not correct?</p>","a":[{"id":183519,"option":"HBase is not an ACID-compliant database","correct":false},{"id":183520,"option":"HBase does not support a full relational model","correct":false},{"id":183521,"option":"HBase provides clients with a simple data model","correct":false},{"id":183522,"option":"All of these","correct":true}]},{"q":"<p>Which of the following enables bidirectional data transfer between Hadoop and a relational database?</p>","a":[{"id":45444,"option":"Oozie","correct":false},{"id":45445,"option":"Sqoop","correct":true},{"id":45446,"option":"FlumeNG","correct":false},{"id":45447,"option":"Zookeeper","correct":false}]},{"q":"<p>In Hadoop, which of these commands compresses the output of the Mapper function&nbsp;without compressing the output of&nbsp;the Reducer function?&nbsp;</p>\n\n<p>Mike is using Hadoop framework for his clients. Then, what command should be used by him to compress the output generated by the mapper without reducing the output of generated by reducer?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>conf.set(\"mapreduce.map.output.compress\", false)\nconf.set(\"mapreduce.output.fileoutputformat.compress\", true)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>conf.set(\"mapreduce.reduce.output.compress\", true)\nconf.set(\"mapreduce.output.fileoutputformat.compress\", true)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>conf.set(\"mapreduce.reduce.output.compress\", true)\nconf.set(\"mapreduce.output.fileoutputformat.compress\", false)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>conf.set(\"mapreduce.map.output.compress\", true)\nconf.set(\"mapreduce.output.fileoutputformat.compress\", false)</code></pre>\n\t</li>\n</ol>","a":[{"id":745390,"option":"1","correct":false},{"id":745391,"option":"2","correct":false},{"id":745392,"option":"3","correct":false},{"id":745393,"option":"4","correct":true}]},{"q":"<p>In Hadoop, which of the commands is used to check the&nbsp;health status of FileSystem?</p>","a":[{"id":171869,"option":"hdfs fsck / -files –blocks –locations","correct":false},{"id":171870,"option":"hdfs fsck / -files –blocks –locations > dfs-fsck.log","correct":true},{"id":171871,"option":"hdfs fsck <path> -files -blocks dfs-fsck.log","correct":false},{"id":171872,"option":"hdfs fsck <path> -files -blocks","correct":false}]},{"q":"<p>Mike is using Hadoop framework for his clients. He is asked to process large volumes of data. Then which of the following uses the output of mapper as its input?</p>","a":[{"id":171509,"option":"Reducer","correct":true},{"id":171510,"option":"Mapper","correct":false},{"id":171511,"option":"RecordWriter","correct":false},{"id":171512,"option":"Sorter","correct":false}]},{"q":"<p>Which of the following requirements must be satisfied to use streaming in Hive:</p>\n\n<ol>\n\t<li>Only the ORC storage format is supported.</li>\n\t<li>The Hive table must be bucketed but not sorted.</li>\n</ol>","a":[{"id":1176715,"option":"1","correct":false},{"id":1176716,"option":"2","correct":false},{"id":1176717,"option":"Both of these","correct":true},{"id":1176718,"option":"None of these","correct":false}]},{"q":"<p>You are invoking Hive using the following code. Which of these configurations can be used to perform the same configuration in Hive by configuring the <strong>HIVE_OPTS</strong> setting:</p>\n\n<pre class=\"prettyprint\"><code>$ bin/hive --hiveconf x1=y1 --hiveconf x2=y2\n$ bin/hiveserver2 --hiveconf x1=y1 --hiveconf x2=y2  \n$ bin/beeline --hiveconf x1=y1 --hiveconf x2=y2</code></pre>\n\n<p> </p>","a":[{"id":1176619,"option":"\"--hiveconf x1=y1 --hiveconf x2=y2\" ","correct":true},{"id":1176620,"option":"\"--hive-init= x2=y2\" ","correct":false},{"id":1176621,"option":"\"--hive-init= x2-y2\" ","correct":false},{"id":1176622,"option":"\"--hive-init= x2y2\" ","correct":false}]},{"q":"<p>You have created a Hive table as shown alongside to access Kudu tables. What should be used in place of YYY to inform Hive that Kudu will back this Hive table?</p>\n\n<pre class=\"prettyprint\"><code>CREATE EXTERNAL TABLE kudu_table (foo INT, bar STRING, baz DOUBLE)\nYYY\nTBLPROPERTIES (\n  \"kudu.table_name\"=\"default.kudu_table\", \n  \"kudu.master_addresses\"=\"localhost:7051\"\n);</code></pre>\n\n<p> </p>","a":[{"id":1179181,"option":"YYY: STORED BY 'org.apache.hadoop.hive.kudu.KuduStorageHandler'","correct":true},{"id":1179182,"option":"YYY: STORED BY 'org.apache.hadoop.hive.kudu.KuduPredicateHandler'","correct":false},{"id":1179183,"option":"YYY: ALTERED BY 'org.apache.hadoop.hive.kudu.KuduPredicateHandler'","correct":false},{"id":1179184,"option":"YYY: ALTERED BY 'org.apache.hadoop.hive.kudu.KuduStorageHandler'","correct":false}]},{"q":"You have a base table named Ads which has two columns pid and ad_list. <br>1. The column pid is of type STRING and consists the name of the page.<br>2. The column ad_list is of type Array<int> and consists array of ads appearing on the page.<br><br>A user wants to count the total number of times an ad appears across all pages. In the given scenario, which of the following queries can be used to achieve this?","a":[{"id":1179157,"option":"SELECT pid, adid\nFROM Ads VIEW (adid_list) adTable AS adid;","correct":false},{"id":1179158,"option":"SELECT pid, adid\nFROM Ads LATERAL VIEW (adid_list) adTable AS adid;","correct":false},{"id":1179159,"option":"SELECT pid, adid\nFROM Ads LATERAL VIEW explode(adid_list) adTable AS adid;","correct":true},{"id":1179160,"option":"SELECT pid, adid\nFROM Ads VIEW explode(adid_list) adTable AS adid;","correct":false}]},{"q":"<p>In which of these cases would you be advised to use a managed table while working with Hive?<br>\n<br>\n1. When Hive should manage the lifecycle of the table<br>\n2. When generating temporary tables.</p>","a":[{"id":1178545,"option":"Only 1","correct":false},{"id":1178546,"option":"Only 2","correct":false},{"id":1178547,"option":"Both 1,2","correct":true},{"id":1178548,"option":"None of these","correct":false}]},{"q":"<p>What is the correct order of the steps that are involved in removing the nodes of a Hadoop cluster?</p>\n\n<ol>\n\t<li>Issue $hadoop dfsadmin -refreshNodes and Hadoop mradmin-refreshNodes commands</li>\n\t<li>Check web UI for the “Decommissioning in Progress” message</li>\n\t<li>Update network addresses in the dfs.exclude and mapred.exclude functionalities</li>\n\t<li>Remove the nodes from the slave file</li>\n\t<li>Remove the nodes from the include file and then run step 2 with refreshNodes again</li>\n</ol>","a":[{"id":745346,"option":"1->2->3->4->5","correct":false},{"id":745347,"option":"2->3->1->5->4","correct":false},{"id":745348,"option":"3->1->2->4->5","correct":false},{"id":745349,"option":"3->1->2->5->4","correct":true}]},{"q":"<p>In Hadoop, which of the following properties is used for retrieving the maximum amount of memory used by a mapper within a container?</p>","a":[{"id":745326,"option":"mapreduce.map.java.opts","correct":false},{"id":745327,"option":"mapreduce.reduce.memory.mb","correct":false},{"id":745328,"option":"yarn.scheduler.maximum-allocation-mb","correct":false},{"id":745329,"option":"mapreduce.map.memory.mb","correct":true}]},{"q":"<p>In Hadoop, which of the following is a slave in the <strong>MapReduce</strong> framework?</p>","a":[{"id":183902,"option":"JobTracker","correct":false},{"id":183903,"option":"TaskTracker","correct":true},{"id":183904,"option":"MasterTracker","correct":false},{"id":183905,"option":"SlaveTracker","correct":false}]},{"q":"<p>John is using Hadoop framework for his clients. He knows MapReduce is used to process data in the framework. Then which of the following is the master in it?</p>","a":[{"id":183898,"option":"TaskTracker","correct":false},{"id":183899,"option":"MasterTracker","correct":false},{"id":183900,"option":"SlaveTracker","correct":false},{"id":183901,"option":"JobTracker","correct":true}]},{"q":"<p>Mike is using Hadoop framework for his clients. He wants to use a file system which stores data on a number of machines and would offer him backup by data redundancy. Then which of the following should he take into consideration?</p>","a":[{"id":183487,"option":"HDFS","correct":true},{"id":183488,"option":"NAS","correct":false},{"id":183489,"option":"AFS","correct":false},{"id":183490,"option":"All of these","correct":false}]},{"q":"<p>Mike is using Hadoop framework for his clients and working on different filesystems. Then which of the following should he use for transferring the data?</p>","a":[{"id":171689,"option":"HDF","correct":false},{"id":171690,"option":"FS Shell","correct":true},{"id":171691,"option":"DFS Shell","correct":false},{"id":171692,"option":"Shell","correct":false}]},{"q":"<p>Which of these parameters must be provided to issue queries against Accumulo using Hive:</p>\n\n<ol>\n\t<li>accumulo.instance.name</li>\n\t<li>accumulo.zookeepers</li>\n\t<li>accumulo.user.name</li>\n</ol>","a":[{"id":1176735,"option":"1 and 2","correct":false},{"id":1176736,"option":"2 and 3","correct":false},{"id":1176737,"option":"1 and 3","correct":false},{"id":1176738,"option":"All of these","correct":true}]},{"q":"<p>You are required to solve OLAP queries on event data while working on Hive. Which of the following integrations will you implement to perform this task?</p>","a":[{"id":1176695,"option":"Druid ","correct":true},{"id":1176696,"option":"Kudo","correct":false},{"id":1176697,"option":"Accumulo","correct":false},{"id":1176698,"option":"HBase","correct":false}]},{"q":"<p>Which of the following are used by SerDe to serialize an object that is created by another SerDe in Hive?</p>","a":[{"id":1176635,"option":"ObjectInspector\r\n","correct":true},{"id":1176636,"option":"Row","correct":false},{"id":1176637,"option":"Format","correct":false},{"id":1176638,"option":"Settable","correct":false}]},{"q":"<p>When working with dynamic partitions in Hive you come across the error shown alongside.<br>\n<br>\nWhat can be done to fix this error?</p>\n\n<pre class=\"prettyprint\"><code>[Fatal Error] Operator FS_28 (id=41): fatal error. Killing the job.\nEnded Job = job_201005052204_28178 with errors</code></pre>\n\n<p> </p>","a":[{"id":1179213,"option":"Group the rows by the dynamic partition columns in the mapper and distribute them to the reducers where the dynamic partitions will be created","correct":true},{"id":1179214,"option":"Let one mapper take a random set of rows not letting the number of distinct pairs to exceed the limit of hive.exec.max.dynamic.partitions.pernode","correct":false},{"id":1179215,"option":"Let more than one mapper take a random set of rows letting the number of distinct pairs to exceed the limit of hive.exec.max.dynamic.partitions.pernode","correct":false},{"id":1179216,"option":"None of these","correct":false}]},{"q":"<p>When working with Dynamic Partitions in Hive, the query given alongside throws an error. How can the query be rewritten in order to avoid errors?</p>\n\n<pre class=\"prettyprint\"><code>FROM page_view_stg pvs\n INSERT OVERWRITE TABLE page_view PARTITION(dt, country)\n                 SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\n                        from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country;</code></pre>\n\n<p> </p>","a":[{"id":1179197,"option":"FROM page_view_stg pvs\r\nINSERT OVERWRITE TABLE page_view PARTITION(dt, country)\r\n             SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\r\n                    from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country\r\n             DISTRIBUTE BY ds, country;","correct":true},{"id":1179198,"option":"FROM page_view_stg pvs\r\nINSERT OVERWRITE TABLE page_view PARTITION(dt, country)\r\n\r\n             SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\r\n                  DISTRIBUTE BY ds, country   from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country;\r\n            ","correct":false},{"id":1179199,"option":"FROM page_view_stg pvs\r\nINSERT OVERWRITE TABLE page_view PARTITION(dt, country)  DISTRIBUTE BY ds, country\r\n             SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\r\n                    from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country;\r\n            ","correct":false},{"id":1179200,"option":"Either 1 or 2","correct":false}]},{"q":"You are performing column mapping to map HBase columns to Hive. <br><br>Which of these are valid forms of mapping entries?<br>1. :key<br>2. :timestamp<br>3. column-family-name:[column-name][#(binary|string)","a":[{"id":1178849,"option":"Only 1 and 2","correct":false},{"id":1178850,"option":"Only 3","correct":false},{"id":1178851,"option":"All 1, 2 and 3","correct":true},{"id":1178852,"option":"Only 1","correct":false}]},{"q":"You have a set of repeated or complex column expressions when working with Hive SQL.<br><br>What can you do to encapsulate these expressions?","a":[{"id":1178765,"option":"Use macros","correct":false},{"id":1178766,"option":"Use UDFs","correct":false},{"id":1178767,"option":"Use macros and UDFs","correct":false},{"id":1178768,"option":"Use macros or UDFs","correct":true}]},{"q":"Which of the following examples are valid ways of creating Union Types in Hive?<br>1. {2:[\"three\",\"four\"]} <br>2. {3:{\"a\":5,\"b\":\"five\"}} <br>3. {1:2.0}","a":[{"id":1178785,"option":"Only 1 and 2","correct":false},{"id":1178786,"option":"Only 2 and 3","correct":false},{"id":1178787,"option":"Only 1 and 3","correct":false},{"id":1178788,"option":"All 1, 2 and 3","correct":true}]},{"q":"You want to apply ORDER BY to an individual SELECT statement when working with Union in Hive DML.<br><br>Which of the following queries is the correct way to do this? ","a":[{"id":1178741,"option":"SELECT key FROM (SELECT key FROM src ORDER BY key LIMIT 10)subq1\nUNION\nSELECT key FROM (SELECT key FROM src1 ORDER BY key LIMIT 10)subq2","correct":true},{"id":1178742,"option":"SELECT key FROM src\nUNION\nSELECT key FROM src1 \nORDER BY key LIMIT 10","correct":false},{"id":1178743,"option":"Either 1 or 2","correct":false},{"id":1178744,"option":"None of these","correct":false}]},{"q":"Which of these complex join expressions are allowed in Hive?<br>1. SELECT a.* FROM a JOIN b ON (a.id = b.id)<br>2. SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id <> b.id)","a":[{"id":1178729,"option":"Only 1","correct":false},{"id":1178730,"option":"Only 2","correct":false},{"id":1178731,"option":"Both 1 and 2","correct":true},{"id":1178732,"option":"Neither 1 nor 2","correct":false}]},{"q":"A table has an OutputFormat that implements AcidOutputFormat. Also, the system is configured to use a transaction manager that implements ACID,<br><br>What will be disabled for such tables in Hive?","a":[{"id":1178709,"option":"INSERT INTO","correct":false},{"id":1178710,"option":"INSERT OVERWRITE","correct":true},{"id":1178711,"option":"INSERT OVERWRITE when IF NOT EXISTS is provided","correct":false},{"id":1178712,"option":"INSERT INTO when IF NOT EXISTS is provided","correct":false}]},{"q":"It is given that query given alongside is executed on the mapper and the tables A and B have 4 buckets. You want the mapper for the bucket for A to traverse the corresponding bucket for B.<br><br>Which of these parameters must be set in order to achieve this?","a":[{"id":1178717,"option":"set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n  set hive.optimize.bucketmapjoin = true;\n","correct":false},{"id":1178718,"option":"set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n  set hive.optimize.bucketmapjoin.sortedmerge = true;","correct":false},{"id":1178719,"option":"set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n  set hive.optimize.bucketmapjoin = true;\n  set hive.optimize.bucketmapjoin.sortedmerge = true;","correct":true},{"id":1178720,"option":"\n  set hive.optimize.bucketmapjoin = true;\n  set hive.optimize.bucketmapjoin.sortedmerge = true;","correct":false}]},{"q":"You are using the query given alongside to rebuild index on a partition when working with Hive Indexes.<br><br>What can be used in place of XXX and YYY to complete the query?","a":[{"id":1178637,"option":"XXX: REBUILD\nYYY: ALTER","correct":false},{"id":1178638,"option":"XXX: ALTER\nYYY: REBUILD","correct":true},{"id":1178639,"option":"XXX: REBUILD\nYYY: IF EXISTS","correct":false},{"id":1178640,"option":"XXX: ALTER\nYYY: IF EXISTS","correct":false}]},{"q":"<p>You encounter the error shown alongside while integrating spark with Hive. Which of the following can be considered as a valid cause for the same?</p>\n\n<pre class=\"prettyprint\"><code>org.apache.spark.SparkException: Job aborted due to stage failure:\n\nTask 5.0:0 had a not serializable result: java.io.NotSerializableException: org.apache.hadoop.io.BytesWritable</code></pre>\n\n<p> </p>","a":[{"id":1178521,"option":"Spark serializer not set to Kryo.","correct":true},{"id":1178522,"option":"Spark dependency not correctly set.","correct":false},{"id":1178523,"option":"Hive has upgraded to Jline2 but jline 0.94 exists in the Hadoop lib.","correct":false},{"id":1178524,"option":"Yarn dependency is not added to Hadoop Lib","correct":false}]},{"q":"<p>Which of the following Hive DML(Data Manipulation Language) statements can you use to overwrite any existing data in the table?</p>","a":[{"id":1176871,"option":"INSERT INTO","correct":false},{"id":1176872,"option":"INSERT OVERWRITE","correct":true},{"id":1176873,"option":"INSERT IN","correct":false},{"id":1176874,"option":"INSERT OVERWRITEIN","correct":false}]},{"q":"<p>Which of the following parameters is required to use <strong>HBASE</strong>?</p>","a":[{"id":183838,"option":"A variable schema must be needed","correct":false},{"id":183839,"option":"When data is stored in collections then only we can use HBASE","correct":false},{"id":183840,"option":"Both of these\r\n","correct":true},{"id":183841,"option":"None of these","correct":false}]},{"q":"<p>In Hadoop, which of the following is the process by which the system transfers the map outputs to the reducers as inputs?</p>","a":[{"id":260597,"option":"Sorting","correct":false},{"id":260598,"option":"Shuffling","correct":true},{"id":260599,"option":"Combiner","correct":false},{"id":260600,"option":"Partitioner","correct":false}]},{"q":"<p>Which of the following is the port number for the Task Tracker in Hadoop?</p>","a":[{"id":183818,"option":"50060","correct":true},{"id":183819,"option":"50030\r\n","correct":false},{"id":183820,"option":"50050","correct":false},{"id":183821,"option":"None of these","correct":false}]},{"q":"<p>In Hadoop, which of the following statements about getInstance() is incorrect?</p>","a":[{"id":172224,"option":"It creates a new job with particular cluster.","correct":true},{"id":172225,"option":"It creates a new Job with no particular cluster and a given configuration","correct":false},{"id":172226,"option":"It creates a new job with no particular cluster and given job status.","correct":false},{"id":172227,"option":"All of these","correct":false}]},{"q":"<p>In Hive, which of the following modes allows the <strong>Beeline</strong> shell to work with respect to <strong>HiveServer2</strong>:</p>\n\n<ol>\n\t<li>Embedded</li>\n\t<li>Remote</li>\n\t<li>Private</li>\n</ol>","a":[{"id":1176795,"option":"1 and 2","correct":true},{"id":1176796,"option":"2 and 3","correct":false},{"id":1176797,"option":"1 and 3","correct":false},{"id":1176798,"option":"None of these","correct":false}]},{"q":"<p>While working with the Hive Streaming API, you are required to manage some settings in the <strong>hive-site.xml</strong> file to enable ACID support for streaming. Which of the following must be set to true in this context?</p>","a":[{"id":1176767,"option":"hive.txn.manager","correct":false},{"id":1176768,"option":"hive.compactor.initiator.on","correct":true},{"id":1176769,"option":"hive.compactor.initiator.off","correct":false},{"id":1176770,"option":"hive.txn.manager.on","correct":false}]},{"q":"<p>Which of the following Hive APIs can be used for constructing processes in the following scenarios:</p>\n\n<ol>\n\t<li>Hive API operations are created dynamically</li>\n\t<li>Processes must interact in a declarative manner</li>\n</ol>","a":[{"id":1176759,"option":"1. Query-based APIs\r\n2. Operation-based APIs","correct":true},{"id":1176760,"option":"1. Operation-based APIs\r\n2. Query-based APIs","correct":false},{"id":1176761,"option":"Query-based APIs for both 1 and 2","correct":false},{"id":1176762,"option":"Operation-based APIs for both 1 and 2","correct":false}]},{"q":"<p>Which of the following cases allows Hive to add <strong>ReduceSinkOperators</strong> as parents of an operator while performing <strong>intra-query correlations</strong>:</p>\n\n<ol>\n\t<li>If the operators do not require data shuffling</li>\n\t<li>If the operator requires data shuffling</li>\n\t<li>If the operator is not serializable</li>\n</ol>","a":[{"id":1176671,"option":"1","correct":true},{"id":1176672,"option":"2","correct":false},{"id":1176673,"option":"3","correct":false},{"id":1176674,"option":"1 and 2","correct":false}]},{"q":"<p>How will you store and query <strong>Avro</strong> objects in HBase columns while working with Hive 0.14?</p>","a":[{"id":1176651,"option":"Make the Avro objects visible as Structs to Hive","correct":true},{"id":1176652,"option":"Make the Avro objects visible as Lists to Hive","correct":false},{"id":1176653,"option":"Make the Avro objects visible as Maps to Hive","correct":false},{"id":1176654,"option":"Make the Avro objects visible as Collections to Hive","correct":false}]},{"q":"<p>An <strong>Avro Enum</strong> type must be defined in Hive while writing a table. Which of the following values must be assigned to the <strong>Avro Enum</strong> type?</p>","a":[{"id":1176643,"option":"String","correct":true},{"id":1176644,"option":"Tiny int","correct":false},{"id":1176645,"option":"Bytes","correct":false},{"id":1176646,"option":"Float","correct":false}]},{"q":"<p>Assume that your Hive storage handler uses the following code to create base tables. Which of these types of base tables are created in this context?</p>\n\n<pre class=\"prettyprint\"><code> CREATE EXTERNAL TABLE when no STORED BY clause is </code></pre>\n\n<p> </p>","a":[{"id":1176639,"option":"Managed native","correct":false},{"id":1176640,"option":"External native","correct":true},{"id":1176641,"option":"Managed non-native","correct":false},{"id":1176642,"option":"External non-native","correct":false}]},{"q":"<p>You are using the code given alongside to load data into a table when working with a JDBC client in Hive. Which of these criterias should the code satisfy for successful execution?</p>\n\n<pre class=\"prettyprint\"><code> String filepath = \"/tmp/a.txt\";\n    sql = \"load data local inpath '\" + filepath + \"' into table \" + tableName;\n    System.out.println(\"Running: \" + sql);\n    res = stmt.executeQuery(sql);</code></pre>\n\n<p> </p>","a":[{"id":1179177,"option":"Only the filepath has to be local to the hive server ","correct":false},{"id":1179178,"option":"The filepath has to be local to the hive server and the query must be a regular SQL query","correct":false},{"id":1179179,"option":"/tmp/a.txt should be a ctrl-A separated file with two fields per line and the query must be a regular SQL query","correct":false},{"id":1179180,"option":"The filepath has to be local to the hive server and /tmp/a.txt should be a ctrl-A separated file with two fields per line","correct":true}]},{"q":"<p>You have created a table 'source' with 'CLUSTERED BY id INTO 32 BUCKETS'. You have also created a TABLESAMPLE clause as given alongside.<br>\n<br>\nIn the given scenario, which of these clusters will be picked out and how many clusters will each bucket be composed of?</p>\n\n<pre class=\"prettyprint\"><code>TABLESAMPLE(BUCKET 3 OUT OF 16 ON id)</code></pre>\n\n<p> </p>","a":[{"id":1179169,"option":"The 3rd and 19th clusters are picked out and each bucket would be composed of 1/2 of a cluster","correct":false},{"id":1179170,"option":"The 3rd cluster is picked out and each bucket would be composed of 1/2 of a cluster","correct":false},{"id":1179171,"option":"The 3rd and 19th clusters are picked out and each bucket would be composed of 2 clusters","correct":true},{"id":1179172,"option":"The 3rd cluster is picked out and each bucket would be composed of 2 clusters","correct":false}]},{"q":"<p>When working in Hive, you observe that after a transform, the numeric data are sorted lexicographically.<br>\n<br>\nWhat can be used in place of XXX in the query given alongside to overcome this problem?</p>\n\n<pre class=\"prettyprint\"><code>FROM (FROM (FROM src\n            SELECT TRANSFORM(value)\n            USING 'mapper'\n            AS value, count) mapped\n   XXX\nSELECT TRANSFORM(value, count)\nUSING 'reducer'\nAS whatever</code></pre>\n\n<p> </p>","a":[{"id":1179149,"option":"   SELECT cast(value as double) AS value, cast(count as int) AS count\r\n      SORT BY value, count) sorted","correct":true},{"id":1179150,"option":" \r\n     SORT BY value, count) sorted","correct":false},{"id":1179151,"option":"  SELECT cast(value as double) AS value, cast(count as int) AS count","correct":false},{"id":1179152,"option":" SORT BY value, count) sorted   \r\nSELECT cast(value as double) AS value, cast(count as int) AS count\r\n     ","correct":false}]},{"q":"<p>You are casting a string to the date format as shown alongside when working in Hive.<br>\n<br>\nWhat happens when the string is not in the format 'YYYY-MM-DD' in the given context?</p>\n\n<pre class=\"prettyprint\"><code>cast(string as date)</code></pre>\n\n<p> </p>","a":[{"id":1179125,"option":"Error is returned","correct":false},{"id":1179126,"option":"NULL is returned ","correct":true},{"id":1179127,"option":"Required change in format' warning is returned","correct":false},{"id":1179128,"option":"Current date in format 'YYYY-MM-DD' is returned","correct":false}]},{"q":"<p>The trial table given alongside contains rows with duplicate keys.<br>\n<br>\nWhat happens when this table is copied into a HBase table?</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE trail2(foo INT, bar STRING);\nINSERT OVERWRITE TABLE trial2 SELECT * FROM \ntrials;\n\nSELECT COUNT(1) FROM trials WHERE foo=498;\n\nSELECT COUNT(1) FROM trial2 WHERE foo=498;</code></pre>\n\n<p> </p>","a":[{"id":1178873,"option":"CREATE TABLE trial3(foo INT, bar STRING)\r\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\r\nWITH SERDEPROPERTIES (\r\n\"hbase.columns.mapping\" = \":key,cf:bar\"\r\n);\r\nINSERT OVERWRITE TABLE trial3 SELECT * FROM trials;\r\n\r\nSELECT COUNT(1) FROM trial3 WHERE foo=498;","correct":true},{"id":1178874,"option":"CREATE TABLE trial3(foo INT, bar STRING)\r\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\r\nWITH SERDEPROPERTIES (\r\n\"hbase.columns.mapping\" = \":key,cf:bar\"\r\n);\r\nINSERT OVERWRITE TABLE trial3 SELECT * FROM trials;\r\n-- this will return 1 instead of 3\r\nSELECT COUNT(1) FROM trials WHERE foo=498;\r\n\r\nSELECT COUNT(1) FROM trial3 WHERE foo=498;","correct":false},{"id":1178875,"option":"CREATE TABLE trial3(foo INT, bar STRING)\r\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\r\nWITH SERDEPROPERTIES (\r\n\"hbase.columns.mapping\" = \":key,cf:bar\"\r\n);\r\nINSERT OVERWRITE TABLE trial3 SELECT * FROM trials;\r\n-- this will return 1 instead of 3\r\nSELECT COUNT(1) FROM trials WHERE foo=498;\r\n\r\n","correct":false},{"id":1178876,"option":"None of these","correct":false}]},{"q":"<p>When performing Column Mapping in Accumulo Hive, the rowid controls which Hive column is used as the Accumulo rowid.<br>\n<br>\nWhat can be said about the following statements in the given context?</p>\n\n<p>Statement 1: Exactly one \":rowid\" element must exist in each column mapping<br>\nStatement 2: :rowID is equivalent to :rowId</p>","a":[{"id":1178861,"option":"Statement 1 is true and Statement 2 is false","correct":false},{"id":1178862,"option":"Statement 1 is false and Statement 2 is false","correct":false},{"id":1178863,"option":"Statement 1 is true and Statement 2 is true","correct":true},{"id":1178864,"option":"Statement 1 is false and Statement 2 is true","correct":false}]},{"q":"<p>Consider the Hive SerDe operations given alongside. In the given context, what can be said about the \"key\" part of?</p>\n\n<pre class=\"prettyprint\"><code>HDFS files --&gt; InputFileFormat --&gt; &lt;key, value&gt; --&gt; Deserializer --&gt; Row object\nRow object --&gt; Serializer --&gt; &lt;key, value&gt; --&gt; OutputFileFormat --&gt; HDFS files</code></pre>\n\n<p> </p>","a":[{"id":1178773,"option":"The \"key\" part is ignored when reading","correct":false},{"id":1178774,"option":"The \"key\" part is ignored when writing","correct":false},{"id":1178775,"option":"The \"key\" part is always constant when writing ","correct":false},{"id":1178776,"option":"Both 1 and 3","correct":true}]},{"q":"In order to deal with the inefficiencies in loading multiple partitions you are using a partition insert that does the following: <br>1. The input column values are evaluated to determine which partition this row should be inserted into. <br>2. If a partition has not been created, it will create that partition automatically.<br><br>How many insert statements should be created to implement this method? What is the number of corresponding MapReduce jobs used?","a":[{"id":1179193,"option":"One insert statement should be created and one corresponding MapReduce jobs should be used","correct":true},{"id":1179194,"option":"One insert statement should be created and two corresponding MapReduce jobs should be used","correct":false},{"id":1179195,"option":"Two insert statement should be created and one corresponding MapReduce jobs should be used","correct":false},{"id":1179196,"option":"Two insert statement should be created and two corresponding MapReduce jobs should be used","correct":false}]},{"q":"You are using the Hive ObjectInspector to analyze the internal structure of the row object and also the structure of the individual columns.<br><br>The ObjectInspector provides a uniform way to access which of these complex objects?<br>","a":[{"id":1178825,"option":"Only an instance of a Java class and a standard Java object","correct":false},{"id":1178826,"option":"Only an instance of a Java class and a lazily-initialized object ","correct":false},{"id":1178827,"option":"Only an instance of a Java class, a lazily-initialized object and a standard Java object","correct":true},{"id":1178828,"option":"None of these","correct":false}]},{"q":"Which of these is valid ways of using the '+' operator in Hive PL/SQL?<br>1. 'Ab' + 'c'<br>2. DATE '2014-12-31' + 1<br>3. DATE '2014-12-31 + 1'","a":[{"id":1178801,"option":"Only 1 and 3","correct":false},{"id":1178802,"option":"Only 1 and 2","correct":true},{"id":1178803,"option":"Only 2 and 3","correct":false},{"id":1178804,"option":"All 1, 2 and 3","correct":false}]},{"q":"The Hive query given alongside fails with an error message \"FAILED: SemanticException 4:47 Schema of both sides of union should match.\".<br><br>Which of the following queries can be used alternatively to avoid such an error?","a":[{"id":1178749,"option":"INSERT OVERWRITE TABLE target_table\n  SELECT name, id, \"Category159\" as category FROM source_table_1\n  UNION \n  SELECT name, id, category FROM source_table_2","correct":false},{"id":1178750,"option":"INSERT OVERWRITE TABLE target_table\n  SELECT name, id, category FROM source_table_1\n  UNION ALL\n  SELECT name, id, \"Category159\" as category FROM source_table_2","correct":true},{"id":1178751,"option":"INSERT OVERWRITE TABLE target_table\n  SELECT name, id, category FROM source_table_1\n  UNION ALL\n  SELECT name, id, category FROM source_table_2\nWHERE category = \"Category159\"","correct":false},{"id":1178752,"option":"INSERT OVERWRITE TABLE target_table\n  SELECT name, id, category FROM source_table_1\n  UNION \n  SELECT name, id, \"Category159\" as category FROM source_table_2\nWHERE category = \"Category159\"","correct":false}]},{"q":"The HiveServer2 URL string contains a comma separated list of server instances to connect to, as shown alongside. Given that the dynamic service discovery is enabled. <br><br>What happens if the server instances are empty?","a":[{"id":1178673,"option":" The embedded server will be used","correct":true},{"id":1178674,"option":"A prompt for one pair of server instance is received","correct":false},{"id":1178675,"option":"Exisiting server instances are used","correct":false},{"id":1178676,"option":"None of these","correct":false}]},{"q":"<p>Which of these commands can be used to retrive hiveconfig variables?</p>","a":[{"id":1176875,"option":"$[hiveconf:x]","correct":false},{"id":1176876,"option":"$\"hiveconf:x\"","correct":false},{"id":1176877,"option":"${hiveconf:x}","correct":true},{"id":1176878,"option":"${\"hiveconf:x\"}","correct":false}]},{"q":"<p>A string value when converted to a varchar value exceeds the length specifier. What happens to the string when working with Hive?</p>","a":[{"id":1176843,"option":"The string is truncated","correct":true},{"id":1176844,"option":"The string is not converted ","correct":false},{"id":1176845,"option":"The string is deleted","correct":false},{"id":1176846,"option":"None of these","correct":false}]},{"q":"<p>In Hadoop, which of the following is used to change the maximum number of cells in a table?</p>","a":[{"id":171929,"option":"Update","correct":false},{"id":171930,"option":"Reset","correct":false},{"id":171931,"option":"Alter","correct":true},{"id":171932,"option":"Select","correct":false}]},{"q":"<p>Bob is using Hadoop framework for his clients. Then which of the following sustains splittable compression?</p>","a":[{"id":171799,"option":"LZO","correct":false},{"id":171800,"option":"BZIP2","correct":false},{"id":171801,"option":"LZ4","correct":false},{"id":171802,"option":"All of these","correct":true}]},{"q":"<p>Which of the following Hive commands will you use to generate <strong>.lzo.index</strong> for <strong>.lzo</strong> files?</p>","a":[{"id":1176703,"option":"lzop","correct":true},{"id":1176704,"option":"lzoc","correct":false},{"id":1176705,"option":"lzos","correct":false},{"id":1176706,"option":"lzot","correct":false}]},{"q":"<p>You want to run different test instances simultaneously as one user while executing parallel tests on Hive. Which of the following environment variables will you export to do perform this task?</p>","a":[{"id":1176683,"option":"HIVE_PTEST_SUFFIX","correct":true},{"id":1176684,"option":"HIVE_PTEST","correct":false},{"id":1176685,"option":"HIVE_PTEST_CONFIG","correct":false},{"id":1176686,"option":"HIVE_PTEST_INSTANCE","correct":false}]},{"q":"<p>You find that Hive consumes exceptions that occur before job submissions while working with AvroSerde. If you want to force Hive to be more verbose, then which of these code statements will you use?</p>","a":[{"id":1176647,"option":"*hive --hiveconf hive.root.logger=DEBUG,console*","correct":false},{"id":1176648,"option":"*hive --hiveconf hive.root.logger=WARN,console*","correct":false},{"id":1176649,"option":"*hive --hiveconf hive.root.logger=INFO,console*","correct":true},{"id":1176650,"option":"*hive --hiveconf hive.root.logger=PUSH,console*","correct":false}]},{"q":"<p>Which of the following are required by Jetty to utilize the Hive web interface?</p>","a":[{"id":1176631,"option":"Apache Ant ","correct":true},{"id":1176632,"option":"Apache Hadoop","correct":false},{"id":1176633,"option":"Apache Spark","correct":false},{"id":1176634,"option":"Apache Allura","correct":false}]},{"q":"<p>Analyse the given queries and determine which of the following are valid ways of performing multiple aggregations in Hive?</p>\n\n<pre class=\"prettyprint\"><code>1. INSERT OVERWRITE TABLE pv_gender_agg\nSELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid)\nFROM pv_users\nGROUP BY pv_users.gender;\n\n2. INSERT OVERWRITE TABLE pv_gender_agg\nSELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip)\nFROM pv_users\nGROUP BY pv_users.gender;</code></pre>\n\n<p><br>\n </p>","a":[{"id":1179201,"option":"Only 1","correct":true},{"id":1179202,"option":"Only 2","correct":false},{"id":1179203,"option":"Both 1 and 2","correct":false},{"id":1179204,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>The schema given alongside has missing partition information and doesn't throw an error. It is given that the file(s) located at filepath conform to the table schema such that each row ends with partition column(s).<br>\n<br>\nWhat does the load operation do in the given situation?</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE tab1 (col1 int, col2 int) PARTITIONED BY (col3 int) STORED AS ORC;\nLOAD DATA LOCAL INPATH 'filepath' INTO TABLE tab1;</code></pre>\n\n<p> </p>","a":[{"id":1179189,"option":"It rewrites into an INSERT AS SELECT job.","correct":true},{"id":1179190,"option":"It assumes that the last set of columns are partition columns.","correct":false},{"id":1179191,"option":" It makes sure each file conforms to the schema.","correct":false},{"id":1179192,"option":"It assumes that the last set of columns are partition columns and it makes sure each file conforms to the schema.","correct":false}]},{"q":"<p>When working with TRANSFORM in Hive, you have set the output fields from a script as the default string type as shown alongside.<br>\n<br>\nWhich of the following queries can be used to cast the output as INT?</p>\n\n<pre class=\"prettyprint\"><code>SELECT TRANSFORM(stuff)\n  USING 'script'\n  AS thing1, thing2</code></pre>\n\n<p> </p>","a":[{"id":1179121,"option":"  SELECT TRANSFORM(stuff)\r\n  USING 'script'\r\n  AS (thing1 INT, thing2 INT)","correct":true},{"id":1179122,"option":"  SELECT TRANSFORM(stuff)\r\n  USING 'script'\r\nCAST (thing1 INT, thing2 INT)","correct":false},{"id":1179123,"option":"  SELECT TRANSFORM(stuff)\r\n  USING 'script'\r\nCAST AS (thing1 INT, thing2 INT)","correct":false},{"id":1179124,"option":"  SELECT TRANSFORM(stuff)\r\n  USING 'script'\r\nCAST (thing1, thing2) AS (INT, INT)","correct":false}]},{"q":"<p>You are creating a new HBase table using the query given alongside.<br>\n<br>\nIn the given context, what will the value of the column \"tags\" be?</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE hbase_table_1(tags map&lt;string,int&gt;, row_key string) \n</code></pre>\n\n<p> </p>","a":[{"id":1178853,"option":"\"x\" : 1","correct":true},{"id":1178854,"option":"\"tag_x\" : 1","correct":false},{"id":1178855,"option":"x : \"1\"","correct":false},{"id":1178856,"option":"tag_x : \"1\"","correct":false}]},{"q":"<p>You are using the Hive SerDe class LazySimpleSerDe to read the same data format as MetadataTypedColumnsetSerDe. Assume that the hive.lazysimple.extended_boolean_literal is set to false.<br>\n<br>\nWhich of these among the values shown alongside are treated as legal boolean values?<br>\n </p>\n\n<pre class=\"prettyprint\"><code>'TRUE', 't', 'FALSE', 'f', '1', and '0' </code></pre>\n\n<p> </p>","a":[{"id":1178821,"option":"Only TRUE and FALSE","correct":true},{"id":1178822,"option":"Only TRUE, FALSE, t and f","correct":false},{"id":1178823,"option":"Only 1, 0","correct":false},{"id":1178824,"option":"Only t and f","correct":false}]},{"q":"<p>You have a table db.trial that is partitioned by d and city as shown alongside.<br>\n<br>\nWhich of the following HPL/SQL functions can you use to get the top-level partitions and the number of its sub-partitions?</p>\n\n<pre class=\"prettyprint\"><code>d=2015-09-01/city=1\nd=2015-09-01/city=2\nd=2015-09-02/city=1\nd=2015-09-03/city=3\nd=2015-09-03/city=2</code></pre>\n\n<p> </p>","a":[{"id":1178805,"option":"part_count_by(db.trial, d); ","correct":true},{"id":1178806,"option":"part_count_by(db.trial); ","correct":false},{"id":1178807,"option":"part_count_by(db.trial, city); ","correct":false},{"id":1178808,"option":"part_count_by(db.trial, d/city); ","correct":false}]},{"q":"You have a table employee with the fields Id, Name, Salary, Designation, Dept, and JoiningYear. You want to partition the table based on the JoiningYear. <br><br>Which of the following queries can be used to do this?<br><br>[Note: Create a partition for the JoiningYear 1998]","a":[{"id":1179145,"option":"ALTER TABLE employee\n ADD PARTITION (year=’1998’)\nlocation '/1998/part1998';","correct":true},{"id":1179146,"option":"ALTER TABLE employee\n ADD PARTITION (year=’1998’)\n","correct":false},{"id":1179147,"option":"ALTER TABLE employee\n ADD PARTITION location '/1998/part1998';","correct":false},{"id":1179148,"option":"None of these","correct":false}]},{"q":"In the Hive Compiler, the Query Plan Generator, the operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks.<br><br>What can this be used for with respect to the Hadoop distributed file systems?","a":[{"id":1178857,"option":"The series of map-reduce serializable tasks can be submitted to the map-reduce framework ","correct":true},{"id":1178858,"option":"The series of map-reduce serializable tasks can be submitted to the map-reduce boundary","correct":false},{"id":1178859,"option":"The series of map-reduce serializable tasks can be used in the reduceSink operator","correct":false},{"id":1178860,"option":"None of these","correct":false}]},{"q":"Which of these Hive frameworks can you use to satisfy the following requirements?<br>1. Full test isolation<br>2. Seamless UDF integration<br>3. Hive/Hadoop installation is not required","a":[{"id":1178761,"option":"hive_test","correct":false},{"id":1178762,"option":"beetest","correct":false},{"id":1178763,"option":"HiveRunner","correct":true},{"id":1178764,"option":"HiveQLUnit","correct":false}]},{"q":"A table is created in Hive as shown alongside. You are using SHOW COLUMNS to produce the output as follows.<br><br>Output: C1,C2,C3,a,b,c<br><br>Which of the following queries can be used to achieve the desired results?","a":[{"id":1178753,"option":"SHOW COLUMNS FROM trial FROM trial_db LIKE 'col*';","correct":false},{"id":1178754,"option":"SHOW COLUMNS FROM foo '*c';  ","correct":false},{"id":1178755,"option":"SHOW COLUMNS IN foo IN test_db LIKE 'col*'; ","correct":false},{"id":1178756,"option":"Either 1 or 3","correct":true}]},{"q":"When working with Hive you want to do some additional processing on the result of the UNION. Which of the following queries can be used to do so?","a":[{"id":1178737,"option":"SELECT *\nFROM (\n                      select_statement\n              UNION ALL\n                       select_statement\n           ) ","correct":false},{"id":1178738,"option":"SELECT *\nFROM (\n                      select_statement\n              UNION ALL\n                       select_statement\n           ) unionResult","correct":true},{"id":1178739,"option":"SELECT *\nFROM (\n                      select_statement\n         JOIN\n                       select_statement\n           ) UNION","correct":false},{"id":1178740,"option":"Either 1 or 3","correct":false}]},{"q":"You have computed the column statistics for all partitions in a partitioned table. Now, you want to view the gathered column statistics.<br><br>Which of these queries can you use to do so when working with Hive 0.14.0?","a":[{"id":1178641,"option":"DESCRIBE FORMATTED [db_name.]table_name column_name PARTITION (partition_spec);","correct":true},{"id":1178642,"option":"DESCRIBE FORMATTED [db_name.]table_name PARTITION (partition_spec);","correct":false},{"id":1178643,"option":"DESCRIBE FORMATTED [db_name.]table_name column_name; ","correct":false},{"id":1178644,"option":"DESCRIBE FORMATTED [db_name.]table_name column_name (partition_spec);","correct":false}]},{"q":"<p>Which of these configuration parameters will need to be set for Postgres while working with Hive Metastore to avoid any failures?</p>","a":[{"id":1178469,"option":" metastore.try.direct.sql.ddl","correct":true},{"id":1178470,"option":" metastore.try.standalone","correct":false},{"id":1178471,"option":" metastore.try.tasks","correct":false},{"id":1178472,"option":" metastore.try.proxy","correct":false}]},{"q":"<p>Which of these test properties in Hive can be used to avoid the overhead of spawning new JVMs?</p>","a":[{"id":1178445,"option":"hive.exec.submit.local.task.via.child","correct":true},{"id":1178446,"option":"hive.test.mode.nosamplelist","correct":false},{"id":1178447,"option":"hive.test.mode.samplefreq","correct":false},{"id":1178448,"option":"hive.test.mode","correct":false}]},{"q":"<p>Which of these are case sensitive in the Data Definition Language?<br>\n1. Table names<br>\n2. Column names<br>\n3. Property names</p>","a":[{"id":1176923,"option":"Only 1 ","correct":false},{"id":1176924,"option":"Only 2","correct":false},{"id":1176925,"option":"Only 3","correct":true},{"id":1176926,"option":"All 1, 2 and 3","correct":false}]},{"q":"<p>You have a textfile that is compressed with Gzip. Which of these criterias should a table fulfill inorder to import textfile into it?</p>","a":[{"id":1176883,"option":"The table needs to be stored as a SequenceFile","correct":false},{"id":1176884,"option":"The table needs to be stored as a TextFile","correct":true},{"id":1176885,"option":"The table needs to be stored as a RecordFile","correct":false},{"id":1176886,"option":"Either 1 or 3","correct":false}]},{"q":"<p>Which of the following statements about Spark is false?</p>","a":[{"id":730742,"option":"Spark SQL is a special component on the Spark Core engine that supports SQL and Hive Query Language without changing any syntax.","correct":false},{"id":730743,"option":"It is possible to join SQL table and HQL table to Spark SQL.","correct":false},{"id":730744,"option":"Spark is preferred over Hadoop for real-time querying of data, e.g. Stock Market Analysis, Banking, Healthcare, Telecommunications, etc.","correct":false},{"id":730745,"option":"Spark runs up to 10 times faster than Hadoop when it comes to processing medium and large-sized data-sets.","correct":true}]},{"q":"<p>You are using the HiveServer2 <strong>Clients#vertical</strong> output format in Beeline to display the result. What do the keys represent in this output format?</p>","a":[{"id":1176803,"option":"Column names","correct":true},{"id":1176804,"option":"Each row of the result","correct":false},{"id":1176805,"option":"Value of rows","correct":false},{"id":1176806,"option":"Value of columns","correct":false}]},{"q":"<p>What should a client user do before invoking the Hive Streaming API?</p>","a":[{"id":1176763,"option":"Partition the table location","correct":false},{"id":1176764,"option":"Log in using hive-streaming module","correct":false},{"id":1176765,"option":"Login using kerberos","correct":true},{"id":1176766,"option":"None of these","correct":false}]},{"q":"<p>Which of the following tasks are performed by the <strong>Hive PTest2 WebServer</strong> process:</p>\n\n<ol>\n\t<li>Creating EC2 slaves</li>\n\t<li>Reading the specified property-file and compiling Hive</li>\n\t<li>Distributing the compiled artifacts across the EC2 slaves</li>\n</ol>","a":[{"id":1176731,"option":"1 and 2","correct":false},{"id":1176732,"option":"2 and 3","correct":false},{"id":1176733,"option":"1 and 3","correct":false},{"id":1176734,"option":"All of these","correct":true}]},{"q":"Assume that you have specified a list of partition column names in the PARTITION clause but you have not specified the partitions values. It is given that the partition creation is determined by the value of the input column.<br><br>How should you specify the partition columns in the given scenario?","a":[{"id":1179221,"option":"It must be specified last among the columns in the SELECT statement only ","correct":false},{"id":1179222,"option":"It must only be specified in the same order in which they appear in the PARTITION() clause.","correct":false},{"id":1179223,"option":"It must be specified last among the columns in the SELECT statement and in the same order in which they appear in the PARTITION() clause.","correct":true},{"id":1179224,"option":"It must be specified last among the columns in the SELECT statement and in at least one static partition.","correct":false}]},{"q":"Assume that when working in Hive, you have a load command with the LOCAL keyword specified. You are running this command against a HiveServer2 instance.<br><br>&nbsp;&nbsp;&nbsp;&nbsp;What does the local path refer to in the given scenario?","a":[{"id":1179185,"option":"It refers to a path on the localpath instance given that the HiveServer2 instance must have the proper permissions to access that file.","correct":false},{"id":1179186,"option":"It refers to a path on the HiveServer2 given that the HiveServer2 instance must have the proper permissions to access that file.","correct":true},{"id":1179187,"option":"It refers to a path on the localpath instance","correct":false},{"id":1179188,"option":"None of these","correct":false}]},{"q":"When working in Hive 3.0, you are using the load operations. Assume that the tables you are working with are bucketed, <br><br>What rules apply in the following modes?<br>1. Strict mode<br>2. Non-strict mode (file names do not conform to the naming convention ","a":[{"id":1178701,"option":"1. Launches an INSERT AS SELECT job.\n2. A pure copy/move operation is made","correct":true},{"id":1178702,"option":"Launches an INSERT AS SELECT job in both 1 and 2\n","correct":false},{"id":1178703,"option":"1. Launches an INSERT AS SELECT job.\n2. A pure copy/move operation is made","correct":false},{"id":1178704,"option":"A pure copy/move operation is made in both 1 and 2","correct":false}]},{"q":"When working with Hive Transactions, SHOW LOCKS returns shared_read as the type of the lock. <br><br>In the given scenario, which of these operations could have been used to obtain this lock?","a":[{"id":1178649,"option":"Delete operation","correct":false},{"id":1178650,"option":"Update operation","correct":false},{"id":1178651,"option":"Insert operation","correct":true},{"id":1178652,"option":"The DDL operation drop table","correct":false}]},{"q":"<p>What happens to the variables values in Hive when two different Hive sessions are run?</p>","a":[{"id":1176879,"option":"The variable values get mixed across sessions.","correct":true},{"id":1176880,"option":"The variable values do not get mixed across sessions.","correct":false},{"id":1176881,"option":"The variable values get mixed altered","correct":false},{"id":1176882,"option":"Both 1 and 3","correct":false}]},{"q":"<p>In Hadoop, which of the following services run <strong>MapReduce</strong> jobs on the cluster?</p>","a":[{"id":183551,"option":"NameNode","correct":false},{"id":183552,"option":"JobTracker","correct":true},{"id":183553,"option":"TaskNode","correct":false},{"id":183554,"option":"DataNode","correct":false}]}]