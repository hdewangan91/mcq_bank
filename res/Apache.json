[{"q":"<p>In Kafka, you have created the following topic and it has one partition.&nbsp;Which of these&nbsp;statements about this scenario are correct?</p>\n\n<p><strong>Topic:</strong></p>\n\n<pre class=\"prettyprint\"><code>&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-sample-topic\nTopic:my-sample-topic        PartitionCount:1        ReplicationFactor:3        Configs:\nTopic: my-sample-topic        Partition: 0      Leader: 1      Replicas: 1,2,0    Isr:1,2,0</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>Leader&nbsp;is the node that is responsible for all the reads and writes for the given partition, where each node is the leader for a randomly selected portion of the partitions.</li>\n\t<li>Replicas&nbsp;is the list of nodes that replicate the log for this partition irrespective of whether they are the leader or they are currently active.&nbsp;</li>\n\t<li>Isr&nbsp;is the set of in-sync&nbsp;replicas that represents a set of the replicas that are currently inactive.</li>\n</ol>","a":[{"id":1166763,"option":"1 and 2","correct":true},{"id":1166764,"option":"2 and 3","correct":false},{"id":1166765,"option":"1 and 3","correct":false},{"id":1166766,"option":"All of these","correct":false}]},{"q":"<p>In Apache Oozie, you are working on its Bundle system to define and execute a group of coordinator applications. Which of the following statements about the bundle job statuses are correct:</p>\n\n<ol>\n\t<li>When a bundle job is submitted, Oozie parses the bundle job XML. Oozie then creates a record for the bundle with the status <em>PRESUSPEND</em> and returns a unique ID.</li>\n\t<li>When the pause time reaches for a bundle job with <em>PREP</em> status, Oozie puts the job in status <em>PREPPAUSED</em>.</li>\n\t<li>When a user requests to resume a <em>PREPSUSPENDED</em> bundle job, Oozie puts the job in status <em>PREP</em>.</li>\n\t<li>When pause time is reset for a bundle job that is in a <em>PREPPAUSED</em> state, Oozie puts the job in status <em>PREP</em>.</li>\n</ol>\n\n<p>&nbsp;</p>","a":[{"id":1481709,"option":"1, 2, and 3","correct":false},{"id":1481710,"option":"2, 3, and 4","correct":true},{"id":1481711,"option":"1, 2, and 4","correct":false},{"id":1481712,"option":"1, 3, and 4","correct":false}]},{"q":"<p>Alice is trying to convert the existing RDDs into DataFrames by using Spark SQL in her Spark application. Assume that she knows the schema while writing her Spark application. If she has inferred the RDD schema containing specific types of objects to perform this action, then which of the following methods she might have implemented with RDDs in this scenario?</p>","a":[{"id":1577928,"option":"Reflection based method","correct":true},{"id":1577929,"option":"Programmatic interface method","correct":false},{"id":1577930,"option":"Schema API method","correct":false},{"id":1577931,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming, which of these statements about the following function are correct:</p>\n\n<p><strong>Function</strong></p>\n\n<pre class=\"prettyprint\"><code>groupByKeyAndWindow(windowDuration, slideDuration, [numTasks])</code></pre>\n\n<p><strong>Statement</strong></p>\n\n<ol>\n\t<li>When it is called on a DStream of (K, V) pairs, it returns a new DStream of (K, Seq[V]) pairs.</li>\n\t<li>The values for each key in the existing DStream pair are aggregated using a normal function.</li>\n\t<li>By default, it uses Spark's default number of parallel tasks to perform the grouping.</li>\n\t<li>You can pass only one numTasks argument to set a different number of tasks</li>\n</ol>\n\n<p> </p>","a":[{"id":1166913,"option":"1 and 2","correct":false},{"id":1166914,"option":"1 and 3","correct":true},{"id":1166915,"option":"1, 2, and 3","correct":false},{"id":1166916,"option":"2, 3, and 4","correct":false}]},{"q":"<p>In Kafka, if the <strong>auto.leader.rebalance.enable</strong> property is enabled, then which of the following statements is correct:</p>\n\n<ol>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by periodically returning the leadership to the preferred replica for each partition, if available.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to any of the replicas that are available.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to the preferred replica for each partition irrespective of its availability.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to any of the replicas for each partition irrespective of its availability.</li>\n</ol>\n\n<p> </p>","a":[{"id":1166921,"option":"1","correct":true},{"id":1166922,"option":"2","correct":false},{"id":1166923,"option":"3","correct":false},{"id":1166924,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, you want to write data to an external system. You create a connection object in the Spark driver as displayed in the following code. You encounter an error. Which of these statements is a valid reason for this error:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>  dstream.foreachRDD(rdd =&gt; {\n      val connection = createNewConnection()  \n      rdd.foreach(record =&gt; {\n          connection.send(record) \n      })\n  })</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>The connection object must be parallelized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the worker to the driver. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>There is no error. The transferability of the connection object across machines is successful.</li>\n</ol>","a":[{"id":1167806,"option":"1","correct":false},{"id":1167807,"option":"2","correct":true},{"id":1167808,"option":"3","correct":false},{"id":1167809,"option":"4","correct":false}]},{"q":"<p>Consider that you are using the <strong>Azure Databricks</strong> interactive notebook to run the following code. If you want to send data to Slack to generate feedback from users through Slack, then which of these statements represents the task performed by the following code:</p>\n\n<p><strong>Code </strong></p>\n\n<pre class=\"prettyprint\"><code>val superSecretSlackToken = \"xoxb-...\" superSecretSlackToken: String = xoxb-... </code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>Defines the Slack Bot API token</li>\n\t<li>Defines the functions used by Slack</li>\n\t<li>Prepares the Kafka producer to send messages to Slack</li>\n\t<li>Initializes the Slack channel</li>\n</ol>","a":[{"id":1166933,"option":"1","correct":true},{"id":1166934,"option":"2","correct":false},{"id":1166935,"option":"3","correct":false},{"id":1166936,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, if you increase the number of topic-specific partitions in <strong>KafkaUtils.createStream()</strong>, then<strong> </strong>which of the following occurs:</p>\n\n<ol>\n\t<li>The number of threads using which the topics are consumed within multiple receivers increases.</li>\n\t<li>The number of threads using which the topics are consumed within a single receiver increases.</li>\n\t<li>The parallelism property of Spark does not increase while processing data.</li>\n\t<li>The parallelism property of Spark decreases significantly while processing data.</li>\n</ol>","a":[{"id":1166929,"option":"1 and 2","correct":false},{"id":1166930,"option":"2 and 3","correct":true},{"id":1166931,"option":"3 and 4","correct":false},{"id":1166932,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark Streaming, which of the following statements about transformations in <strong>DStream</strong> are correct:</p>\n\n<ol>\n\t<li>The stateless transformation includes common RDD transformations such as map(), filter(), reduceByKey(), etc.</li>\n\t<li>The stateful transformation uses the intermediate results from previous batches and then computes the result of the present batch.</li>\n\t<li>Stateless transformations can combine data from only one DStreams at an instance of time.</li>\n</ol>\n\n<p> </p>","a":[{"id":1166909,"option":"1 and 2","correct":false},{"id":1166910,"option":"2 and 3","correct":true},{"id":1166911,"option":"1 and 3","correct":false},{"id":1166912,"option":"All of these","correct":false}]},{"q":"<p>In Spark Streaming, the following new <strong>StreamingContext</strong> object is created from an existing <strong>SparkContext</strong> object and a context is defined. Which of the following statements about this scenario are correct:</p>\n\n<p><strong>Object</strong></p>\n\n<pre class=\"prettyprint\"><code>import org.apache.spark.streaming._\nval sc = ...                \nval ssc = new StreamingContext(sc, Seconds(5))</code></pre>\n\n<p><strong>Statements</strong> </p>\n\n<ol>\n\t<li>Once a Context is started, new streaming computations cannot be set up or added</li>\n\t<li>stop() on Context can only stop the SparkContext</li>\n\t<li>Multiple StreamingContext can be active in a JVM at the same time</li>\n\t<li>Once a Context is stopped, it cannot be restarted</li>\n</ol>","a":[{"id":1166905,"option":"1 and 2","correct":false},{"id":1166906,"option":"2 and 3","correct":false},{"id":1166907,"option":"3 and 4","correct":false},{"id":1166908,"option":"1 and 4","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these tasks is performed by the following artifact for Scala/Java applications using SBT/Maven project definitions:</p>\n\n<pre class=\"prettyprint\"><code>NameID = org.apache.spark \nitemId = spark-streaming-kafka-0-8_2.11 \nv = 2.2.0 </code></pre>\n\n<p><strong>Tasks</strong> </p>\n\n<ol>\n\t<li>It removes your Kafka streaming application.</li>\n\t<li>It links your Kafka streaming application.</li>\n\t<li>It starts your Kafka streaming application.</li>\n</ol>","a":[{"id":1166783,"option":"1","correct":false},{"id":1166784,"option":"2","correct":true},{"id":1166785,"option":"3","correct":false},{"id":1166786,"option":"None of these","correct":false}]},{"q":"<p>In Kafka, you have created the following topic named <strong>sample</strong>. Which of these statements represent the properties of this topic correctly:</p>\n\n<pre class=\"prettyprint\"><code>&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample </code></pre>\n\n<p><strong>Properties </strong></p>\n\n<ol>\n\t<li>It is a topic with a single partition and only one replica.</li>\n\t<li>It is a topic with multiple partitions and many replicas.</li>\n\t<li>It is a topic with a single partition and many replicas.</li>\n\t<li>It is a topic with multiple partitions and only one replica.</li>\n</ol>","a":[{"id":1166759,"option":"1","correct":true},{"id":1166760,"option":"2","correct":false},{"id":1166761,"option":"3","correct":false},{"id":1166762,"option":"4","correct":false}]},{"q":"<p>In Kafka, which of the following statements about the <strong>consumer </strong>API is correct:</p>\n\n<ol>\n\t<li>It allows applications to subscribe to one or more topics and process the produced stream of records.</li>\n\t<li>It allows applications to subscribe to topics only and process the produced stream of records.</li>\n\t<li>It allows applications to subscribe to two topics only and process the produced stream of records.</li>\n</ol>","a":[{"id":1166751,"option":"1","correct":true},{"id":1166752,"option":"2","correct":false},{"id":1166753,"option":"3","correct":false},{"id":1166754,"option":"None of these","correct":false}]},{"q":"<p>In Kafka, which of the following statements about <strong>topic</strong> are correct:</p>\n\n<ol>\n\t<li>It is a feed name to which records are published.</li>\n\t<li>They are not multi-subscriber. Each topic can have only one consumer that can subscribe to the data.</li>\n</ol>","a":[{"id":1166743,"option":"1","correct":true},{"id":1166744,"option":"2","correct":false},{"id":1166745,"option":"All of these","correct":false},{"id":1166746,"option":"None of these","correct":false}]},{"q":"<p>Which of the following statements about Spark Streaming are correct:</p>\n\n<ol>\n\t<li>It is an extension of the core Spark API.</li>\n\t<li>It enables high-throughput and fault-tolerant stream processing of live data streams.</li>\n</ol>","a":[{"id":1166719,"option":"1","correct":false},{"id":1166720,"option":"2","correct":false},{"id":1166721,"option":"All of these","correct":true},{"id":1166722,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of the following statements about a <strong>rebalancing</strong> event is correct:</p>\n\n<ol>\n\t<li>It is a lifecycle event that occurs when consumers join or leave a consumer group.</li>\n\t<li>It is a lifecycle event that occurs when consumers are unable to enter a consumer group.</li>\n\t<li>It is a lifecycle event that occurs when a consumer deletes a consumer group.</li>\n</ol>","a":[{"id":1166747,"option":"1 ","correct":true},{"id":1166748,"option":"2 ","correct":false},{"id":1166749,"option":"3","correct":false},{"id":1166750,"option":"None of these","correct":false}]},{"q":"<p>Which of the following are core APIs of Kafka:</p>\n\n<ol>\n\t<li>Producer</li>\n\t<li>Consumer</li>\n\t<li>Connector</li>\n\t<li>Stream</li>\n</ol>","a":[{"id":1166739,"option":"1, 2, and 3","correct":false},{"id":1166740,"option":"2, 3, and 4","correct":false},{"id":1166741,"option":"1, 3, and 4","correct":false},{"id":1166742,"option":"All of these","correct":true}]},{"q":"<p>In Spark Streaming, you want to retrieve data from Kafka which is not present in the core API. Which of the following artifacts must be added to the dependencies to retrieve the required data in this scenario?</p>","a":[{"id":1166731,"option":"spark-streaming-flume_2.12","correct":false},{"id":1166732,"option":"spark-streaming-kinesis-asl_2.12","correct":false},{"id":1166733,"option":"spark-streaming-kafka-0-10_2.12","correct":true},{"id":1166734,"option":"spark-streaming-kafka-spark-streaming","correct":false}]},{"q":"<p>Which of the following statements represent the advantages of Spark Streaming:</p>\n\n<ol>\n\t<li>Quick recovery from failures</li>\n\t<li>Better load balancing</li>\n\t<li>Better resource usage</li>\n\t<li>Native integration with advanced processing libraries such as SQL, machine learning, and graph processing</li>\n</ol>","a":[{"id":1166727,"option":"1 and 2","correct":false},{"id":1166728,"option":"2, 3, and 4","correct":false},{"id":1166729,"option":"1, 2, and 3","correct":false},{"id":1166730,"option":"All of these","correct":true}]},{"q":"<p>You are given the following steps. Which of these sequences represent the internal working of Spark Streaming correctly:</p>\n\n<p><strong>Steps </strong></p>\n\n<ol>\n\t<li>Receives live input data streams</li>\n\t<li>Spark engine generates the final stream of results in batches</li>\n\t<li>Batches are then processed by the Spark engine</li>\n\t<li>Divides the data into batches</li>\n</ol>","a":[{"id":1166723,"option":"1=>3=>2=>4","correct":false},{"id":1166724,"option":"1=>3=>2=>4","correct":false},{"id":1166725,"option":"1=>4=>3=>2","correct":true},{"id":1166726,"option":"1=>2=>3=>4","correct":false}]},{"q":"<p>Which of these transformations will you use to join the following dataset and windowed stream:</p>\n\n<pre class=\"prettyprint\"><code>val myDataExample : RDD[String, String] = ...\nval myWindowedStream = stream.window(Seconds(100))...</code></pre>\n\n<p> </p>","a":[{"id":790063,"option":"myWindowedStream.transform { rdd => rdd.join(myDataExample) }","correct":true},{"id":790064,"option":"myWindowedStream.action{ rdd => rdd.join(myDataExample) }","correct":false},{"id":790065,"option":"rdd.join(myWindowedStream.transform(myDataExample) ","correct":false},{"id":790066,"option":"rdd.join(myWindowedStream.transform { rdd => (myDataExample) }","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following artifact is correct:</p>\n\n<p><strong>Artifact </strong></p>\n\n<pre class=\"prettyprint\"><code>ConsumerStrategies.Subscribe </code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>It allows you to subscribe to a fixed collection of topics.</li>\n\t<li>It allows you to use a regex to specify topics of interest.</li>\n\t<li>It allows you to distribute partitions evenly across available executors.</li>\n</ol>","a":[{"id":1167600,"option":"1","correct":true},{"id":1167601,"option":"2","correct":false},{"id":1167602,"option":"3","correct":false},{"id":1167603,"option":"None of these","correct":false}]},{"q":"<p>You are calling the <strong>reduceByKey(func, [numTasks])</strong> property on a DStream of <strong>(K, V)</strong> pairs that return a new DStream of <strong>(K, V)</strong> pairs.<br>\nIt uses the default number of parallel tasks that are available in the Spark framework. Which of the following correctly represents the value if you are calling the property in the cluster mode?</p>","a":[{"id":789984,"option":"0","correct":false},{"id":789985,"option":"1","correct":false},{"id":789986,"option":"2","correct":false},{"id":789987,"option":"Determined by the config property spark.default.parallelism","correct":true}]},{"q":"<p>In Spark shell, a <strong>SparkContext</strong> gate is already created in a variable <strong>sc</strong>. Which of the following arguments will you use to set the <strong>master </strong>instance that can connect you to the created variable?</p>","a":[{"id":787841,"option":"--connect","correct":false},{"id":787842,"option":"--master","correct":true},{"id":787843,"option":"--context","correct":false},{"id":787844,"option":"--connectionContext","correct":false}]},{"q":"<p>In Apache NiFi, if the <strong>ProcessException</strong> exception is thrown by the processor, then which of the following statements is correct in this scenario:</p>\n\n<ol>\n\t<li>It results in a condition wherein, any attempt to process the data will always lead to failure</li>\n\t<li>The flowfiles that are being processed are penalized</li>\n\t<li>Framework will rollback the session</li>\n</ol>","a":[{"id":786851,"option":"It results in a condition wherein, any attempt to process the data will always lead to failure","correct":false},{"id":786852,"option":"The flowfiles that are being processed are penalized","correct":false},{"id":786853,"option":"Framework will rollback the session","correct":false},{"id":786854,"option":"Both choice 2 and 3","correct":true}]},{"q":"<p>In Apache NiFi, which of the following processors cannot be used to publish data to an external source?</p>","a":[{"id":786847,"option":"PUTEmail","correct":false},{"id":786848,"option":"PUTHTTP","correct":true},{"id":786849,"option":"PUTSFTP","correct":false},{"id":786850,"option":"POSTHTTP","correct":false}]},{"q":"<p>Which of the following methods requires a scope to run in Apache NiFi:</p>\n\n<ol>\n\t<li>replace</li>\n\t<li>clear</li>\n\t<li>getState</li>\n</ol>","a":[{"id":786839,"option":"1 and 2","correct":false},{"id":786840,"option":"2 and 3","correct":false},{"id":786841,"option":"1 and 3","correct":false},{"id":786842,"option":"All of these","correct":true}]},{"q":"<p>In Apache NiFi, you must enqueue flow files for the processor before even triggering a process to run. Which of the following forms can be used to add data for the enqueue methods:</p>\n\n<ol>\n\t<li>byte[]</li>\n\t<li>InputStream</li>\n\t<li>Path</li>\n</ol>","a":[{"id":786835,"option":"3","correct":false},{"id":786836,"option":"2 and 3","correct":false},{"id":786837,"option":"1 and 3","correct":false},{"id":786838,"option":"All of these","correct":true}]},{"q":"<p>In Apache NiFi, a user changes the property values of a processor, therefore the <strong>onPropertyModified</strong> method is called. If the property is removed, then which of the following statements is true for this condition:</p>\n\n<p><strong>Method</strong></p>\n\n<p>onPropertyModified (PropertyDescriptor, Old value, New value)</p>\n\n<p><strong>Statements</strong></p>\n\n<ul>\n\t<li>Statement 1: Value of the third argument is NULL</li>\n\t<li>Statement 2: Value of the second argument is NULL</li>\n</ul>","a":[{"id":786819,"option":"1","correct":false},{"id":786820,"option":"2","correct":true},{"id":786821,"option":"Both of these","correct":false},{"id":786822,"option":"None of these","correct":false}]},{"q":"<p>In Apache NiFi, you are given an original file of <strong>7000</strong> bytes. You call the <strong>clone</strong> method with the following attributes:</p>\n\n<pre class=\"prettyprint\"><code>clone (flowFile, 500, 100)</code></pre>\n\n<p>Which of these statements about the output of the attribute is correct?</p>","a":[{"id":786815,"option":"The length of the content that is available in the newly-created FlowFile entity will be 100 bytes.","correct":true},{"id":786816,"option":"The length of the content that is available in the newly-created FlowFile entity will be 500 bytes.","correct":false},{"id":786817,"option":"The length of the content of the output file will start from the offset 100 of the original FlowFile..","correct":false},{"id":786818,"option":"None of these","correct":false}]},{"q":"<p>In Apache NiFi, if a new <strong>FlowFile </strong>has to be created that is only contiguous subset bytes of the original <strong>FlowFile</strong>, then which of the following methods will you use?</p>","a":[{"id":786811,"option":"Clone","correct":true},{"id":786812,"option":"Copy  ","correct":false},{"id":786813,"option":"Create","correct":false},{"id":786814,"option":"None of these","correct":false}]},{"q":"<p>In Apache NiFi, you are required to add your own user-defined attributes to the <strong>FlowFile</strong> entity at a specific memory in the flow. Which of the following processors will you use to complete your task?</p>","a":[{"id":786803,"option":"UpdateAttribute ","correct":true},{"id":786804,"option":"Addattribute ","correct":false},{"id":786805,"option":"AlterAttribute","correct":false},{"id":786806,"option":"UserAttribute","correct":false}]},{"q":"<p>In Apache NiFi, which of the following annotations from the processor's class signifies that a framework is allowed to invoke the processor multiple times with the same input without causing any unexpected results?</p>","a":[{"id":786831,"option":"EventDriven","correct":false},{"id":786832,"option":"InputRequirement","correct":false},{"id":786833,"option":"SupportsBatching","correct":false},{"id":786834,"option":"SideEffectFree","correct":true}]},{"q":"<p>In Apache NiFi, you have triggered a processor to run while it is being tested. This is done by calling the <strong>run</strong> method of the <strong>TestRunner</strong> class. You called this method without using any arguments. Which of the following annotations will allow any methods, that are available in the processor, to get invoked?</p>","a":[{"id":786827,"option":"@OnScheduled","correct":true},{"id":786828,"option":"@OnUnscheduled ","correct":false},{"id":786829,"option":"@OnStart ","correct":false},{"id":786830,"option":"@OnEnabled ","correct":false}]},{"q":"<p>Which of the following processors cannot be used to store data into Apache Nifi:</p>\n\n<ol>\n\t<li>GetFile</li>\n\t<li>GetSFTP</li>\n\t<li>ListenHTTP</li>\n\t<li>PostHTTP</li>\n</ol>","a":[{"id":786823,"option":"1","correct":false},{"id":786824,"option":"1 and 3","correct":false},{"id":786825,"option":"4","correct":true},{"id":786826,"option":"3","correct":false}]},{"q":"<p>In Apache NiFi, you can set the maximum lifespan of the <strong>flow.xml</strong> files by using the <strong>nifi.flow.configuration.archive.max.time*</strong> property that is available in the <strong>nifi.properties</strong> file in the <strong>conf </strong>directory. If you are unable to set an archive limitation in the <strong>nifi.properties</strong> file, then which of the following statements is correct?</p>","a":[{"id":786807,"option":"NiFi deletes archives that are older than 10 days ","correct":false},{"id":786808,"option":"NiFi deletes archives that are older than 30 days ","correct":true},{"id":786809,"option":"NiFi deletes archives that are older than 45 days ","correct":false},{"id":786810,"option":"NiFi never deletes any archive files","correct":false}]},{"q":"<p>In Apache NiFi, a method is called as a transaction on the <strong>ProcessSession</strong> attribute. The <strong>onTrigger</strong> method is throwing an exception when you are trying to end the transaction. Which of the following statements about this condition is not correct?</p>","a":[{"id":786791,"option":"AbstractProcessor catches the exception","correct":false},{"id":786792,"option":"AbstractProcessor calls the session.rollback() method","correct":false},{"id":786793,"option":"AbstractProcessor calls the commit() method","correct":true},{"id":786794,"option":"AbstractProcessor catches the exception again","correct":false}]},{"q":"<p>In Apache NiFi, the initialization method is provided by both the <strong>Processor </strong>interface and the <strong>ControllerService</strong> interface for initialization purpose. Which of the following arguments is passed to this method if you are using the method that is provided by the <strong>ControllerService </strong>interface?</p>","a":[{"id":786787,"option":"ControllerServiceInitializationContext","correct":true},{"id":786788,"option":"ControllerServiceProcessorInitializationContext","correct":false},{"id":786789,"option":"ProcessorInitializationContext","correct":false},{"id":786790,"option":"ControllerServiceContext","correct":false}]},{"q":"<p>You are testing an Apache NiFi processor. You have created a <strong>newTestRunner</strong> class by calling the static <strong>newTestRunner</strong> method of the <strong>TestRunners</strong> class. Which of the following arguments is passed in this method:</p>\n\n<ol>\n\t<li>Instances of a Processor</li>\n\t<li>Class of the Processor for testing</li>\n\t<li>A ProcessContext</li>\n</ol>","a":[{"id":786779,"option":"1","correct":false},{"id":786780,"option":"2 and 3","correct":false},{"id":786781,"option":"1 and 2","correct":true},{"id":786782,"option":"1 and 3","correct":false}]},{"q":"<p>You are adding a Controller Service to the TestRunner class that is created by calling the static new <strong>TestRunner</strong> method to test the processor.<br>\nIn order to configure this <strong>Controller </strong>Service, you have called the <strong>setProperty()</strong> method. Which of the following is not the correct set of arguments that is accepted by this method?</p>","a":[{"id":786775,"option":"ControllerService, PropertyDescriptor, String  ","correct":false},{"id":786776,"option":"ControllerService, String, String ","correct":false},{"id":786777,"option":"ControllerService, PropertyDescriptor, AllowableValue ","correct":false},{"id":786778,"option":"ControllerService, String, AllowableValue","correct":true}]},{"q":"<p>In Apache NiFi, which of the following statements must be called when a connection, whose destination is the processor, contains at least one FlowFile in its queue and the processor has no incoming connections?</p>","a":[{"id":785740,"option":"onTrigger","correct":true},{"id":785741,"option":"getControllerServiceLookup","correct":false},{"id":785742,"option":"getSupportedPropertyDescriptors","correct":false},{"id":785743,"option":"getState","correct":false}]},{"q":"<p>In Apache NiFi, you are using the <strong>getStateManager()</strong> method to provide a simple API for storing and retrieving a state. What should be the maximum size of the State map after being serialized so that you do not get any exceptions when you store <strong>Map&lt;String, String&gt;</strong>?</p>","a":[{"id":785732,"option":"1 KB","correct":false},{"id":785733,"option":"1 MB","correct":true},{"id":785734,"option":"10MB","correct":false},{"id":785735,"option":"2 MB","correct":false}]},{"q":"<p>In Apache NiFi, you are required to store a set of key/value pairs and update them atomically to the Simple state. You want the state that is saved across all nodes in a cluster to provide API for storing and retrieving state. Which of the following components allows you to call the state manager by using the <strong>getStateManager()</strong> method:</p>\n\n<ol>\n\t<li>ProcessContex</li>\n\t<li>ReportingContext</li>\n\t<li>ControllerServiceInitializationContext</li>\n\t<li>ValidationContext</li>\n</ol>","a":[{"id":785724,"option":"1 and 2","correct":false},{"id":785725,"option":"2, 3, and 4","correct":false},{"id":785726,"option":"1, 3 and 4","correct":false},{"id":785727,"option":"1, 2, and 3","correct":true}]},{"q":"<p>In Apache NiFi, if the Apache NiFi processor is instantiated by using Java's <strong>ServiceLoader</strong> mechanism, then which of the following techniques will you use to create an object in the processor?</p>","a":[{"id":785720,"option":"Creating the clone() method","correct":false},{"id":785721,"option":"By deserialization","correct":false},{"id":785722,"option":"By using a constructor","correct":true},{"id":785723,"option":"None of these","correct":false}]},{"q":"<p>In Apache NiFi, you are using Java's <strong>ServiceLoader</strong> mechanism to load and instantiate the processor of NiFi. Which of the following text file entries will you make in the<strong> META-INF/services</strong> directory in the Processor's JAR file to add the valid class name of the processor?</p>","a":[{"id":785716,"option":"org.apache.nifi.processor.Processor","correct":true},{"id":785717,"option":"org.apache.nifi.processor.AbstractProcessor","correct":false},{"id":785718,"option":"org.apache.nifi.flowfile.attributes.CoreAttributes","correct":false},{"id":785719,"option":"All of these","correct":false}]},{"q":"<p>In Apache NiFi, you are using the <strong>ExecuteProcess </strong>processor but you determined that the system call is not receiving any inputs. Which of the following processors will you use to provide input to the process in this condition?</p>","a":[{"id":785712,"option":"ExecuteStreamCommand","correct":true},{"id":785713,"option":"ExecuteProcessCommand","correct":false},{"id":785714,"option":"ExecuteProcessInput","correct":false},{"id":785715,"option":"IngestStreamProcess","correct":false}]},{"q":"<p>You want to use a processor that is provided by Apache Nifi. This processor can update the database. Which of the following processors will you use to perform your task by executing the SQL DDM statement that defined by the <strong>FlowFile’s</strong> content?</p>","a":[{"id":785708,"option":"ExecuteSQL","correct":false},{"id":785709,"option":"PutHiveQL","correct":false},{"id":785710,"option":"SelectHiveQL","correct":false},{"id":785711,"option":"PutSQL","correct":true}]},{"q":"<p>In Apache NiFi, you are required to set an expiration for the data that is currently being transferred. You changed the value from '0 sec' to '20 sec'. Which of the following event is created when the data in the connection reaches the defined duration and gets deleted?</p>","a":[{"id":785333,"option":"Backpressure event","correct":false},{"id":785334,"option":"EXPIRE Provenance event","correct":true},{"id":785335,"option":"DELETE Proverance event","correct":false},{"id":785336,"option":"Log EXPIRE event","correct":false}]},{"q":"<p>In Apache NiFi, if you are getting the retrieved values as NULL for a processor that has stored a value with the key of the Demo key by using the <strong>scope(A)</strong> method. The processor is also trying to retrieve the value by using the <strong>scope(B)</strong> method. Which of the following correctly represents <strong>A</strong> and <strong>B</strong>?</p>","a":[{"id":785325,"option":"A. Scope.CLUSTER scope\r\nB. Scope.GLOBAL scope","correct":false},{"id":785326,"option":"A. Scope.LOCAL scope\r\nB. Scope.CLUSTER scope","correct":false},{"id":785327,"option":"A. Scope.LOCAL scope\r\nB. Scope.GLOBAL scope","correct":false},{"id":785328,"option":"A. Scope.CLUSTERscope\r\nB. Scope.LOCALscope","correct":true}]},{"q":"<p>Which of the following Java annotations in an Apache NiFi component do not take any arguments:</p>\n\n<ol>\n\t<li>@OnRemoved</li>\n\t<li>@OnStopped</li>\n\t<li>@OnShutdown</li>\n</ol>","a":[{"id":785321,"option":"1","correct":false},{"id":785322,"option":"2","correct":false},{"id":785323,"option":"3","correct":false},{"id":785324,"option":"1 and 3","correct":true}]},{"q":"<p>In Apache NiFi, if a processor is created, then which of the following methods of the <strong>AbstractProcessor </strong>class is invoked before all other methods?</p>","a":[{"id":785317,"option":"Start()","correct":false},{"id":785318,"option":"Assign()","correct":false},{"id":785319,"option":"Destroy()","correct":false},{"id":785320,"option":"Init()","correct":true}]},{"q":"<p>In Apache NiFi, the core attributes for the <strong>FlowFiles</strong> record are declared in the <strong>org.apache.nifi.flowfile.attributes.CoreAttributes</strong> enum. Which of the following values of the attribute contains the name of a file with the <strong>CoreAttribute</strong> enum?</p>","a":[{"id":785313,"option":"Path","correct":false},{"id":785314,"option":"Absolute.path","correct":false},{"id":785315,"option":"Path.flow","correct":false},{"id":785316,"option":"None of these","correct":true}]},{"q":"<p>If the Apache Airflow framework returns a value from the PythonOperator’s <strong>python_callable</strong> function, then which of the following statements is correct?</p>","a":[{"id":787586,"option":"An XCom containing that value is automatically pushed.","correct":true},{"id":787587,"option":"The value returned by the task is automatically pulled by all the existing XComs.","correct":false},{"id":787588,"option":"The DAG generates a separate task to pull the value returned via a XCom.","correct":false},{"id":787589,"option":"None of these","correct":false}]},{"q":"<p>If you are integrating the Kerberos protocol with the Apache Airflow framework, then which of the following limitations is valid</p>\n\n<ol>\n\t<li>All hooks that are available in Airflow cannot be used in such an implementation.</li>\n\t<li>You must rely on network-level security since the web interface does not integrate with Kerberos.</li>\n</ol>","a":[{"id":787582,"option":"1","correct":false},{"id":787583,"option":"2","correct":false},{"id":787584,"option":"Both of these","correct":true},{"id":787585,"option":"None of these","correct":false}]},{"q":"<p>You are using the <strong>Active Directory</strong> while working with Apache Airflow. However, you have not explicitly specified an OU that your users are in. Which of the following search scopes you must set for the implementation of an <strong>LDAP</strong> authentication?</p>","a":[{"id":787578,"option":"TREE","correct":false},{"id":787579,"option":"SUBTREE","correct":true},{"id":787580,"option":"ROOT","correct":false},{"id":787581,"option":"None of these","correct":false}]},{"q":"<p>You are required to specify the <strong>authMechanism</strong> technique for the following Apache Airflow module:</p>\n\n<pre class=\"prettyprint\"><code>airflow.hooks.base_hook.BaseHook</code></pre>\n\n<p>Which of these parts of the connection allows you to specify the additional configuration that performs the same operation?</p>","a":[{"id":787574,"option":"Auth","correct":false},{"id":787575,"option":"Extra","correct":true},{"id":787576,"option":"Additional","correct":false},{"id":787577,"option":"Route","correct":false}]},{"q":"<p>In Apache Airflow, which of these will you use to override the following parameter that you have passed while working with Apache Airflow:</p>\n\n<pre class=\"prettyprint\"><code>{\"hive_cli_params\": \"-hiveconf mapred.job.tracker=some.jobtracker:444\"}</code></pre>\n\n<p> </p>","a":[{"id":787566,"option":"hive_conf in run_cli","correct":true},{"id":787567,"option":"beeline","correct":false},{"id":787568,"option":"auth","correct":false},{"id":787569,"option":"_prepare_cli","correct":false}]},{"q":"<p>In Apache Airflow, which of the following can be used to keep the record of SLA failures and avoid double triggering of alert emails?</p>","a":[{"id":787558,"option":"airflow.models.base","correct":true},{"id":787559,"option":"airflow.models.KubeWorkerIdentifier","correct":false},{"id":787560,"option":"airflow.models.KubeResourceVersion","correct":false},{"id":787561,"option":"airflow.models.vector","correct":false}]},{"q":"<p>Which of these statements about the implementation of the following object in the Apache Airflow platform is correct:</p>\n\n<pre class=\"prettyprint\"><code>airflow.models.DagPickle(dag)[source]</code></pre>\n\n<p> </p>","a":[{"id":787554,"option":"This object represents a version of a DAG","correct":false},{"id":787555,"option":"This object becomes a source of truth for a BackfillJob execution","correct":false},{"id":787556,"option":"Both of these","correct":true},{"id":787557,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, you called the Bash script by using the <strong>bash_command</strong> argument (<strong>bash_command=\"/home/batcher/test.sh</strong>) while using the <strong>BashOperator</strong> operator in the Bash shell. This execution failed and the '<strong>Jinja template not found</strong>' exception got raised. Which of the following techniques must be used to resolve this issue?</p>\n\n<p> </p>","a":[{"id":787546,"option":"Modify the directory (bash_command=\"/home/script_file/test.sh \"","correct":false},{"id":787547,"option":"Modify the directory (bash_command=\"/etc/script_file/test.sh \"","correct":false},{"id":787548,"option":"Add a space after the script name (bash_command=\"/home/batcher/test.sh \"","correct":true},{"id":787549,"option":"Modify the directory by adding the script_file folder (bash_command=\"/home/script_file/script_file/test.sh \"","correct":false}]},{"q":"<p>In Apache Airflow, which of the following sequences of steps is used to add module dependencies to your DAG?</p>","a":[{"id":787542,"option":"1. mkdir zip_dag_contents                   cd zip_dag_contents                     \r\n2. pip install --install-option=\"--install-lib=$PWD\" my_useful_package               cp ~/my_dag.py .           \r\n3. zip -r zip_dag.zip *","correct":false},{"id":787543,"option":"1. virtualenv zip_dag                source zip_dag/bin/activate                          \r\n2. mkdir zip_dag_contents                   cd zip_dag_contents                     \r\n3. pip install --install-option=\"--install-lib=$PWD\" my_useful_package               cp ~/my_dag.py .           \r\n4. zip -r zip_dag.zip *","correct":true},{"id":787544,"option":"1. mkdir zip_dag_contents                   cd zip_dag_contents                    \r\n2. zip -r zip_dag.zip *","correct":false},{"id":787545,"option":"1. virtualenv zip_dag                source zip_dag/bin/deactivate                         \r\n2. mkdir zip_dag_contents                   cd zip_dag_contents                     \r\n3. pip install --install-option=\"--install-lib=$PWD\" my_useful_package .           \r\n4. zip -r zip_dag.zip *","correct":false}]},{"q":"<p>You have terminated an Apache Airflow process externally but its status is still running the database. Which of the following techniques allows you to terminate this process permanently?</p>","a":[{"id":787534,"option":"By deleting rows in the task instances","correct":false},{"id":787535,"option":"By verifying the state of the process as part of the heartbeat routine","correct":false},{"id":787536,"option":"Process  of termination is performed periodically by the scheduler’s process","correct":true},{"id":787537,"option":"By changing its end_time schedule to 'now'","correct":false}]},{"q":"<p>In Apache Airflow, the start date of your DAG scheduler is set to '<strong>2018-04-21</strong>' and you turn on the scheduling toggle at '<strong>2018-04-22T00:00:00</strong>'. Your DAG is configured to run on an hourly basis. Which of the following conditions can occur in this scenario?</p>","a":[{"id":787530,"option":"Scheduler backfills 24 DAG runs and then start running on the scheduled interval","correct":true},{"id":787531,"option":"Scheduler instantly set the start_date parameter at '2018-04-22T00:00:00' and continue previous running on scheduled interval","correct":false},{"id":787532,"option":"Scheduler continues to run on the scheduled interval","correct":false},{"id":787533,"option":"Scheduler backfills one DAG run and then start running on the scheduled interval","correct":false}]},{"q":"<p>In Apache Airflow, you have used a CLI '<strong>airflow clear -h</strong>' command for clearing a task instance. What is the function of this instance:</p>\n\n<ol>\n\t<li>Update the max_tries property </li>\n\t<li>Delete the task instance record</li>\n\t<li>Set the current task instance state to be <strong>None</strong></li>\n\t<li>Remove the task instance that is marked as <strong>Success</strong></li>\n\t<li>Remove the task instance that is marked as <strong>Failed</strong></li>\n</ol>","a":[{"id":787159,"option":"2","correct":false},{"id":787160,"option":"1 and 3","correct":true},{"id":787161,"option":"2, 3, and 5","correct":false},{"id":787162,"option":"1, 3, and 5","correct":false}]},{"q":"<p>In Apache Airflow, your DAG is written in such a way that it can handle its own catchup. Your task is to turn off the catchup on the DAG itself. Which of the configurations will you use to complete this task?</p>","a":[{"id":787155,"option":"dag_default_catchup = False","correct":false},{"id":787156,"option":"default_dag_catchup = False","correct":false},{"id":787157,"option":"catchup_by_default = False","correct":false},{"id":787158,"option":"dag.catch= False","correct":true}]},{"q":"<p>In Apache Airflow, which of the following arguments will you use to stop the scheduling process of your DAG?</p>","a":[{"id":787151,"option":"schedule_interval='None'","correct":false},{"id":787152,"option":"dag_schedule='None'","correct":false},{"id":787153,"option":"schedule_interval=None","correct":true},{"id":787154,"option":"dag_schedule=None","correct":false}]},{"q":"<p>In Apache Airflow, if processes are integrated with the <strong>upstart</strong> file, then which of the following will you use to manually start a process, stop a process, and view the status of a process?</p>","a":[{"id":787143,"option":"airflow airflow-webserver status","correct":false},{"id":787144,"option":"initctl airflow-webserver status","correct":true},{"id":787145,"option":"airflow airflow-process status","correct":false},{"id":787146,"option":"airflow airflow-webprocess status","correct":false}]},{"q":"<p>In Apache Airflow, which of the following techniques is used to automatically start the system boot by using the <strong>upstart</strong> file?</p>","a":[{"id":787139,"option":"A corresponding *.conf file in /etc/conf","correct":false},{"id":787140,"option":"Define configuration at AIRFLOW_CONFIG","correct":false},{"id":787141,"option":"Define configuration at AIRFLOW_HOME","correct":false},{"id":787142,"option":"A corresponding *.conf file in /etc/init","correct":true}]},{"q":"<p>In Apache Airflow, if you set the value of <strong>depends_on_set</strong> to <strong>true</strong> while branching, then which of the following conditions can occur?</p>","a":[{"id":787135,"option":"Only one path at each DagRun is available","correct":false},{"id":787136,"option":"DAG gets completely locked","correct":true},{"id":787137,"option":"Apache Airflow crashes","correct":false},{"id":787138,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, you are given two XComs that contain similar keys. Which of the following conditions will you allow you to determine which XCom gets retrieved first?</p>","a":[{"id":787131,"option":"The XCOM having the most recent execution_date gets retrieved first by default. ","correct":true},{"id":787132,"option":"The XCOM having the least execution_date gets retrieved first by default. ","correct":false},{"id":787133,"option":"You cannot have two XCOMs with the same keys. ","correct":false},{"id":787134,"option":"It cannot be predicted. ","correct":false}]},{"q":"<p>Which of the following is the correct location to store your plugins so that they can be used by Apache Airflow?</p>","a":[{"id":787127,"option":"In the $AIRFLOW_HOME/plugins folder ","correct":true},{"id":787128,"option":"In the $AlRFLOW_HOME/addons folder","correct":false},{"id":787129,"option":"In the $AlRFLOW_HOME/config folder","correct":false},{"id":787130,"option":"In the $AIRFLOW_HOME/Setup folder ","correct":false}]},{"q":"<p>In Apache Airflow, if you add a new container to an existing multi-container application, then what is the purpose of the <strong>--network</strong> option in the <strong>docker-compose</strong> container?</p>","a":[{"id":787123,"option":"The --network option allows the new container to create a new network ","correct":false},{"id":787124,"option":"The --network option allows the new container to be connected with the host network ","correct":false},{"id":787125,"option":"The --network option allows the new container to be connected with an existing network","correct":true},{"id":787126,"option":"The --network option can be used to create an isolated container ","correct":false}]},{"q":"<p>In Apache Airflow, which of the following statements is the description of the <strong>sensor</strong> operator?</p>","a":[{"id":787119,"option":"It is a long running task that is waiting for no operation to perform. ","correct":false},{"id":787120,"option":"It is a task that allows to transfer data from a source system to a destination system.","correct":false},{"id":787121,"option":"It is a long running task that is waiting for an event to happen. A poke function is called every n seconds to check if the criteria are met.","correct":true},{"id":787122,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, you are required to modify the configuration value of the dags_folder parameter by using an environment variable. Which of the following is the correct technique to perform this task?</p>","a":[{"id":787115,"option":"By setting the environment variable AIRFLOW DAGS_FOLDER=\"your/path/dags\" ","correct":false},{"id":787116,"option":"By setting the environment variable AIRFLOW CORE_DAGS_FOLDER=\"your/path/dags\" ","correct":true},{"id":787117,"option":"By setting the environment variable DAGS_FOLDER=\"your/path/dags\"","correct":false},{"id":787118,"option":"None of these","correct":false}]},{"q":"<p>In Apache Airflow, which of the following operations represents the dependency into a DAG while performing a task?</p>","a":[{"id":787107,"option":"Using the set_upstream and set_downstream functions ","correct":false},{"id":787108,"option":"Using bit shift operators such as >> and << ","correct":false},{"id":787109,"option":"Using the set_upstream and set_downstream functions and bit shift operators such as >> and <<","correct":true},{"id":787110,"option":"None of these","correct":false}]},{"q":"<p>You are required to set up the Apache Airflow platform for a reverse proxy. Which of the following settings will you set up in your <strong>airflow.cfg</strong> file to perform this task?</p>","a":[{"id":787103,"option":"sql_alchemy_conn_cmd = bash_command_to_run","correct":false},{"id":787104,"option":"base_url = https://lab.mycompany.com/myorg/airflow/","correct":false},{"id":787105,"option":"base_url = http://my_host/myorg/airflow","correct":true},{"id":787106,"option":"base_url = /myorg/flower","correct":false}]},{"q":"<p>The Apache Airflow platform is throwing an exception due to the non-availability of arguments in tasks. Which of the following arguments must be included in the respective task to avoid this scenario:</p>\n\n<ol>\n\t<li>Owner</li>\n\t<li>Dag_id</li>\n\t<li>Task_id</li>\n\t<li>Schedule_interval</li>\n</ol>","a":[{"id":786995,"option":"1 and 2","correct":false},{"id":786996,"option":"1 and 3","correct":true},{"id":786997,"option":"1 and 4","correct":false},{"id":786998,"option":"2 and 3","correct":false}]},{"q":"<p>In Airflow, tasks can remove XComs at any time to make it available for other tasks. Which of the following techniques can be used by a task for pushing an XCom:</p>\n\n<ol>\n\t<li>By calling the <strong>xcom_push()</strong> method</li>\n\t<li>By returning a value from the <strong>execute(</strong>) method that available in operators</li>\n\t<li>By returning a value from the PythonOperator’s <strong>python_callable</strong> function<br>\n\t </li>\n</ol>","a":[{"id":786991,"option":"1","correct":false},{"id":786992,"option":"1 and 2","correct":false},{"id":786993,"option":"2 and 3","correct":false},{"id":786994,"option":"All of these","correct":true}]},{"q":"<p>You are given a function. This function contains two DAGs, <strong>dag_1</strong> and <strong>dag_2</strong>. Which of the following DAG objects is stored by the Apache Airflow platform:</p>\n\n<pre class=\"prettyprint\"><code>dag_1 = DAG('this_dag_will_be_discovered')\n\ndef my_function( ):\n    dag_2 = DAG('this_dag_will_not_be_discovered')\n\nmy_function( )</code></pre>\n\n<p> </p>","a":[{"id":786983,"option":"dag_1","correct":true},{"id":786984,"option":"dag_2","correct":false},{"id":786985,"option":"Both of these","correct":false},{"id":786986,"option":"None of these","correct":false}]},{"q":"<p>While searching for DAGs in the Apache Airflow platform, the platform only considers the Python files that contain the strings '<strong>airflow</strong>' and '<strong>DAG</strong>'. Assume that you are required to consider all the Python files. Which of the following configuration flags must be disabled to perform this task?<br>\n </p>","a":[{"id":786979,"option":"DAG_FOLDER","correct":false},{"id":786980,"option":"AIRFLOW_HOME","correct":false},{"id":786981,"option":"AIRFLOW_CONFIG","correct":false},{"id":786982,"option":"DAG_DISCOVERY_SAFE_MODE","correct":true}]},{"q":"<p>Which of the following order of precedence is correct to perform tasks in Apache Airflow:</p>\n\n<ol>\n\t<li>Explicitly passed arguments</li>\n\t<li>Operator’s default value</li>\n\t<li>Values that exist in the <strong>default_args</strong> dictionary</li>\n</ol>","a":[{"id":786975,"option":"1 -> 2 -> 3","correct":false},{"id":786976,"option":"2 -> 3 -> 1","correct":false},{"id":786977,"option":"2 -> 1 -> 3","correct":false},{"id":786978,"option":"1 -> 3 -> 2","correct":true}]},{"q":"<p>In Apache Airflow, if you want to prevent overloading users with logging messages about skipped files while configuring the Airflow settings. Therefore, a file is skipped only once per <strong>DagBag</strong> file. Which of the following will you use to perform this task? </p>","a":[{"id":786967,"option":"dag_folder (unicode) parameter of dagbag","correct":false},{"id":786968,"option":"has_logged parameter of dagbag","correct":true},{"id":786969,"option":"executor parameter of dagbag","correct":false},{"id":786970,"option":"include_examples (bool) parameter of dagbag","correct":false}]},{"q":"<p>In Apache Airflow, which of the following storage locations is used to store the DAG directory?</p>","a":[{"id":786959,"option":"[AIRFLOW_ENV]/dags","correct":false},{"id":786960,"option":"[AIRFLOW_HOME]/dags","correct":true},{"id":786961,"option":"[AIRFLOW_HOME]/conf/dags","correct":false},{"id":786962,"option":"[AIRFLOW_CONF]/dags","correct":false}]},{"q":"<p>You are required to perform a task in isolation, without affecting the metadata database or being concerned about task dependencies. Which of the following commands will you use while developing the Apache Airflow platform to perform this task?</p>","a":[{"id":786955,"option":"airflow backfill DAG_ID TASK_ID -s START_DATE -e END_DATE","correct":false},{"id":786956,"option":"airflow resetdb","correct":false},{"id":786957,"option":"airflow clear DAG_ID","correct":false},{"id":786958,"option":"airflow test DAG_ID TASK_ID EXECUTION_DATE","correct":true}]},{"q":"<p>Which of the following tasks is performed by hooks in Apache Airflow?</p>","a":[{"id":786951,"option":"Limits the execution parallelism on arbitrary sets of tasks","correct":false},{"id":786952,"option":"Implements a common interface when possible and act as a building block for operators","correct":true},{"id":786953,"option":"Allows tasks to exchange messages implementing more nuanced forms of control and shared state","correct":false},{"id":786954,"option":"Stores and retrieves arbitrary content or settings as a simple key value store within Airflow","correct":false}]},{"q":"<p>In Apache Airflow, which of the following statements about operators is correct?</p>","a":[{"id":786939,"option":"It is a task that is assigned to a DAG","correct":false},{"id":786940,"option":"It is a parameterized instance of an operator","correct":false},{"id":786941,"option":"It is a class that acts as a template for performing a task","correct":true},{"id":786942,"option":"It is a description of the order in which task must be performed","correct":false}]},{"q":"<p>Which of the following conditions makes the status of an Apache Airflow instance component unhealthy?</p>","a":[{"id":786935,"option":"When the last heartbeat was received more than 30 seconds earlier than the current time","correct":true},{"id":786936,"option":"When the last heartbeat was received less than 30 seconds earlier than the current time","correct":false},{"id":786937,"option":"When the last heartbeat was received more than 60 seconds earlier than the current time","correct":false},{"id":786938,"option":"When the last heartbeat was received less than 20 seconds earlier than the current time","correct":false}]},{"q":"<p>In Apache Airflow, you are required to enable the Kerberos protocol. To complete this task, you must generate a (service) key tab. Which of the following storage locations allows you to store the file so that Airflow users can read it?</p>","a":[{"id":786927,"option":"/etc/airflow/sources/airflow.keytab","correct":false},{"id":786928,"option":"/etc/airflow/docs/airflow.keytab","correct":false},{"id":786929,"option":"/etc/airflow/airflow.keytab","correct":true},{"id":786930,"option":"/etc/airflow/airflow.keytab","correct":false}]},{"q":"<p>In Apache Airflow, if you want to deserialize a JSON object from a variable, then which of the following statements will you use to perform this task?</p>","a":[{"id":786923,"option":"echo {{ var.value.<variable_name> }}","correct":false},{"id":786924,"option":"variable_name = Variable.get(\"variable_name\", default_var=None)","correct":false},{"id":786925,"option":"variable_name = Variable.get(\"variable_name\")","correct":false},{"id":786926,"option":"echo {{ var.json.<variable_name> }}","correct":true}]},{"q":"<p>In Apache Airflow, which of the following components is used to create a view into the Airflow UI?</p>","a":[{"id":786919,"option":"Blueprint ","correct":false},{"id":786920,"option":"Admin View","correct":false},{"id":786921,"option":"Template ","correct":false},{"id":786922,"option":"All of these","correct":true}]},{"q":"<p>In Spark, you are using the built-in DataFrames aggregation functions. You cannot perform the assigned task by using these built-in functions. If you are creating a custom untyped aggregate function, then which of the following abstract classes must be extended to implement this custom function?</p>","a":[{"id":787901,"option":"AggregateFunction","correct":false},{"id":787902,"option":"UserDefinedAggregateFunction","correct":true},{"id":787903,"option":"Aggregator","correct":false},{"id":787904,"option":"DataFrameUserDefined","correct":false}]},{"q":"<p>In Spark, you are required to store the persisted RDD by using different storage levels while working with RDD. You are using the <strong>cache()</strong> method for serving the required objective. Which of the following storage levels would have been used in the given scenario?</p>","a":[{"id":789226,"option":"StorageLevel.MEMORY_ONLY","correct":true},{"id":789227,"option":"StorageLevel.MEMORY_AND_DISK","correct":false},{"id":789228,"option":"StorageLevel.MEMORY_ONLY_SER ","correct":false},{"id":789229,"option":"StorageLevel.DISK_ONLY","correct":false}]},{"q":"<p>Your task is to save DataFrames as persistent tables in the Hive metastore. Which of these Spark commands will you use for this purpose so that the contents of the DataFrame can be materialized and a pointer to the data that are available in the Hive metastore are created during the process?</p>","a":[{"id":789230,"option":"createOrReplaceTempView ","correct":false},{"id":789231,"option":"saveAsTable ","correct":true},{"id":789232,"option":"saveAsView","correct":false},{"id":789233,"option":"createOrReplaceTempTable","correct":false}]},{"q":"<p>You are required to use the shared variables that are provided by the Spark framework. The tasks across multiple stages require the same data and data caching must be done in the deserialized format. Which of the following shared variables will you explicitly create in this scenario?</p>","a":[{"id":789242,"option":"Global","correct":false},{"id":789243,"option":"Broadcast","correct":true},{"id":789244,"option":"Accumulator","correct":false},{"id":789245,"option":"Aggregator","correct":false}]},{"q":"<p>You created a numeric accumulator by calling the <strong>SparkContext.longAccumulator()</strong> method to accumulate the values of the <strong>Long</strong> data type. The tasks that are running on a cluster are also added to the accumulator by using the <strong>add</strong> method. If the value method is used to read the accumulator's value, then which of the following statements is correct?</p>","a":[{"id":789246,"option":"Tasks can read the accumulator's value","correct":false},{"id":789247,"option":"Driver program cannot read the accumulator’s value","correct":false},{"id":789248,"option":"Driver program can read the accumulator’s value","correct":true},{"id":789249,"option":"Both the tasks and the driver program can read the accumulator's value","correct":false}]},{"q":"<p>In Spark, you are applying certain operations that are causing a <strong>shuffle</strong> event. The data for this shuffle event must be organized and aggregated. Which of the following statements is correct regarding the organization and aggregation of the data?</p>","a":[{"id":789612,"option":"Spark generates a set of map tasks for organization and a set of reduce tasks for aggregation","correct":true},{"id":789613,"option":"Spark generates a set of reduce tasks for organization and a set of map tasks for aggregation","correct":false},{"id":789614,"option":"Spark generates a set of map tasks for organization as well as for aggregation","correct":false},{"id":789615,"option":"Spark generates a set of reduce tasks for organization as well as for aggregation","correct":false}]},{"q":"<p>You are running a Spark Streaming program on a cluster. Before running the application, you came to know that the number of cores that are allocated to the application is less than the number of receivers. Which of the following conditions can occur if the application is running in the <strong>cluster</strong> mode?</p>","a":[{"id":789656,"option":"System receives data but it cannot process it","correct":true},{"id":789657,"option":"System receives data and process it","correct":false},{"id":789658,"option":"System cannot receive or process data","correct":false},{"id":789659,"option":"An exception is thrown","correct":false}]},{"q":"<p>You are applying an operation that is causing a shuffle event. Therefore, a lot of heap memory is getting consumed. While implementing this operation, you find that the data size is more than the provided memory. The Spark framework is spilling these tables to a disk. </p>\n\n<p>Which of the following conditions can occur in this scenario?</p>","a":[{"id":789933,"option":"Garbage collection decreases","correct":false},{"id":789934,"option":"Cause an overhead of network I/O","correct":false},{"id":789935,"option":"Cause an overhead of disk I/O","correct":true},{"id":789936,"option":"None of these","correct":false}]},{"q":"<p>To maintain an arbitrary state in a Spark streaming application, you are using the <strong>updateStateByKey</strong> operation. The state update function is applied to all existing keys in a batch by the Spark framework to complete this operation. Which of the following values is returned if the update function that returns the key-value pair gets eliminated?</p>","a":[{"id":790001,"option":"0","correct":false},{"id":790002,"option":"1","correct":false},{"id":790003,"option":"Null","correct":false},{"id":790004,"option":"None of these","correct":true}]},{"q":"<p>In Spark, you are using the table partitioning technique as an optimization approach. You noticed that the data types of the columns are automatically retrieved during partitioning. You configured the <strong>spark.sql.sources.partitionColumnTypeInference.enabled</strong> property and set it to false in the given scenario. Which of the following data types will you use to partition the columns once you disable the configuration?</p>","a":[{"id":790038,"option":"Numeric","correct":false},{"id":790039,"option":"String","correct":true},{"id":790040,"option":"Date","correct":false},{"id":790041,"option":"Timestamp","correct":false}]},{"q":"<p>In Spark, you are using the following properties to read data from a database by using the JDBC API:</p>\n\n<ul>\n\t<li>partitionColumn</li>\n\t<li>lowerBound</li>\n\t<li>upperBound</li>\n</ul>\n\n<p>Along with these properties, which of the following properties must be used to specify the technique through which the table is partitioned while it is being read from multiple workers?</p>","a":[{"id":790100,"option":"fetchPartitions ","correct":false},{"id":790101,"option":"numPartitions ","correct":true},{"id":790102,"option":"createPartitions ","correct":false},{"id":790103,"option":"cascadePartitions","correct":false}]},{"q":"<p>You are using the <strong>dataFrame.cache()</strong> method that is provided by the Spark SQL module to cache tables by using an in-memory columnar format. To minimize the memory usage and GC pressure, the Spark SQL module scans only the required columns and automatically tunes the compression process. Which of the following will you use to remove the table from the memory?</p>","a":[{"id":790112,"option":"spark.catalog.uncacheTable(\"tableName\")","correct":true},{"id":790113,"option":"spark.catalog.cacheTable(\"tableName\")","correct":false},{"id":790114,"option":"spark.sql.inMemoryColumnarStorage.compressed(\"tableName\")","correct":false},{"id":790115,"option":"spark.sql.inMemoryColumnarStorage.catchTable(\"tableName\")","correct":false}]},{"q":"<p>You are required to call the <strong>groupByKey([numPartitions])</strong> transformation on a dataset of <strong>(K, V)</strong> pairs to retrieve a dataset of <strong>(K, Iterable&lt;V&gt;)</strong> pairs. You are grouping them in order to perform an aggregation. The final result contains a level of parallelism in the Spark framework. Which of the following factors is the main component for these levels of parallelism?</p>","a":[{"id":790156,"option":"Number of partitions of the child RDDs","correct":false},{"id":790157,"option":"Number of partitions of the parent RDDs","correct":true},{"id":790158,"option":"Both of these","correct":false},{"id":790159,"option":"None of these","correct":false}]},{"q":"<p>You are required to configure the maximum size of a table. The size must be configured to broadcast the table to all worker nodes while performing a join operation. You are using the <strong>spark.sql.autoBroadcastJoinThreshold</strong> property of the Spark framework. To disable the broadcast of the table, which of the following values will you set the property to?</p>","a":[{"id":790176,"option":"0","correct":false},{"id":790177,"option":"1","correct":false},{"id":790178,"option":"-1","correct":true},{"id":790179,"option":"Null","correct":false}]},{"q":"<p>In order to maintain an arbitrary state in a Spark streaming application, you are using the <strong>updateStateByKey</strong> operation. To perform this operation, you must apply the <strong>update</strong> function to all existing keys in a batch by using the Spark framework.<br>\nWhich of the following conditions, if satisfied, allows you to apply this function to all the keys?</p>","a":[{"id":790192,"option":"Only if new data is available in a batch","correct":false},{"id":790193,"option":"Only if new data is not available in a batch","correct":false},{"id":790194,"option":"New data may or may not be available in a batch","correct":true},{"id":790195,"option":"If the function has yet not been exposed in the DStream API","correct":false}]},{"q":"<p>In Spark, you are unable to convert an RDD that contains case classes to a <strong>DataFrame</strong> collection. However, you decided to create the <strong>DataFrame</strong> programmatically. Which of the following methods will you use to apply the schema that you have created to the RDD of rows while performing this task?</p>","a":[{"id":787897,"option":"createDataFrame ","correct":true},{"id":787898,"option":"applyDataFrame","correct":false},{"id":787899,"option":"createRowS","correct":false},{"id":787900,"option":"collectDataFrame","correct":false}]},{"q":"<p>You have created a <strong>SparkSession</strong> class to access the functionalities of the Spark framework. Which of the following applications cannot create a data frame using the <strong>SparkSession</strong> class?</p>\n\n<p> </p>\n\n<p> </p>","a":[{"id":787885,"option":"Existing RDD","correct":false},{"id":787886,"option":"Hive table","correct":false},{"id":787887,"option":"Spark data source","correct":false},{"id":787888,"option":"XML file","correct":true}]},{"q":"<p>In Spark, which of the following methods is used to retrieve the first n elements of the dataset as an array?</p>","a":[{"id":787881,"option":"collect()","correct":false},{"id":787882,"option":"first()","correct":false},{"id":787883,"option":"take()","correct":true},{"id":787884,"option":"join()","correct":false}]},{"q":"<p>You are given a part of a transformation process whose some part is encoded as follows:</p>\n\n<pre class=\"prettyprint\"><code>XXXX(func)</code></pre>\n\n<p>Your task is to determine <strong>XXXX</strong> so that a new dataset is returned in such a way that it is created by selecting the elements of the source on which <strong>func</strong> returns true.</p>","a":[{"id":787877,"option":"map","correct":false},{"id":787878,"option":"filter","correct":true},{"id":787879,"option":"flatMap","correct":false},{"id":787880,"option":"sample","correct":false}]},{"q":"<p>You are performing an operation by using a distributed dataset in the Spark framework. Which of the following techniques will you use to aggregate the elements of the <strong>Resilient Distributed Datasets</strong> (RDD) data structure and then return the final result to the <strong>driver</strong> program?</p>","a":[{"id":787853,"option":"Map","correct":false},{"id":787854,"option":"Reduce","correct":true},{"id":787855,"option":"Persist","correct":false},{"id":787856,"option":"Replicate","correct":false}]},{"q":"<p>Your task is to perform some operations in the Spark framework by using the <strong>Resilient Distributed Datasets</strong> data structure. If you are required to retrieve the value after computation is performed on the dataset, then which of the following operations will you use to complete this task?</p>\n\n<p> </p>","a":[{"id":787849,"option":"Transformations","correct":false},{"id":787850,"option":"Actions","correct":true},{"id":787851,"option":"Computations","correct":false},{"id":787852,"option":"Mapping","correct":false}]},{"q":"<p>You are writing a Spark Streaming program. Which of the following objects will you require to create that serves as the entry point of all Spark Streaming functionality to initialize this program?</p>","a":[{"id":787837,"option":"Stream object","correct":false},{"id":787838,"option":"StreamingContext ","correct":true},{"id":787839,"option":"SparkContext","correct":false},{"id":787840,"option":"SparkConf","correct":false}]},{"q":"<p>You are converting a Hive metastore Parquet table to a Spark SQL Parquet table. For this you need to reconcile Hive metastore schema with Parquet schema.<br>\n<br>\nOnce the reconciliation is done, you observe that the reconciled field have the data type of the Parquet side.<br>\nWhich of the following statements will hold true in the given scenario?</p>","a":[{"id":790054,"option":"Field appearing in the Parquet schema are added as nullable field in the reconciled schema","correct":false},{"id":790055,"option":"Field appearing in the Hive metastore  schema are dropped in the reconciled schema","correct":false},{"id":790056,"option":"Reconciled schema contains exactly those fields defined in Parquet schema","correct":false},{"id":790057,"option":"Field appearing in the Hive metastore schema are added as nullable field in the reconciled schema","correct":true}]},{"q":"<p>You find that the hive table used to store your data is not an efficient serialization format. In the given context which of the following can be used to store data in a more efficient manner before working with hooks in apache airflow?</p>","a":[{"id":787570,"option":"HiveOperator","correct":true},{"id":787571,"option":"kwargs","correct":false},{"id":787572,"option":"field_dict","correct":false},{"id":787573,"option":"tblproperties","correct":false}]},{"q":"<p>You are migrating your apache airflow installation to a new server. You require to import the pool from JSON file to do so. In the given context which of these arguments would you utilize to do so?</p>","a":[{"id":787562,"option":"-g","correct":false},{"id":787563,"option":"-x","correct":false},{"id":787564,"option":"-i","correct":true},{"id":787565,"option":"-u","correct":false}]},{"q":"<p>In Airflow, a task instance represents a task that has been assigned to a DAG and has a state associated with a specific run of the DAG.<br>\n<br>\nWhich of the following could be the indicative state of a task instance?</p>\n\n<ol>\n\t<li>failed</li>\n\t<li>success</li>\n\t<li>running</li>\n\t<li>skipped</li>\n</ol>","a":[{"id":786971,"option":"1 and 2","correct":false},{"id":786972,"option":"3 and 4","correct":false},{"id":786973,"option":"1,2 and 3","correct":false},{"id":786974,"option":"All of these","correct":true}]},{"q":"<p>You use tls-toolkit in Nifi for securing Client/Server mode of operation. You generate json payload containing a CSR and an HMAC with the token as the key and the CSR’s public key fingerprint as the data. Where will you store the generated key pair along with the certificate chain after you have verified CA certificate from the TLS session and signed the certificate in the payload ?</p>","a":[{"id":786911,"option":"Node","correct":false},{"id":786912,"option":"Abstract processor","correct":false},{"id":786913,"option":"Cluster","correct":false},{"id":786914,"option":"None of these","correct":true}]},{"q":"<p>In the archive directory specified by the location<br>\n<br>\nnifi.flow.configuration.archive.dir* ,<br>\n<br>\nThere are certain properties that specify the limitations for archiving.<br>\nAssume that you are manually creating archive files, what would be the maximum size of files that you can store here?</p>","a":[{"id":786907,"option":"500 MB","correct":false},{"id":786908,"option":"1000MB","correct":false},{"id":786909,"option":"2000MB","correct":false},{"id":786910,"option":"Cannot be determined","correct":true}]},{"q":"<p>You have configured NIFI to use kerberos SPNEGO for authentication.<br>\n<br>\nA user hits the /access/kerberos endpoint<br>\n<br>\nAssuming that you are using a https connection and the appropriate properties have been set for kerberos to work, what would be the HTTP response and response header from the server?</p>","a":[{"id":786903,"option":"HTTP 400\r\nWWW-Authenticate: Negotiate.","correct":false},{"id":786904,"option":"HTTP 401\r\nWWW-Authenticate:  Negotiate","correct":true},{"id":786905,"option":"HTTP 400\r\nWWW-Authenticate: Negotiate YII","correct":false},{"id":786906,"option":"HTTP 401\r\nWWW-Authenticate:  Negotiate YII","correct":false}]},{"q":"<p>You want to configure NIFI to work with kerberos SPNEGO for authentication.<br>\n<br>\nYou find that a user who is trying to access the endpoint gets an unexpected result and the authentication fails.<br>\n<br>\nWhich of the following would need to be set as true in nifi.properties for the authentication to work?<br>\n1:krb5.file<br>\n2:Service Principal<br>\n3:Keytab Location</p>","a":[{"id":786899,"option":"Only 1 ","correct":false},{"id":786900,"option":"Only 1 and 2","correct":false},{"id":786901,"option":"Only 2 and 3","correct":true},{"id":786902,"option":"All 1,2 and 3","correct":false}]},{"q":"<p>You are using the NIFI zookeeper migrator to perform certain operations with the zk-migrator.sh command. You use the option -z tp specify something.<br>\n<br>\nWhat would be the format of the connect string that you use with this option?</p>","a":[{"id":786895,"option":"host:port","correct":false},{"id":786896,"option":"host:port[,host2:port…","correct":true},{"id":786897,"option":"host/port","correct":false},{"id":786898,"option":"host/port[;host2/port...","correct":false}]},{"q":"<p>When the bootstrap detects that NIFI has died, it needs to send a notification.<br>\n<br>\nAssume that you do not want to use email notifications. You need to configure the notification capabilities in a certain XML file. In what level of this XML file would you be defining the class name of the notification service?<br>\n<br>\nassume root is level 0</p>","a":[{"id":786891,"option":"0","correct":false},{"id":786892,"option":"1","correct":false},{"id":786893,"option":"2","correct":true},{"id":786894,"option":"3","correct":false}]},{"q":"<p>While working on a Data Egress processor on Apache Nifi, You encounter a certain issue that requires that the Flowfile be penalized. Which of these kinds of issues has probably been encountered by you in the given scenario?</p>","a":[{"id":786887,"option":"Communication Issue","correct":false},{"id":786888,"option":"Network issue","correct":false},{"id":786889,"option":"Data related issue","correct":true},{"id":786890,"option":"Cannot be determined","correct":false}]},{"q":"<p>You are inspecting the state of a Processor before the occurrence of @OnUnscheduled and @OnStopped life-cycle events. Suppose you called @OnScheduled life-cycle method that causes an issue. Which of the following can you pass as an argument to run(int,boolean,boolean) to call the onTrigger to avoid the occurrence of these events?</p>","a":[{"id":786883,"option":"False as the third argument","correct":true},{"id":786884,"option":"True as the third argument","correct":false},{"id":786885,"option":"Either True or False as the third argument","correct":false},{"id":786886,"option":"False as second argument","correct":false}]},{"q":"<p>You are using a Processor that publishes data to an external source.<br>\nWhich of the following will happen if this Processor will fail in leasing a connection from connection pool or in creating a new connection?<br>\n<br>\n1. FlowFile is routed to failure<br>\n2. The event will not be logged<br>\n3. The method will return</p>","a":[{"id":786879,"option":"Only 1 ","correct":false},{"id":786880,"option":"2 and 3","correct":false},{"id":786881,"option":"1 and 2","correct":false},{"id":786882,"option":"1 and 3","correct":true}]},{"q":"<p>Which of the following Provenance Events in Apache Nifi do not need to be reported ?</p>","a":[{"id":786875,"option":"ROUTE","correct":false},{"id":786876,"option":"SEND","correct":false},{"id":786877,"option":"CLONE","correct":true},{"id":786878,"option":"RECORD","correct":false}]},{"q":"<p>You are working with a Data Egress processor in Apache Nifi. While obtaining a connection a Network condition related issue is encountered. Which of these behaviours can be observed in the given scenario?</p>","a":[{"id":786871,"option":"Connection is terminated and not returned","correct":false},{"id":786872,"option":"Connection is allowed to trigger an OnError Event","correct":false},{"id":786873,"option":"The Flowfile will be routed to failure","correct":true},{"id":786874,"option":"None of these","correct":false}]},{"q":"<p>You receive a Provenance Drop event for an Object while working on Apache Nifi. Which of the following conclusions would you infer from this kind of a Provenance event report?</p>","a":[{"id":786867,"option":"Object's life has expired","correct":false},{"id":786868,"option":"Objects Life has ended for some other reason other than object expiration","correct":true},{"id":786869,"option":"Objects has been forcefully destroyed by user","correct":false},{"id":786870,"option":"None of these","correct":false}]},{"q":"<p>You are exposing a valid set of static processor relationships using getRelationships method overrides in Apache Nifi. Which of these would seem like valid guidances to be followed while doing so?<br>\n1. An immutable Set should be created within the constructor of this Processor.<br>\n2. An Immutable Set should be created within the init method of the Processor and its value must be returned.</p>","a":[{"id":786863,"option":"Only 1","correct":false},{"id":786864,"option":"Only 2","correct":false},{"id":786865,"option":"Either 1 or 2","correct":true},{"id":786866,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>If you are doing a Regex compilation in order to specify a routing criteria Processor property for a One-to-One relationship on a Flowfile being used in Apache Nifi. Which of these annotations would you be likely to specify in the method being used to accomplish the task given above?</p>","a":[{"id":786859,"option":"@OnStopped","correct":false},{"id":786860,"option":"@OnScheduled","correct":true},{"id":786861,"option":"@OnTrigger","correct":false},{"id":786862,"option":"@OnEvent","correct":false}]},{"q":"<p>Which of the following type of exception are propagated back to the onTrigger method, if an exception occurs in a processor while processing the FlowFile's content?</p>","a":[{"id":786855,"option":"RuntimeException","correct":true},{"id":786856,"option":"IOException","correct":false},{"id":786857,"option":"InterruptedException","correct":false},{"id":786858,"option":"UncheckedException","correct":false}]},{"q":"<p>The tls-toolkit is used to prevent man in the middle attacks using HMAC verification of the public keys. In the process the client needs to generate a request json payload. Which of the following does this payload contain as data?</p>","a":[{"id":786783,"option":"CSR","correct":false},{"id":786784,"option":" HMAC with the token","correct":false},{"id":786785,"option":"CSR and an HMAC","correct":false},{"id":786786,"option":" CSR’s public key fingerprint","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, what does the following code allow you to do:</p>\n\n<pre class=\"prettyprint\"><code>Ranges = []\n\n def storeRanges(rdd):\n     global Ranges\n     Ranges = rdd.Ranges()\n     return rdd\n\n def printRanges(rdd):\n     for o in Ranges:\n         print \"%s %s %s %s\" % (o.topic, o.partition, o.fromOffset, o.untilOffset)\n\n directKafkaStream \\\n     .transform(storeRanges) \\\n     .foreachRDD(printRanges)</code></pre>\n\n<p><strong>Tasks</strong></p>\n\n<ol>\n\t<li>Access the Kafka offsets consumed in each batch.</li>\n\t<li>If you want Zookeeper-based Kafka monitoring tools to show the progress of the streaming application then it updates Zookeeper.</li>\n\t<li>Create multiple-input Kafka streams and merge them.</li>\n\t<li>Increase the number of topic-specific partitions.</li>\n</ol>","a":[{"id":1167960,"option":"1 and 2","correct":true},{"id":1167961,"option":"2 and 3","correct":false},{"id":1167962,"option":"3 and 4","correct":false},{"id":1167963,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark streaming integrated with Kafka, which of the following artifacts is used to change this setting to handle more than (64 * number of executors) Kafka partitions?</p>","a":[{"id":1166925,"option":"spark.streaming.kafka.consumer.cache.maxCapacity","correct":true},{"id":1166926,"option":"spark-streaming-kafka-0-10_2.11","correct":false},{"id":1166927,"option":"LocationStrategies.PreferConsistent","correct":false},{"id":1166928,"option":"spark.streaming.kafka.consumer.cache.enabled","correct":false}]},{"q":"<p>In Kafka, the <strong>num.network.threads</strong> thread represents the number of network threads that the server uses for handling network requests. What is the default value of this thread?</p>","a":[{"id":1166767,"option":"1","correct":false},{"id":1166768,"option":"5","correct":false},{"id":1166769,"option":"7","correct":false},{"id":1166770,"option":"3","correct":true}]},{"q":"<p>In Spark, you are using the <strong>collect()</strong> method to transfer an RDD to the driver node while working in the cluster mode. Your task is to print all the elements of that specific RDD. You observed that this task can cause the driver to run out of memory.</p>\n\n<p>Which of the following will you use to print only a few elements of the RDD?</p>","a":[{"id":787893,"option":"rdd.foreach(println) ","correct":false},{"id":787894,"option":"rdd.map(println)","correct":false},{"id":787895,"option":"rdd.take(100).foreach(println)","correct":true},{"id":787896,"option":"rdd.collect().foreach(println)","correct":false}]},{"q":"<p>In Spark, you are working on a file-based data source. The dataset APIs are used here. Which of the following techniques will you use to make the source capable of applying both the <strong>save</strong> and <strong>saveAsTable</strong> operation in this scenario?</p>","a":[{"id":789234,"option":"Bucketing ","correct":false},{"id":789235,"option":"Sorting","correct":false},{"id":789236,"option":"Partitioning","correct":true},{"id":789237,"option":"None of these","correct":false}]},{"q":"<p>You are working on a Spark Streaming application. You want to receive multiple streams of data in your application to satisfy a certain requirement. What will you do to receive such data in the application?</p>","a":[{"id":789539,"option":"Create multiple input DStreams ","correct":true},{"id":789540,"option":"Create multiple Receivers","correct":false},{"id":789541,"option":"Allocate the same number of cores as the number of streams that are available in the application","correct":false},{"id":789542,"option":"Not possible to receive multiple streams of data in parallel","correct":false}]},{"q":"<p>In Spark, you are required to store the persisted RDD by using different storage levels. You used the <strong>persist()</strong> method to implement this task. Which of the following conditions can occur if you use the <strong>MEMORY_ONLY</strong> storage level in this scenario?</p>","a":[{"id":789628,"option":"RDD is stored as deserialized objects in the memory","correct":true},{"id":789629,"option":"RDD is stored as serialized objects in the memory","correct":false},{"id":789630,"option":"RDD is stored on the off-heap memory","correct":false},{"id":789631,"option":"RDD partitions are stored on a disk","correct":false}]},{"q":"<p>You are running a Spark Streaming program locally. The tasks are implemented locally by using a single thread that might not allow the received data to get processed. Which of the master URLs will you use to run the program locally to resolve this issue?</p>","a":[{"id":789644,"option":"local","correct":false},{"id":789645,"option":"local[0]","correct":false},{"id":789646,"option":"local[1]","correct":false},{"id":789647,"option":"local[n]","correct":true}]},{"q":"<p>In Spark, the transform operation is used to apply an arbitrary RDD-to-RDD function on a DStream. Which of the following conditions must be satisfied in order to use it to apply an RDD operation?</p>","a":[{"id":790017,"option":"Operation must not have been exposed in the DStream API","correct":true},{"id":790018,"option":"Operation must have been exposed in the DStream API","correct":false},{"id":790019,"option":"Operation may or may not have been exposed in the DStream API","correct":false},{"id":790020,"option":"Can be used regardless of any condition","correct":false}]},{"q":"<p>In Spark, you are using the <strong>spark.sql.parquet.compression.codec</strong> property to set the compression codec that can be used while writing Parquet files. Assume that, you have specified the <strong>parquet.compression</strong> property in the table-specific options or properties before starting the <strong>write</strong> operation. Which of the following is the correct order of precedence in the provided scenario?</p>","a":[{"id":790092,"option":"Compression -> parquet.compression -> spark.sql.parquet.compression.codec","correct":true},{"id":790093,"option":"Parquet.compression -> compression -> spark.sql.parquet.compression.code","correct":false},{"id":790094,"option":"Compression -> spark.sql.parquet.compression.codec -> parquet.compression","correct":false},{"id":790095,"option":"Spark.sql.parquet.compression.codec -> parquet.compression -> compression","correct":false}]},{"q":"<p>Your task is to print all the elements of an RDD on the driver. Which of the following methods will you use to retrieve the RDD to the driver node so that the elements can be printed?</p>","a":[{"id":787873,"option":"bring()","correct":false},{"id":787874,"option":"take()","correct":false},{"id":787875,"option":"collect()","correct":true},{"id":787876,"option":"gather()","correct":false}]},{"q":"<p>You are required to set the following options if you want both batch and streaming queries for a Kafka source. Which statements about this scenario are correct:</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>assign</li>\n\t<li>subscribePattern</li>\n\t<li>subscribe</li>\n\t<li>kafka.bootstrap.servers</li>\n</ol>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>assign</strong>:<strong> </strong>Represents the specific TopicPartitions to consume</li>\n\t<li><strong>subscribePattern</strong>: Represents the topic list to subscribe</li>\n\t<li><strong>subscribe</strong>:<strong> </strong>Represents the list of subscribers </li>\n\t<li><strong>kafka.bootstrap.servers</strong>:<strong> </strong>Represents the pattern used to subscribe to topics</li>\n\t<li>All these options—assign, subscribe, and subscribePattern—can be specified for a Kafka source</li>\n\t<li>Only one of these options—assign, subscribe, or subscribePattern—can be specified for a Kafka source</li>\n</ol>","a":[{"id":1167987,"option":"1, 2, 4","correct":false},{"id":1167988,"option":"1, 4, 6","correct":true},{"id":1167989,"option":"3, 5, 6","correct":false},{"id":1167990,"option":"1, 3, 4, 6","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, your application uses the consumer group ID <strong>terran </strong>to read from the <strong>zerg.hydra </strong>Kafka topic, which contains 10 partitions. You have configured your application such that it can consume the topic with only one thread. However, during runtime, you increase the number of threads from 1 to 14.</p>\n\n<p>Which of the following statements about the consumption of the threads in this scenario is correct:</p>\n\n<ol>\n\t<li>All the 14 threads are consumed from a single partition—each consumed in two rebalancing events.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads stay idle.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads are consumed in the next rebalancing event.</li>\n\t<li>Once the rebalancing is complete, 7 of the 14 threads are consumed—each from a single partition—and the remaining 7 threads stay idle.</li>\n</ol>","a":[{"id":1167798,"option":"1","correct":false},{"id":1167799,"option":"2","correct":true},{"id":1167800,"option":"3","correct":false},{"id":1167801,"option":"4","correct":false}]},{"q":"<p>Which of the following are the advantages of Kafka:</p>\n\n<ol>\n\t<li>Tracks web activities by storing or sending events to real-time processes</li>\n\t<li>Resilient to node failures and it enables automatic recovery</li>\n\t<li>Transforms data of different formats into a standard format to avoid ambiguity</li>\n</ol>","a":[{"id":1166755,"option":"1 and 2","correct":false},{"id":1166756,"option":"2 and 3","correct":false},{"id":1166757,"option":"1 and 3","correct":false},{"id":1166758,"option":"All of these","correct":true}]},{"q":"<p>In Kafka, which of the following represent the two essential configurations of the <strong>consumer API</strong>:</p>\n\n<ol>\n\t<li>group.id</li>\n\t<li>segment.ms</li>\n\t<li>zookeeper.connect</li>\n\t<li>retention.bytes</li>\n</ol>","a":[{"id":1166775,"option":"1 and 2","correct":false},{"id":1166776,"option":"2 and 3","correct":false},{"id":1166777,"option":"1 and 3","correct":true},{"id":1166778,"option":"2 and 4","correct":false}]},{"q":"<p>Which of these sequences of operators relationship is functionally equivalent to the following bit-wise statement:</p>\n\n<blockquote>\n<p>op1 &gt;&gt; op2 &gt;&gt; op3 &lt;&lt; op4</p>\n</blockquote>","a":[{"id":786987,"option":"op1.set_downstream(op2)\r\nop2.set_upstream(op3)\r\nop3.set_downstream(op4)","correct":false},{"id":786988,"option":"op1.set_downstream(op2)\r\nop2.set_downstream(op3)\r\nop3.set_upstream(op4)","correct":true},{"id":786989,"option":"op1.set_upstream(op2)\r\nop2.set_downstream(op3)\r\nop3.set_upstream(op4)","correct":false},{"id":786990,"option":"op1.set_upstream(op2)\r\nop2.set_downstream(op3)\r\nop3.set_downstream(op4)","correct":false}]},{"q":"<p>In Spark, you have been assigned the task of selecting the storage level that must be used for storing the persisted RDD. The process to select the storage level must be fast and fault recoverable in such a way that you do not have to wait for recomputing a lost partition to run tasks on RDD.<br>\n<br>\nWhich of the following storage levels will you select in the given scenario?</p>","a":[{"id":789543,"option":"MEMORY_ONLY","correct":false},{"id":789544,"option":"MEMORY_ONLY_SER","correct":false},{"id":789545,"option":"Replicated storage level","correct":true},{"id":789546,"option":"DISK_ONLY","correct":false}]},{"q":"<p>You are using the output operations on the <strong>DStream</strong> abstraction procedure. These output operations execute the DStreams slowly. If you are working on a Spark Streaming application, then which of the following conditions does not allow you to execute your code:</p>\n\n<ol>\n\t<li>Application does not contain output operations</li>\n\t<li>Application contains output operations without RDD actions in these operations</li>\n\t<li>Application contains output operations with RDD actions that are available in these operations</li>\n</ol>","a":[{"id":790075,"option":"1 and 2","correct":true},{"id":790076,"option":"2 and 3","correct":false},{"id":790077,"option":"1 and 3","correct":false},{"id":790078,"option":"1, 2, and 3","correct":false}]},{"q":"<p>The <strong>Parquet </strong>metadata is cached by the Spark SQL module to ensure better performance of the table. The Hive metastore Parquet table conversion method is enabled. Since the metadata of these converted tables is also cached, which of the following techniques will you use to ensure consistent metadata if Hive or any other external tool updates these tables?</p>","a":[{"id":790079,"option":"Reconcile Hive metastore schema with the Parquet schema","correct":false},{"id":790080,"option":"Set spark.sql.hive.convertMetastoreParquet to true","correct":false},{"id":790081,"option":"Refresh the tables manually","correct":true},{"id":790082,"option":"Tables are automatically updated","correct":false}]},{"q":"<p>In the Spark framework, the <strong>SparkSession</strong> class is the basic requirement for all the functionalities that are available in the framework. Which of the following methods will you use to create this class?</p>","a":[{"id":787889,"option":"SparkSession.builder()","correct":true},{"id":787890,"option":"SparkSession.create()","correct":false},{"id":787891,"option":"SparkSession.generator()","correct":false},{"id":787892,"option":"SparkSession.start()","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following code are correct:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>def function(): StreamingContext = {\n  val hack_ssc = new StreamingContext(...)   \n  val h_lines = hack_ssc.socketTextStream(...) \n  ...\n  hack_ssc.checkpoint(checkpointDirectory)   \n  hack_ssc\n}\nval hack_context = StreamingContext.getOrCreate(checkpointDirectory, function _)\nhack_context. ...\nhack_context.start()\nhack_context.awaitTermination()</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>If the program starts for the first time, the <strong>StreamingContext.getOrCreate()</strong> function allows you to create a new StreamingContext class, set up all the streams, and then call <strong>stop()</strong>.</li>\n\t<li>If the program restarts after failure, then the <strong>StreamingContext.getOrCreate()</strong> function allows you to replace a StreamingContext from the checkpoint data in the checkpoint directory.</li>\n\t<li>If the checkpointDirectory exists, then the context is replaced from the checkpoint data.</li>\n\t<li>If the checkpoint directory does not exist, that is, running for the first time, then the <strong>functionToCreateContext </strong>function is called to create a new context and disable the DStreams.</li>\n</ol>","a":[{"id":1167886,"option":"1, 2, 3","correct":false},{"id":1167887,"option":"2, 3, 4","correct":false},{"id":1167888,"option":"1, 3, 4","correct":false},{"id":1167889,"option":"None of these","correct":true}]},{"q":"<p>In Spark Streaming, the following new <strong>StreamingContext</strong> object is created from an existing <strong>SparkContext</strong> object. Which of these steps can be performed after a context is defined:</p>\n\n<p><strong>Object</strong></p>\n\n<pre class=\"prettyprint\"><code>import org.apache.spark.streaming._\nval sc = ...                \nval ssc = new StreamingContext(sc, Seconds(5))</code></pre>\n\n<p><strong>Steps </strong></p>\n\n<ol>\n\t<li>Defining the input sources by creating an input DStream</li>\n\t<li>Start receiving the data and then process it using streamingContext.start()</li>\n</ol>","a":[{"id":1166901,"option":"1","correct":false},{"id":1166902,"option":"2","correct":false},{"id":1166903,"option":"All of these","correct":true},{"id":1166904,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, how many input <strong>DStreams</strong> are created in the following code:</p>\n\n<pre class=\"prettyprint\"><code>val numInputDStreams = 5.5\nval kafkaDStreams = (2 to numInputDStreams).map { _ =&gt; KafkaUtils.createStream(...) }</code></pre>\n\n<p> </p>","a":[{"id":1166779,"option":"1","correct":false},{"id":1166780,"option":"3","correct":false},{"id":1166781,"option":"5","correct":false},{"id":1166782,"option":"None of these","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, RDD actions inside the DStream output operations force the processing of the data that is received. If you used the <strong>dstream.foreachRDD()</strong> artifact without any RDD actions inside them, then which of the following statements about the given scenario is correct:</p>\n\n<ol>\n\t<li>The system receives the data and discards it, and therefore, there is no execution.</li>\n\t<li>The system does not receive any data.</li>\n\t<li>The system receives and processes the data and the execution runs successfully.</li>\n\t<li>The system receives no data, and therefore, there is no execution.</li>\n</ol>","a":[{"id":1167810,"option":"1","correct":true},{"id":1167811,"option":"2","correct":false},{"id":1167812,"option":"3","correct":false},{"id":1167813,"option":"4","correct":false}]},{"q":"<p>You are required to convert an existing RDD into a dataset. You must use a programming interface for performing this conversion. Which of the following conditions is suitable for the implementation of this method?</p>","a":[{"id":787865,"option":"When you already know the schema while writing your Spark application","correct":false},{"id":787866,"option":"When the columns and their types are not known until runtime","correct":true},{"id":787867,"option":"When you have to infer the schema of an RDD","correct":false},{"id":787868,"option":"None of these","correct":false}]},{"q":"<p>In Spark, you are performing operations that are capable of triggering the shuffle event. Which of the following techniques must be used such that it does not trigger the shuffling event?</p>","a":[{"id":787905,"option":"groupByKey","correct":false},{"id":787906,"option":"reduceByKey","correct":false},{"id":787907,"option":"cogroup","correct":false},{"id":787908,"option":"countByKey","correct":true}]},{"q":"<p>You are required to create a view that can be easily shared among all the sessions and can be kept alive until the termination of a Spark application. Which of the following views will you create in this scenario?</p>","a":[{"id":787861,"option":"Local","correct":false},{"id":787862,"option":"Temporary","correct":false},{"id":787863,"option":"Global","correct":false},{"id":787864,"option":"Global temporary","correct":true}]},{"q":"<p>You are using the Spark framework for fault-tolerant stream processing of live data streams. Which of the following types of abstractions is used to represent a continuous stream of data?</p>","a":[{"id":787833,"option":"KStream","correct":false},{"id":787834,"option":"CStream","correct":false},{"id":787835,"option":"DStream","correct":true},{"id":787836,"option":"ZStream","correct":false}]},{"q":"<p>You have a requirement wherein you need to share a SQLite database with two docker containers.<br>\nWhich of the following approach should be adopted for doing so?</p>","a":[{"id":787111,"option":"Nothing special to do, each container has its own SQLite database file and it works ","correct":false},{"id":787112,"option":"You have to create a common volume for both containers where the metadatabase will be shared. ","correct":true},{"id":787113,"option":"You need to create a common persistent volume claim for both the dockers","correct":false},{"id":787114,"option":"You can't because SQLite doesn't allow for concurrent writing.","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements describe the task performed by the following code:</p>\n\n<pre class=\"prettyprint\"><code>createStream(StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map&lt;String,Object&gt; topics, StorageLevel storageLevel)</code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>createStream</strong> allows you to provide the input stream details, which include the systems where the port is created and the topic name.</li>\n\t<li>ssc represents the StreamingFile object.</li>\n\t<li>groupId represents the customer ID of this particular customer.</li>\n\t<li>Checkpoint operations are performed on a stream of data after it is received.</li>\n</ol>","a":[{"id":1167974,"option":"1 and 2","correct":false},{"id":1167975,"option":"2 and 3","correct":false},{"id":1167976,"option":"1 and 3","correct":true},{"id":1167977,"option":"All of these","correct":false}]},{"q":"<p>Which of the following classes is required to initialize a Spark Streaming program?</p>","a":[{"id":1166735,"option":"StreamingContext  ","correct":true},{"id":1166736,"option":"socketTextStream","correct":false},{"id":1166737,"option":" DStream","correct":false},{"id":1166738,"option":"None of these","correct":false}]},{"q":"<p>In Apache NiFi, which of the following type of provenance event in a provenance reporting indicates an event wherein, a user is trying to access an event, for which the is not authorized to know the type?</p>","a":[{"id":786843,"option":"DROP","correct":false},{"id":786844,"option":"OVERRIDE","correct":false},{"id":786845,"option":"UNKNOWN","correct":true},{"id":786846,"option":"CLEAR","correct":false}]},{"q":"<p>In Apache NiFi, a callback function is allowed to throw multiple numbers of exceptions. Which of the following types of exceptions allows the <strong>Exception</strong> function to propagate back to the <strong>onTrigger</strong> method:</p>\n\n<ol>\n\t<li>RuntimeException</li>\n\t<li>IOException</li>\n</ol>","a":[{"id":786795,"option":"1","correct":true},{"id":786796,"option":"2","correct":false},{"id":786797,"option":"Both of these","correct":false},{"id":786798,"option":"None of these","correct":false}]},{"q":"<p>You are running the Apache NiFi project in a cluster and are required to communicate with the state manager. Which of these scopes will you use to provide the framework with important operating information:</p>\n\n<ol>\n\t<li>Scope.LOCAL</li>\n\t<li>Scope.CLUSTER</li>\n</ol>","a":[{"id":785736,"option":"1","correct":false},{"id":785737,"option":"2","correct":false},{"id":785738,"option":"Both of these","correct":true},{"id":785739,"option":"None of these","correct":false}]},{"q":"<p>You are working on a Spark Streaming application that you need to test with test data. To perform this task, you must create a DStream by using a queue of RDDs. Which of the following methods will you use to create the DStream for the required purpose?</p>","a":[{"id":789682,"option":"streamingContext.queueStream(queueOfRDDs)","correct":true},{"id":789683,"option":"streamingContext.createQueueStream(queueOfRDDs)","correct":false},{"id":789684,"option":"streamingContext.createStream(queueOfRDDs)","correct":false},{"id":789685,"option":"streamingContext.queueFileSystem(queueOfRDDs)","correct":false}]},{"q":"<p>In Spark, you are required to create a custom aggregate function because built-in functions cannot be used to fulfill your requirement. Which of the following types of aggregate functions you must create if you are extending the <strong>Aggregator</strong> class to implement the custom aggregate function?</p>","a":[{"id":789956,"option":"Type-Safe User-Defined Aggregate Functions","correct":true},{"id":789957,"option":"Untyped User-Defined Aggregate Functions","correct":false},{"id":789958,"option":"Both of these","correct":false},{"id":789959,"option":"None of these","correct":false}]},{"q":"<p>What is the output of the following Spark statement:</p>\n\n<pre class=\"prettyprint\"><code>SELECT sortedList(array('q', 'z', null, 'e', 'j'), true);</code></pre>\n\n<p> </p>","a":[{"id":787869,"option":"[null,\"e\",\"j\",\"q\",\"z\"]","correct":true},{"id":787870,"option":"[\"e\",\"j\",\"q\",\"z\",null]","correct":false},{"id":787871,"option":"[\"z\",\"q\",\"j\",\"e\",null]","correct":false},{"id":787872,"option":"[null,\"z\",\"q\",\"j\",\"e\"]","correct":false}]},{"q":"<p>You are required to perform RDD operations. These operations are distributed into multiple tasks by the Spark framework. Each of these tasks can be executed by the executor process. Which of the following is computed by the framework for each task before their execution?</p>","a":[{"id":787857,"option":"counter","correct":false},{"id":787858,"option":"controller","correct":false},{"id":787859,"option":"closure","correct":true},{"id":787860,"option":"Time taken for execution","correct":false}]},{"q":"<p>In the following Spark statement, which of these represents XXXX that creates a parallelized collection:</p>\n\n<pre class=\"prettyprint\"><code>val value = Array(10, 20, 30, 40, 50)\nval dataSet = XXXX(value)</code></pre>\n\n<p> </p>","a":[{"id":787845,"option":"sc.parallelize","correct":true},{"id":787846,"option":"SparkContext.parallelize","correct":false},{"id":787847,"option":"builder.parallelize","correct":false},{"id":787848,"option":"sc.builder.parallelize","correct":false}]},{"q":"<p>In Kafka, which of the following statements about the <strong>zookeeper.connect</strong> configuration are correct:</p>\n\n<ol>\n\t<li>It allows you to add a chroot path that makes all the Kafka data for this cluster appear under all the paths.</li>\n\t<li>It allows you to set up multiple Kafka clusters or other applications on the different ZooKeeper cluster at an instance of time. </li>\n\t<li>It specifies the ZooKeeper connection string in the port:hostname where hostname and port are the host and port for a node in the ZooKeeper clusterIn Kafka.</li>\n</ol>","a":[{"id":1166917,"option":"1","correct":false},{"id":1166918,"option":"2","correct":false},{"id":1166919,"option":"3","correct":false},{"id":1166920,"option":"None of these","correct":true}]},{"q":"<p>In Kafka, the load factor of a hash table is used in log cleaning. Which of the following is the default value of the load factor?</p>","a":[{"id":1166771,"option":"0.3","correct":false},{"id":1166772,"option":"0.5","correct":false},{"id":1166773,"option":"0.9","correct":true},{"id":1166774,"option":"0.8","correct":false}]},{"q":"<p>In Apache NiFi, you are configuring the <strong>GetFile</strong> processor by applying the appropriate configuration techniques. After applying the configurations, you view an Alert icon in the top-left corner of the processor. Which of the following messages is displayed in the icon:</p>\n\n<ol>\n\t<li>The processor is not in a valid state.</li>\n\t<li>The failure of the relationship has not been defined.</li>\n\t<li>NiFi has not been provided valid instructions for the data processor transfers to the successful relationship.</li>\n</ol>","a":[{"id":785329,"option":"1","correct":false},{"id":785330,"option":"2 and 3","correct":false},{"id":785331,"option":"1 and 2","correct":false},{"id":785332,"option":"1 and 3","correct":true}]},{"q":"<p>In Apache Airflow, you are running a DAG on a scheduled interval that starts at [2019-08-01T11:59] and ends at [2019-08-01T23:59]. When will the scheduler run your job instance?</p>","a":[{"id":787147,"option":"At 2019-08-01T11:59","correct":false},{"id":787148,"option":"Before 2019-08-01T11:59","correct":false},{"id":787149,"option":"At 2019-08-01T23:59","correct":true},{"id":787150,"option":"At 2019-08-01T05:59","correct":false}]},{"q":"<p>In Airflow, the tasks call the <strong>xcom_pull()</strong> method to retrieve XComs. These tasks apply filters based on some criteria. Which of the following criterion is filtered by the<strong> xcom.pull( )</strong> method as a default?<br>\n </p>","a":[{"id":787099,"option":"key","correct":true},{"id":787100,"option":"conn_id","correct":false},{"id":787101,"option":"Source's task_id","correct":false},{"id":787102,"option":"Source's dag_id","correct":false}]},{"q":"<p>In Apache Airflow, which of the following statements about the <strong>sensor</strong> operator is correct?</p>","a":[{"id":786947,"option":"It sends an HTTP request","correct":false},{"id":786948,"option":"It executes a bash command","correct":false},{"id":786949,"option":"It calls an arbitrary Python function","correct":false},{"id":786950,"option":"It waits for a certain time until a certain criterion is met","correct":true}]},{"q":"<p>In Spark, you have applied an operation that can trigger a shuffle event. Therefore, a lot of heap memory is consumed because of the implementation of in-memory data structures. Which of the following can be used so that these data structures are generated on the reduce side?</p>","a":[{"id":787909,"option":"aggregateByKey ","correct":false},{"id":787910,"option":"reduceByKey ","correct":false},{"id":787911,"option":"collect","correct":false},{"id":787912,"option":"sortByKey","correct":true}]},{"q":"<p>If you are debugging an Apache Airflow operator, then which of the following arguments will you not use for the Airflow test command:</p>\n\n<ol>\n\t<li>Name of DAG</li>\n\t<li>Execute method</li>\n\t<li>Name of a task</li>\n\t<li>Date associated with a specific DAG Run</li>\n\t<li>Environment variable</li>\n</ol>","a":[{"id":786931,"option":"1, 3, and 4","correct":false},{"id":786932,"option":"2 and 5","correct":true},{"id":786933,"option":"2, 3, and 4","correct":false},{"id":786934,"option":"4 and 5","correct":false}]},{"q":"<p>Suppose you want to execute a SQL command in Airflow. You need to use an appropriate operator for doing so.<br>\nWhich of the following operators cannot be used for the required purpose?<br>\n </p>","a":[{"id":786943,"option":"JdbcOperator","correct":false},{"id":786944,"option":"BashOperator","correct":true},{"id":786945,"option":"OracleOperator","correct":false},{"id":786946,"option":"PostgresOperator","correct":false}]},{"q":"<p>If you are creating a processor in Apache NiFi, then the <strong>init()</strong> method of the <strong>AbstractProcessor</strong> class is invoked that takes a single argument of the <strong>ProcessorInitializationContext</strong> type. Which of the following is not supplied to the Processor by using this object?</p>\n\n<p> </p>\n\n<p> </p>","a":[{"id":786799,"option":"ComponentLog","correct":false},{"id":786800,"option":"Processor’s unique identifier ","correct":false},{"id":786801,"option":"ControllerServiceLookup","correct":false},{"id":786802,"option":"ControllerPropertyDescriptor","correct":true}]},{"q":"<p>Your local Apache Airflow settings file has defined a policy function that could apply a specific queue property when using a specific operator or enforce a task timeout policy. This policy restricts tasks to run for more than 18 hours. Which of the following configuration statements must be set up within the <strong>airflow_settings.py</strong> file?</p>","a":[{"id":787538,"option":"def policy(task): if task.__class__.__name__ == 'HivePartitionSensor': task.queue = \"sensor_queue\" task.timeout = timedelta(hours=18)","correct":false},{"id":787539,"option":"def policy(task): if class == 'HivePartitionSensor': task.queue = \"sensor_queue\" task.timeout = timedelta(hours=18)","correct":false},{"id":787540,"option":"def policy(task): if task.timeout > timedelta(hours=18): task.timeout = timedelta(hours=18)","correct":false},{"id":787541,"option":"def policy(task): if task.__class__.__name__ == 'HivePartitionSensor': task.queue = \"sensor_queue\" if task.timeout > timedelta(hours=18): task.timeout = timedelta(hours=18)","correct":true}]},{"q":"<p>Which of the following is the correct universal order of precedence for all the configuration options that are available in Apache Airflow:</p>\n\n<ol>\n\t<li>Set up in the <strong>airflow.cfg</strong> file</li>\n\t<li>Built-in defaults that are available in Airflow</li>\n\t<li>Set an environment variable</li>\n\t<li>Command in the <strong>airflow.cfg</strong> file</li>\n</ol>","a":[{"id":786963,"option":"1 -> 2 -> 3 -> 4","correct":false},{"id":786964,"option":"2 -> 1 -> 4 -> 3","correct":false},{"id":786965,"option":"3 -> 1 -> 4 -> 2","correct":true},{"id":786966,"option":"4 -> 1 -> 2 -> 3","correct":false}]},{"q":"<p>In Spark, you are using some operations while working with RDDs that are only available on RDDs of key-value pairs. Assume that you are using custom objects as a key in these key-value pair operations. Which of the following two methods must be used together in this scenario?</p>","a":[{"id":790132,"option":"Equals() and hashcode()","correct":true},{"id":790133,"option":"Collcet() and take()","correct":false},{"id":790134,"option":"Map() and filter()","correct":false},{"id":790135,"option":"Distinct() and reduceByKey()","correct":false}]}]