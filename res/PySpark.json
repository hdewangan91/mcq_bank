[{"q":"<p>John is working on PySpark. He wants to link the Python API to Spark Core and initializing SparkContext. Help him to find which of the following platform is used?</p>","a":[{"id":1181757,"option":"PySpark SQL","correct":false},{"id":1181758,"option":"PySpark Core","correct":false},{"id":1181759,"option":"PySpark Shell","correct":true},{"id":1181760,"option":"None of these","correct":false}]},{"q":"<p>John is working on SQL in PySpark. He wants to convert an internal SQL object into a native Python object. Help him to find which of the following methods will be used?</p>","a":[{"id":1181753,"option":"toInternal(d)","correct":false},{"id":1181754,"option":"needConversion()","correct":false},{"id":1181755,"option":"toInternal(dt)","correct":false},{"id":1181756,"option":"fromInternal(v)","correct":true}]},{"q":"<p>Assume that you face the <strong>‘NoneType’ object has no attribute ‘_jvm'</strong> issue while working with a PySpark function. Which of the following could be a valid reason for this issue?</p>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>You are using PySpark functions without having an active spark session.</li>\n\t<li>You are using PySpark functions within a UDF.</li>\n</ol>\n\n<p>&nbsp;</p>","a":[{"id":1180084,"option":"1","correct":false},{"id":1180085,"option":"2","correct":false},{"id":1180086,"option":"Both of these","correct":true},{"id":1180087,"option":"None of these","correct":false}]},{"q":"<p>Which of the following statements regarding pyspark.StatusTracker are true?​​​</p>","a":[{"id":1179385,"option":"This API provides very weak consistency semantics","correct":false},{"id":1179386,"option":"The consumer of this API should be prepared to handle empty/missing information","correct":false},{"id":1179387,"option":"Both of these","correct":true},{"id":1179388,"option":"None of these","correct":false}]},{"q":"<p>John is working on SQL in pySpark. He specified the LOCAL parameter in the LOAD DATA statement. Help him to find which of the following happens?</p>","a":[{"id":1179357,"option":"The files are loaded into a distributed storage.","correct":false},{"id":1179358,"option":"It causes the INPATH to be resolved against the default file system.","correct":false},{"id":1179359,"option":"It causes the INPATH to be resolved against the local file system.","correct":true},{"id":1179360,"option":"None of these","correct":false}]},{"q":"<p>John is working on SparkContext in PySpark. He set the batchSize parameter of a SparkContext as 1. Help him to find Which of the following will happens?</p>","a":[{"id":1179297,"option":"Unlimited batchsize can be used","correct":false},{"id":1179298,"option":"The batch size is automatically chosen based on the batch size","correct":false},{"id":1179299,"option":"Batching is disabled","correct":true},{"id":1179300,"option":"Limited batchsize is used ","correct":false}]},{"q":"<p>John is working on textile stream in PySpark. Help him to find Which of the following can be used to select a file in a later window, even if its contents have not changed?</p>","a":[{"id":1182101,"option":"FileSystem.modifyTimestamp()","correct":true},{"id":1182102,"option":"FileSystem.setTimestamp()","correct":false},{"id":1182103,"option":"FileSystem.setTimes()","correct":false},{"id":1182104,"option":"FileSystem.getTimes()","correct":false}]},{"q":"<p>You are using Apache Arrow for optimization when converting a Spark DataFrame to a Pandas DataFrame as shown in the code snippet given alongside. What happens if an error occurs during createDataFrame()?</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>import numpy as np\nimport pandas as pd\n\nspark.conf.set(\"\"spark.sql.execution.arrow.pyspark.enabled\"\", \"\"true\"\")\npdf = pd.DataFrame(np.random.rand(100, 3))\ndf = spark.createDataFrame(pdf)\nresult_pdf = df.select(\"\"*\"\").toPandas()</code></pre>","a":[{"id":1182213,"option":"A DataFrame is created with an Arrow and the supported type is used instead","correct":false},{"id":1182214,"option":"A DataFrame will not be created","correct":false},{"id":1182215,"option":"Spark will fall back to create the DataFrame without Arrow","correct":true},{"id":1182216,"option":"None of these","correct":false}]},{"q":"<p>You are using Apache Arrow for optimization when converting a Spark DataFrame to a Pandas DataFrame. Which of the following statements is true with respect to the given scenario?</p>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>An error can be raised if a column has an unsupported data type</li>\n\t<li>The optimizations with Arrow will produce the same results as when Arrow is not enabled</li>\n</ol>","a":[{"id":1182209,"option":"1","correct":false},{"id":1182210,"option":"2","correct":false},{"id":1182211,"option":"Both of these","correct":true},{"id":1182212,"option":"None of these","correct":false}]},{"q":"<p>Predict the output of the code snippet given alongside.</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>from typing import Iterator, Tuple\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\npdf = pd.DataFrame([1, 2, 3], columns=[\"\"x\"\"])\ndf = spark.createDataFrame(pdf)\n@pandas_udf(\"\"long\"\")\ndef multiply_two_cols(\n        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -&gt; Iterator[pd.Series]:\n    for a, b in iterator:\n        yield a * b\n\ndf.select(multiply_two_cols(\"\"x\"\", \"\"x\"\")).show()</code></pre>\n\n<p>Options:</p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>2\n4\n6</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>1\n4\n9</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>3\n6\n9</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>1\n2\n3</code></pre>\n\t</li>\n</ol>","a":[{"id":1182201,"option":"1","correct":false},{"id":1182202,"option":"2","correct":true},{"id":1182203,"option":"3","correct":false},{"id":1182204,"option":"4","correct":false}]},{"q":"<p>Assume that you encounter the AnalysisException exception while working with the isStreaming function in spark. Which of the following can be a valid cause for the same?</p>","a":[{"id":1182185,"option":"You are using a method that returns a single answer","correct":true},{"id":1182186,"option":"You have not defined the streaming source","correct":false},{"id":1182187,"option":"Your spark streaming components have encountered a version mismatch","correct":false},{"id":1182188,"option":"Your connection to the streaming component source has timed out","correct":false}]},{"q":"<p>Assume that you are working with a Spark RDD on a certain computation. If you expect the result of the computation to be a suitably large array, then the usage of which of these methods should be avoided in the computation?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>subtractByKey(other, numPartitions=None)</li>\n\t<li>take(num)</li>\n\t<li>top(num, key=None)</li>\n</ol>","a":[{"id":1182181,"option":"1","correct":false},{"id":1182182,"option":"2","correct":false},{"id":1182183,"option":"3","correct":false},{"id":1182184,"option":"2 and 3","correct":true}]},{"q":"<p>You are asked to refactor the computation given alongside directly using the aggregateMessages operator. Which of these reasons justify the validation of this request:</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>class GraphOps[VD, ED] {\n  def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]\n  def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[ Array[(VertexId, VD)] ]\n}</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>The given code is deserialized and prone to error</li>\n\t<li>The given code is costly due to the substantial communication required</li>\n\t<li>The given code is costly due to the duplicate information generated</li>\n</ol>","a":[{"id":1182149,"option":"1","correct":false},{"id":1182150,"option":"2","correct":false},{"id":1182151,"option":"3","correct":false},{"id":1182152,"option":"2 and 3","correct":true}]},{"q":"<p>You are using the Python code given alongside to create a scalar that computes the product of 2 columns. Analyze the code and determine which of the following must be placed in place of XXX to complete the code snippet?</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>import pandas as pd\nfrom pyspark.sql.functions import col, pandas_udf\nfrom pyspark.sql.types import LongType\n\ndef multiply_func(a, b):\n    return a * b\nmultiply = pandas_udf(multiply_func, returnType=LongType())\nx = pd.Series([1, 2, 3])\nprint(multiply_func(x, x))\nXXX\ndf.select(multiply(col(\"\"x\"\"), col(\"\"x\"\"))).show()</code></pre>","a":[{"id":1180164,"option":"XXX: df = createDataFrame(pd.DataFrame(x, columns=[\"x\"]))","correct":false},{"id":1180165,"option":"XXX: df = create.sparkDataFrame(pd.DataFrame(columns=[\"x\"], x))","correct":false},{"id":1180166,"option":"XXX: df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))","correct":true},{"id":1180167,"option":"XXX: df = spark.DataFrame(pd.DataFrame(columns=[\"x\"], x))","correct":false}]},{"q":"<p>Assume that when working with PySpark DataFrames, the row of a join between two DataFrame have the fields of same names. It is given that, one of the duplicate fields will be selected by asDict. What does __getitem__ do in the given scenario?</p>","a":[{"id":1180144,"option":"It returns two distinct duplicate fields","correct":false},{"id":1180145,"option":"It returns one of the duplicate fields which might be similar to asDict.","correct":false},{"id":1180146,"option":"It returns one of the duplicate fields which might be different to asDict.","correct":true},{"id":1180147,"option":"It also returns two non distinct duplicate fields","correct":false}]},{"q":"<p>When working with Spark serializer, you have created a stream for reading serialized objects using the DeserializationStream API. Which of the following concrete value members can be used to read the elements of this stream through an iterator over key-value pairs?</p>","a":[{"id":1180140,"option":"defasKeyValueIterator: Iterator[(Any, Any)]","correct":true},{"id":1180141,"option":"defasKeyValueIterator: Iterator[Any, Any]","correct":false},{"id":1180142,"option":"defreadKeyValue[T]()(implicit arg0: ClassTag[T]): T","correct":false},{"id":1180143,"option":"defreadKeyValue[T]()(implicit arg0: ClassTag[T, T])","correct":false}]},{"q":"<p>You want to binarize a column of continuous features given a threshold using pyspark. Assume that you want to map multiple columns at once during this process. Which of the following can be used to do so?</p>","a":[{"id":1180124,"option":"inputCols","correct":true},{"id":1180125,"option":"params","correct":false},{"id":1180126,"option":"freqs","correct":false},{"id":1180127,"option":"ranges","correct":false}]},{"q":"<p>You are given a dataset of 20K records and 10M records.&nbsp;Which of these options will you use for building Machine Learning models?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>For 20K records: Pyspark MLlib\nFor 10M records: Pyspark MLlib</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>For 20K records: Pyspark MLlib\nFor 10M records: Scikit Learn</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>For 20K records: Scikit Learn\nFor 10M records: Scikit Learn</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>For 20K records: Scikit Learn\nFor 10M records: Pyspark MLlib</code></pre>\n\t</li>\n</ol>","a":[{"id":1182017,"option":"1","correct":false},{"id":1182018,"option":"2","correct":false},{"id":1182019,"option":"3","correct":false},{"id":1182020,"option":"4","correct":true}]},{"q":"<p>In PySpark, you are given a dataset that consists of only texts. You are required to convert them into corresponding vectors. Which of the following options will you use to perform this task?</p>","a":[{"id":1182009,"option":"OneHotEncoder","correct":true},{"id":1182010,"option":"PCA","correct":false},{"id":1182011,"option":"Tokenizer","correct":false},{"id":1182012,"option":"StandardScaler","correct":false}]},{"q":"<p>In PySpark, there are five features in a dataset corresponding to AGE, HEIGHT, WEIGHT, DATE, and&nbsp;NAME. If you want to delete the '<strong>DATE</strong>' column, then which of the following options will you use:</p>\n\n<ol>\n\t<li>df.drop('DATE')</li>\n\t<li>df = df[['NAME','AGE','HEIGHT','WEIGHT']]</li>\n</ol>","a":[{"id":1181993,"option":"1","correct":false},{"id":1181994,"option":"2","correct":false},{"id":1181995,"option":"1 and 2","correct":false},{"id":1181996,"option":"None of the Above","correct":true}]},{"q":"<p>John is working on sql in PySpark. He wants to act the spark sql as a distributed query engine. Help him to find when he can use it?</p>","a":[{"id":1179361,"option":"JDBC/ODBC ","correct":false},{"id":1179362,"option":"JDBC","correct":false},{"id":1179363,"option":"Command-line interface","correct":false},{"id":1179364,"option":"1 or 3","correct":true}]},{"q":"<p>Which of these is the correct way to use a Date Literal in PySpark SQL:</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>SELECT DATE '1987' AS col;</li>\n\t<li>SELECT DATE '1987-05-15' AS col;</li>\n\t<li>SELECT DATE '1987-05' AS col;</li>\n</ol>","a":[{"id":1179341,"option":"1","correct":false},{"id":1179342,"option":"2","correct":false},{"id":1179343,"option":"3","correct":false},{"id":1179344,"option":"All of these","correct":true}]},{"q":"<p>John is working in sql on pyspark. He can't use the ANSI SQL reserved keywords as identifiers. Help him to know when the situation occurs?</p>","a":[{"id":1179337,"option":"If spark.sql.ansi.enabled is set to true","correct":true},{"id":1179338,"option":"If spark.sql.ansi.enabled is set to false","correct":false},{"id":1179339,"option":"If spark.sql.ansi is set to true","correct":false},{"id":1179340,"option":"If spark.sql.ansi is set to false","correct":false}]},{"q":"<p>John is working with PySpark. He wants to know about the variable named sc. Help him to know What he can access in the PySpark shell with it?</p>","a":[{"id":1179317,"option":"Spark session","correct":false},{"id":1179318,"option":"Spark context","correct":true},{"id":1179319,"option":"Spark cluster","correct":false},{"id":1179320,"option":"Spark cluster managers","correct":false}]},{"q":"<p>John is working on Spark Streaming in PySpark.He want to use the Basic sources in spark Streaming. Help him to find the example for it?</p>\n\n<ol>\n\t<li>File systems</li>\n\t<li>Socket connections</li>\n\t<li>Utility Classes</li>\n</ol>","a":[{"id":1179285,"option":"1 and 3","correct":false},{"id":1179286,"option":"2 and 3","correct":false},{"id":1179287,"option":"1 and 2","correct":true},{"id":1179288,"option":"All of these","correct":false}]},{"q":"<p>when you are working with PySpark. You are using the checkpoint(eager=True) method to return a checkpointed version&nbsp;of a Dataset. Where will this information be saved?</p>","a":[{"id":1179245,"option":"Inside the checkpoint directory and SparkContext directory","correct":false},{"id":1179246,"option":"In files inside the SparkContext directory","correct":false},{"id":1179247,"option":"In files inside the checkpoint directory","correct":true},{"id":1179248,"option":"Inside the checkpoint directory","correct":false}]},{"q":"<p>You have no Kinesis checkpoint information that exists when an input DStream starts in your PySpark implementation. Which of the following&nbsp;points is responsible to start&nbsp;the Kinesis checkpointing:</p>\n\n<ol>\n\t<li>InitialPositionInStream.LATEST</li>\n\t<li>InitialPositionInStream.TRIM_HORIZON</li>\n</ol>","a":[{"id":1182073,"option":"1","correct":false},{"id":1182074,"option":"2","correct":false},{"id":1182075,"option":"either 1 or 2","correct":true},{"id":1182076,"option":"None of these","correct":false}]},{"q":"<p>In pySpark, you want to join two datasets to approximately find all pairs of rows whose distance is smaller than a certain threshold. Which of the following classes in<strong> </strong>pyspark.ml.feature.BucketedRandomProjectionLSH&nbsp;can be used to perform this task?</p>","a":[{"id":1182057,"option":"approxSimilarityJoin()","correct":false},{"id":1182058,"option":"approxNearestNeighbors()","correct":true},{"id":1182059,"option":"euclideanSimilarityJoin()","correct":false},{"id":1182060,"option":"approxMapJoin()","correct":false}]},{"q":"<p>You want to have a look at the statistical summary of a column 'Position'<strong> </strong>in a data frame 'Class_df'<strong>.&nbsp;</strong>Which of the following methods can you use to retrieve this information?</p>","a":[{"id":1182045,"option":"Class_df.describe(Position).show()","correct":false},{"id":1182046,"option":"Class_df.describe('Position').show()","correct":true},{"id":1182047,"option":"Class_df.column('Position').show()","correct":false},{"id":1182048,"option":"Class_df.describe('Position').show","correct":false}]},{"q":"<p>In PySpark, which of the following&nbsp;issues are solved by the JavaLoader class in the util module?</p>","a":[{"id":1182033,"option":"Mixin for classes that can load saved models using its Scala implementation","correct":true},{"id":1182034,"option":"Load a model from the given path","correct":false},{"id":1182035,"option":"Mixin for classes that can load saved models using its Java implementation","correct":false},{"id":1182036,"option":"All of these","correct":false}]},{"q":"<p>You are training a linear regression model using the Stochastic Gradient Descent to update the model based on each new batch of incoming data from a DStream. It is given that each batch of data is assumed to be an RDD of LabeledPoints. In the given scenario, which of the following must be constant if the PySpark MLlib Util is used?</p>","a":[{"id":1180120,"option":"The number of features and the number of data points per batch","correct":false},{"id":1180121,"option":"The number of features","correct":true},{"id":1180122,"option":"The number of data points per batch","correct":false},{"id":1180123,"option":"The initial weight of the vector","correct":false}]},{"q":"<p>Assume that you are using the MLlib util shown alongside to configure the K Means algorithm. What happens to the decayFactor if the timeUnit is set as 'points'?</p>\n\n<p><strong>Util:</strong></p>\n\n<pre class=\"prettyprint\"><code>class pyspark.mllib.clustering.StreamingKMeans(k, decayFactor, timeUnit)</code></pre>","a":[{"id":1180108,"option":"The decay factor is raised to the power of number of new points","correct":true},{"id":1180109,"option":"The decay factor is raised to the power of previous centroids","correct":false},{"id":1180110,"option":"The decay factor is raised to the number of clusters","correct":false},{"id":1180111,"option":"The decay factor will be used as is","correct":false}]},{"q":"<p>You are using the MLlib util shown alongside to configure the K Means algorithm. Assume that you are using the setInitialCenters(centers, weights) method to set initial centers. In the given context, what can be said about the given statements?</p>\n\n<p><strong>Util:</strong></p>\n\n<pre class=\"prettyprint\"><code>class pyspark.mllib.clustering.StreamingKMeans(k, decayFactor, timeUnit)</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>The initial centers should be set before calling trainOn.</li>\n\t<li>The number of clusters must also be set before setting the initial centers</li>\n</ol>","a":[{"id":1180104,"option":"1","correct":true},{"id":1180105,"option":"2","correct":false},{"id":1180106,"option":"Both of these","correct":false},{"id":1180107,"option":"None of these","correct":false}]},{"q":"<p>Analyze the code snippet given alongside and select the snippet that can be used to implement the same behavior using aggregateMessages.</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>val graph: Graph[Int, Float] = ...\ndef msgFun(triplet: Triplet[Int, Float]): Iterator[(Int, String)] = {\n  Iterator((triplet.dstId, \"\"Hi\"\"))\n}\ndef reduceFun(a: String, b: String): String = a + \"\" \"\" + b\nval result = graph.mapReduceTriplets[String](msgFun, reduceFun)</code></pre>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>val graph: Graph[Int, Float] = ...\ndef msgFun(triplet: EdgeContext[Int, Float, String]) {\n  triplet.sendToDst(\"Hi\")\n}\ndef reduceFun(a: String, b: String): String = a + \" \" + b\nval result = graph.Edgeonly[String](msgFun, reduceFun)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>val graph: Graph[Int, Float] = ...\ndef msgFun(triplet: EdgeContext[Int, Float, String]) {\n  triplet.sendToDst(\"Hi\")\n}\ndef reduceFun(a: String, b: String): String = a + \" \" + b\nval result = graph.aggregateMessages[String](msgFun, reduceFun)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>val graph: Graph[Int, Float] = ...\ndef msgFun(triplet: EdgeContext[Int, Float, String]) {\n  triplet.sendToDst(\"Hi\")\n}\ndef reduceFun(a: String, b: String): String = a + \" \" + b\nval result = graph.Edgeonly[String](msgFun, reduceFun)</code></pre>\n\t</li>\n</ol>","a":[{"id":1180092,"option":"1","correct":false},{"id":1180093,"option":"2","correct":true},{"id":1180094,"option":"3","correct":false},{"id":1180095,"option":"None of these","correct":false}]},{"q":"<p>Which of these configurations can be made to avoid the stackOverflowError while working with the Pregel operator in graphx?</p>","a":[{"id":1180088,"option":"spark.graphx.pregel.cachesize","correct":false},{"id":1180089,"option":"spark.graphx.pregel.ttl","correct":false},{"id":1180090,"option":"spark.graphx.pregel.checkpointInterval\r\n","correct":true},{"id":1180091,"option":"spark.graphx.pregel.memory size","correct":false}]},{"q":"<p>You have created a DataFrame from the text of the FRAME file in the Spark source directory as shown alongside.In the given context, what does the following code do?</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>textFile=spark.read.text(\"FRAME.md\")\ntextFile.filter(textFile.value.contains(\"Hello\")).count()</code></pre>","a":[{"id":1179413,"option":"It returns the number of lines that contain \"Hello\"","correct":true},{"id":1179414,"option":"It returns every instance of \"Hello\"","correct":false},{"id":1179415,"option":"It returns the number of time \"Hello\" appears in the textFile","correct":false},{"id":1179416,"option":"None of these","correct":false}]},{"q":"<p>You are using the Python code given alongside to define a base RDD from an external file and perform certain actions on it. You want to use lineLengths again later such that the lineLengths computed the first time is saved in the memory.How can this be achieved?</p>\n\n<p><strong>code</strong></p>\n\n<pre class=\"prettyprint\"><code>lines = sc.textFile(\"\"data.txt\"\")\nlineLengths = lines.map(lambda s: len(s))\ntotalLength = lineLengths.reduce(lambda a, b: a + b)</code></pre>\n\n<p>&nbsp;</p>","a":[{"id":1179421,"option":"Add lineLengths.persist() before and after reduce","correct":false},{"id":1179422,"option":"Add lineLengths.persist() after reduce","correct":false},{"id":1179423,"option":"Add lineLengths.persist() before reduce ","correct":true},{"id":1179424,"option":"None of these","correct":false}]},{"q":"<p>Consider the file join.py given alongside. What will the output of the following command be?<br>\n<br>\n$SPARK_HOME/bin/spark-submit join.py</p>\n\n<p><strong>join.py</strong></p>\n\n<pre class=\"prettyprint\"><code>from pyspark import SparkContext\nsc = SparkContext(\"\"local\"\", \"\"Join app\"\")\na = sc.parallelize([(\"\"spark\"\", 5), (\"\"hadoop\"\", 1)])\nb = sc.parallelize([(\"\"spark\"\", 6), (\"\"hadoop\"\", 2)])\njoined = a.join(b)\nfinal = joined.collect()\nprint \"\"%s\"\" % (final)</code></pre>\n\n<p>&nbsp;</p>","a":[{"id":1179425,"option":"[('spark', (5, 6)), ('hadoop', (1, 2))]","correct":true},{"id":1179426,"option":"[('spark', (5, 1)), ('hadoop', (6, 2))]","correct":false},{"id":1179427,"option":"['hadoop', (5, 6)), ('spark', (1, 2))]","correct":false},{"id":1179428,"option":"[('spark', (6, 5)),('hadoop', (2, 1))]","correct":false}]},{"q":"<p>You want to convert an internal SQL object into a native Python object. Which of these methods will you use to do so?</p>","a":[{"id":1180052,"option":"toInternal(obj)","correct":true},{"id":1180053,"option":"needConversion()","correct":false},{"id":1180054,"option":"fromInternal(obj)","correct":false},{"id":1180055,"option":"None of these","correct":false}]},{"q":"<p>You want to explicitly create a StreamingContext from the checkpoint data and start the computation while working in PySpark. Which of the following snippets can be used to do so?</p>","a":[{"id":1180060,"option":"StreamingContext.getOrCreate(checkpointDirectory, None)","correct":true},{"id":1180061,"option":"StreamingContext.Create(checkpointDirectory, None)","correct":false},{"id":1180062,"option":"StreamingContext.getOrCreate(None, Obj)","correct":false},{"id":1180063,"option":"None of these","correct":false}]},{"q":"<p>You are using the MLlib util class shown alongside when working in PySpark. Which of the following is true with respect to the property threshold?</p>\n\n<pre class=\"prettyprint\"><code>class pyspark.mllib.classification.LogisticRegressionModel(weights,intercept,numFeatures,numClasses)</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>It returns the threshold used for converting raw prediction scores into 0/1 predictions.</li>\n\t<li>It is used for Binary classification only</li>\n</ol>","a":[{"id":1180072,"option":"1","correct":false},{"id":1180073,"option":"2","correct":false},{"id":1180074,"option":"Both of these","correct":true},{"id":1180075,"option":"None of these","correct":false}]},{"q":"<p>When using the Python code given alongside in Spark, you notice that the whole object is being referenced. Which of these code snippets can be used to avoid such an issue?</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>class MyClass(object):\n    def __init__(self):\n        self.field = \"\"Trial\"\"\n    def doStuff(self, rdd):\n        return rdd.map(lambda s: self.field + s)</code></pre>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>def doStuff(self,rdd):\n self.field=field\n    return rdd.map(lambda s:field+s)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>def doStuff(self,rdd):\n    field=self.field\n    return rdd.map(lambda s:self.field+s)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>def doStuff(self,rdd):\n    field=self.field\n    return rdd.map(lambda s:field+s)</code></pre>\n\t</li>\n</ol>\n\n<p>&nbsp;</p>","a":[{"id":1179393,"option":"1","correct":false},{"id":1179394,"option":"2","correct":false},{"id":1179395,"option":"3","correct":true},{"id":1179396,"option":"None of these","correct":false}]},{"q":"<p>You are using a CLUSTER BY clause to first repartition the data based on the input expressions and then sort the data within each partition. What is this semantically equivalent to?</p>","a":[{"id":1179365,"option":"WHERE and GROUP BY followed by a SORT BY","correct":true},{"id":1179366,"option":"GROUP BY followed by a SORT BY","correct":false},{"id":1179367,"option":"DISTRIBUTE BY followed by a SORT BY","correct":false},{"id":1179368,"option":"WHERE and HAVING followed by a SORT BY","correct":false}]},{"q":"<p>What will the input details in the Batch details page of a batch contain in case of a TextFileStream?</p>","a":[{"id":1179281,"option":"Offsets read for this batch","correct":false},{"id":1179282,"option":"Partitions read for this batch","correct":false},{"id":1179283,"option":"A list of file names that was read for this batch","correct":true},{"id":1179284,"option":"Partitions and Offsets read for this batch","correct":false}]},{"q":"<p>John is working on sql in pyspark. He want to set the Spark master URL to run locally on four cores. Help him to achieve this?</p>","a":[{"id":1179237,"option":"Connect the URL to local[4]","correct":true},{"id":1179238,"option":"Connect the URL to local","correct":false},{"id":1179239,"option":"Connect the URL to spark://master:7077","correct":false},{"id":1179240,"option":"Connect the URL to spark://master:7078","correct":false}]},{"q":"<p>John is working with PySpark. He want to receive multiple streams of data in parallel in his streaming application. What can he do to achieve this?</p>","a":[{"id":1179289,"option":"Create multiple executors","correct":false},{"id":1179290,"option":"Create multiple receivers","correct":false},{"id":1179291,"option":"Create multiple input DStreams","correct":true},{"id":1179292,"option":"Create multiple workers","correct":false}]},{"q":"<p>Consider the code snippet given alongside. Predict the output produced by the following lines of code with respect to the given code snippet.<br>\n<strong>Code snippet</strong></p>\n\n<pre class=\"prettyprint\"><code>df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()</code></pre>\n\n<p><strong>Lines of code</strong></p>\n\n<pre class=\"prettyprint\"><code>from pyspark.sql import Row\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nsc = SparkContext('local')\nspark = SparkSession(sc)\ndf1 = spark.createDataFrame([\n    Row(id=1, value='foo'),\n    Row(id=2, value=None)\n])\ndf1.select(\n    df1['value'] == 'foo',\n    df1['value'].eqNullSafe('foo'),\n    df1['value'].eqNullSafe(None)\n).show()\n\ndf2 = spark.createDataFrame([\n    Row(value = 'bar'),\n    Row(value = None)\n])</code></pre>\n\n<p><strong>Output</strong></p>\n\n<ol>\n\t<li>\n\t<table border=\"1\" style=\"width: 310px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\"><strong>(value = foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\"><strong>(value &lt;=&gt; foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\"><strong>(value &lt;=&gt; NULL)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">false</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">null</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">false</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">true</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<br>\n\t&nbsp;</li>\n\t<li>\n\t<table border=\"1\" style=\"width: 310px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\"><strong>(value = foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\"><strong>(value &lt;=&gt; foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\"><strong>(value &lt;=&gt; NULL)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">false</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">false</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">false</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">null</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">false</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">true</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<br>\n\t&nbsp;</li>\n\t<li>\n\t<table border=\"1\" style=\"width: 310px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\"><strong>(value = foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\"><strong>(value &lt;=&gt; foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\"><strong>(value &lt;=&gt; NULL)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">false</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">null</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">null</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">null</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<br>\n\t&nbsp;</li>\n\t<li>\n\t<table border=\"1\" style=\"width: 310px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\"><strong>(value = foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\"><strong>(value &lt;=&gt; foo)</strong></td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\"><strong>(value &lt;=&gt; NULL)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">true</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 79px;\">true</td>\n\t\t\t\t<td style=\"text-align: center; width: 100px;\">false</td>\n\t\t\t\t<td style=\"text-align: center; width: 107px;\">true</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t</li>\n</ol>","a":[{"id":1180152,"option":"1","correct":true},{"id":1180153,"option":"2","correct":false},{"id":1180154,"option":"3","correct":false},{"id":1180155,"option":"4","correct":false}]},{"q":"<p>Predict the output of the Python code snippet given alongside.</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>import pandas as pd\nfrom pyspark import SparkContext\nfrom pyspark.sql.functions import col, pandas_udf, struct, PandasUDFType\nfrom pyspark.sql import SQLContext\nsc = SparkContext(\"local\", \"test\")\nx = pd.Series([1, 2, 3])\npdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\ndf = SQLContext(sc).createDataFrame(pdf)\n@pandas_udf(\"long\", PandasUDFType.SCALAR_ITER)\ndef plus_one(batch_iter):\n    for x in batch_iter:\n        yield x + 1\n# df.select(plus_one(col(\"x\"))).show()\n@pandas_udf(\"long\", PandasUDFType.SCALAR_ITER)\ndef result(batch_iter):\n    for a, b in batch_iter:\n        yield a * b\ndf.select(result(col(\"x\"), col(\"x\"))).show()</code></pre>\n\n<p><strong>Output</strong></p>\n\n<ol>\n\t<li>\n\t<table border=\"1\" style=\"width: 106px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 94px;\"><strong>result(x, x)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 94px;\">4</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 94px;\">9</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 94px;\">16</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<br>\n\t&nbsp;</li>\n\t<li>\n\t<table border=\"1\" style=\"width: 103px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 91px;\"><strong>result(x, x)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 91px;\">1</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 91px;\">4</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 91px;\">9</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<br>\n\t&nbsp;</li>\n\t<li>\n\t<table border=\"1\" style=\"width: 101px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\"><strong>result(x, x)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\">2</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\">4</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\">6</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t<br>\n\t&nbsp;</li>\n\t<li>\n\t<table border=\"1\" style=\"width: 101px;\">\n\t\t<tbody>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\"><strong>result(x, x)</strong></td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\">4</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\">6</td>\n\t\t\t</tr>\n\t\t\t<tr>\n\t\t\t\t<td style=\"text-align: center; width: 89px;\">8</td>\n\t\t\t</tr>\n\t\t</tbody>\n\t</table>\n\t</li>\n</ol>","a":[{"id":1180160,"option":"1","correct":false},{"id":1180161,"option":"2","correct":true},{"id":1180162,"option":"3","correct":false},{"id":1180163,"option":"4","correct":false}]},{"q":"<p>In PySpark, you are working on the<strong> </strong><em>pyspark.mllib.classification</em> module.</p>\n\n<p>If you have implemented the following class, then which parameter will you use to determine when to terminate the iterations?</p>\n\n<p><strong>Class</strong></p>\n\n<pre class=\"prettyprint\"><code>class StreamingLogisticRegressionWithSGD(stepSize=0.1, numIterations=50, miniBatchFraction=1.0, regParam=0.0, convergenceTol=0.001)</code></pre>\n\n<p> </p>","a":[{"id":1181801,"option":"stepSize","correct":false},{"id":1181802,"option":"numIterations","correct":false},{"id":1181803,"option":"convergenceTol ","correct":true},{"id":1181804,"option":"miniBatchFraction","correct":false}]},{"q":"<p>What is the default smoothing parameter used for training a Naive Bayes model given an RDD of (label, features) vectors while working with classification.NaiveBayes module in PySparks 2.4.4?</p>","a":[{"id":1212757,"option":"0.01","correct":false},{"id":1212758,"option":"1","correct":true},{"id":1212759,"option":"0.05","correct":false},{"id":1212760,"option":"2","correct":false}]},{"q":"<p>You are implementing a custom Transformer in Python while working with PySpark. Which of the following can be used to avoid the necessity to implement the underlying algorithm in Scala to enable persistence?</p>","a":[{"id":1182125,"option":"Acctuators","correct":false},{"id":1182126,"option":"Mixin Classes","correct":true},{"id":1182127,"option":"Brokers","correct":false},{"id":1182128,"option":"DStream","correct":false}]},{"q":"<p>You have 10 million records of industrial data. To achieve a faster reading and reduction in size, you decide to serialize your data. However, the data objects are of different data types. Which of the following options is the best fit for the provided use case?</p>","a":[{"id":1181981,"option":"Marshal serializer","correct":false},{"id":1181982,"option":"Pickle serializer","correct":true},{"id":1181983,"option":"CPickle","correct":false},{"id":1181984,"option":"None of these","correct":false}]},{"q":"<p>In PySpark 2.4.4, if you are given an RDD of ratings by users for a subset of products, then which of these configuration parameters will you modify to set the number of latent factors while training a matrix factorization model?</p>","a":[{"id":1181973,"option":"ratings ","correct":false},{"id":1181974,"option":"rank","correct":true},{"id":1181975,"option":"blocks","correct":false},{"id":1181976,"option":"lambda","correct":false}]},{"q":"<p>You want to mine frequent sequential patterns while working with PySpark. Which of the following alternatives can be used to perform this task?</p>","a":[{"id":1181965,"option":"fpm.PrefixSpan","correct":true},{"id":1181966,"option":"fpm.FPGrowthModel","correct":false},{"id":1181967,"option":"fpm.FPGrowth","correct":false},{"id":1181968,"option":"fpm.train","correct":false}]},{"q":"<p>You want to convert the multinomial <strong>Naive Bayes</strong> model to the <strong>Bernoulli Naive Bayes</strong> model while working with the following classification module method in PySpark 2.4.4. Which of the following actions will you implement to perform this task:</p>\n\n<pre class=\"prettyprint\"><code>classmethod train(data, lambda_=1.0)[source]</code></pre>\n\n<p> </p>","a":[{"id":1181961,"option":"Convert every vector to a 0-1 vector..","correct":true},{"id":1181962,"option":"Convert the input to a DStream.","correct":false},{"id":1181963,"option":"Convert the output range to a flatmap.","correct":false},{"id":1181964,"option":"Add a DStream context to the model input range and maintain state using this context.","correct":false}]},{"q":"<p>You want to split each line of input using space ' ' as a separator when using PySpark Shell. Which of the following lines of code can be used to perform this task?</p>","a":[{"id":1181957,"option":"Map(lambda line: line.split(\" \"))\r\n","correct":false},{"id":1181958,"option":"flatmap(lambda line: line.split(\" \"))\r\n","correct":false},{"id":1181959,"option":"flatMap(lambda line: line.split(\" \"))\r\n","correct":true},{"id":1181960,"option":"flatMap(line: line.split(\" \"))\r\n","correct":false}]},{"q":"<p>PySpark Streaming provides fault-tolerance '<strong>exactly-once</strong>' semantics for stateful operations. What does 'exactly- once' semantics mean?</p>","a":[{"id":1181953,"option":"It means events are processed 'exactly once' by all operators in the stream application but not when a failure occurs.","correct":false},{"id":1181954,"option":"It means events are processed 'exactly once' by all operators in the stream application even if a failure occurs.","correct":true},{"id":1181955,"option":"It means events are processed 'exactly once' by all single operators in the stream application.","correct":false},{"id":1181956,"option":"None of these","correct":false}]},{"q":"<p>You want to recover data from driver failures while working with the following method in PySpark 2.4.4. Which of the following steps must be taken to perform this task:</p>\n\n<pre class=\"prettyprint\"><code>createDirectStream(ssc, topics, kafkaParams, fromOffsets=None, keyDecoder=&lt;function utf8_decoder&gt;, valueDecoder=&lt;function utf8_decoder&gt;, messageHandler=None)</code></pre>\n\n<p> </p>","a":[{"id":1181949,"option":"Enable checkpointing in StreamingContext","correct":true},{"id":1181950,"option":"Enable checkpointing in SparkSession","correct":false},{"id":1181951,"option":"Enable checkpointing in SparkContext","correct":false},{"id":1181952,"option":"Enable DStream usage in SparkContext","correct":false}]},{"q":"<p>What is the significance of the following PySpark code:</p>\n\n<pre class=\"prettyprint\"><code>big_list = range(1000)\nrdd = sc.parallelize(big_list, 2)\nodds = rdd.filter(lambda x: x % 2 != 0)\nodds.take(5)\n[1, 3, 5, 7, 9]</code></pre>\n\n<p> </p>","a":[{"id":1181945,"option":"Creates an interation of 1000 elements","correct":false},{"id":1181946,"option":"Uses the parallelize() method to distribute that data into two partitions","correct":true},{"id":1181947,"option":"Both of these","correct":false},{"id":1181948,"option":"None of these","correct":false}]},{"q":"<p>Which of the following is a feature of PySpark:</p>\n\n<ol>\n\t<li>It is a hundred times faster than traditional large-scale data processing frameworks.</li>\n\t<li>The complex programming layer provides powerful caching.</li>\n\t<li>It provides real-time computation and low latency because of in-memory computation.</li>\n</ol>","a":[{"id":1181925,"option":"1","correct":false},{"id":1181926,"option":"1 and 2","correct":false},{"id":1181927,"option":"1 and 3","correct":true},{"id":1181928,"option":"2 and 3","correct":false}]},{"q":"<p>Which of the following specialized data structures allows you to interact with PySpark?</p>","a":[{"id":1181921,"option":"RDD","correct":true},{"id":1181922,"option":"Distributed RDD","correct":false},{"id":1181923,"option":"PySpark Cluster","correct":false},{"id":1181924,"option":"PySpark Core","correct":false}]},{"q":"<p>The entry point of a PySpark program is an object. This object allows you to connect to a Spark cluster and create RDDs. What is this object called?</p>","a":[{"id":1181917,"option":"SparkContent ","correct":false},{"id":1181918,"option":"SparkContext ","correct":true},{"id":1181919,"option":"ContextSpark","correct":false},{"id":1181920,"option":"ContentSpark","correct":false}]},{"q":"<p>Which of the following is one common way to create PySpark RDDs?</p>","a":[{"id":1181909,"option":"Using the PySpark parallelize() function","correct":true},{"id":1181910,"option":"Using the PySpark filter() function","correct":false},{"id":1181911,"option":"Using the PySpark internalize() function","correct":false},{"id":1181912,"option":"Using the PySpark parallel() function","correct":false}]},{"q":"<p>Which of the following libraries allows PySpark to communicate with the Spark Scala-based API?</p>","a":[{"id":1181905,"option":"PySpark4J Library","correct":false},{"id":1181906,"option":"PyJ Library","correct":false},{"id":1181907,"option":"Py4J Library ","correct":true},{"id":1181908,"option":"PyS4J Library","correct":false}]},{"q":"<p>Which of the following statement justifies that PySpark is based on the functional paradigm:</p>\n\n<ol>\n\t<li>Scala is functional-based.</li>\n\t<li>Functional code is much easier to parallelize</li>\n\t<li>Makes it very readable</li>\n</ol>","a":[{"id":1181901,"option":"1","correct":false},{"id":1181902,"option":"1 and 2","correct":true},{"id":1181903,"option":"1 and 3","correct":false},{"id":1181904,"option":"2 and 3","correct":false}]},{"q":"<p>Which of the following PySpark interfaces provides a variety of ways to submit PySpark programs including <strong>PySpark Shell</strong> and the <strong>spark-submit</strong> command?</p>","a":[{"id":1181897,"option":"Command Line Interface","correct":true},{"id":1181898,"option":"PySpark Shell Interface","correct":false},{"id":1181899,"option":"Cluster Interface","correct":false},{"id":1181900,"option":"Spark Interface","correct":false}]},{"q":"<p>You must submit PySpark code to a cluster using the command line. Which of the following commands will you use to perform this task?</p>","a":[{"id":1181893,"option":"spark submit ","correct":false},{"id":1181894,"option":"spark-submit ","correct":true},{"id":1181895,"option":"spark-cluster","correct":false},{"id":1181896,"option":"spark cluster ","correct":false}]},{"q":"<p>Which of the following statements is correct about PySpark SparkContext:</p>\n\n<ol>\n\t<li>SparkContext sets up internal services</li>\n\t<li>Does not establish a connection to a Spark execution environment</li>\n\t<li>SparkContext object sends the application to executors</li>\n\t<li>SparkContext does not execute tasks in each executor</li>\n</ol>","a":[{"id":1181889,"option":"1, 2, and 3","correct":false},{"id":1181890,"option":"2 and 4","correct":false},{"id":1181891,"option":"1 and 3","correct":true},{"id":1181892,"option":"2, 3, and 4","correct":false}]},{"q":"<p>What is the key abstraction for PySpark Spark Streaming?</p>","a":[{"id":1181877,"option":"PySpark Stream","correct":false},{"id":1181878,"option":"Kafka","correct":false},{"id":1181879,"option":"Discretized Stream ","correct":true},{"id":1181880,"option":"PySpark Cluster","correct":false}]},{"q":"<p>You want to perform <strong>Alternating Least Squares matrix</strong> factorization while working with PySpark 2.4.4. Which of the following modules will you use to perform this task?</p>","a":[{"id":1181869,"option":"pyspark.mllib.recommendation","correct":true},{"id":1181870,"option":"pyspark.mllib.features","correct":false},{"id":1181871,"option":"pyspark.mllib.linalg","correct":false},{"id":1181872,"option":"pyspark.mllib.standard","correct":false}]},{"q":"<p>What is the default smoothing parameter used for training a <strong>Naive Bayes</strong> model given an RDD of (label, features) vectors while working with the <strong>classification.NaiveBayes</strong> module in PySpark 2.4.4?</p>","a":[{"id":1181865,"option":"0.01","correct":false},{"id":1181866,"option":"1","correct":true},{"id":1181867,"option":"0.05","correct":false},{"id":1181868,"option":"2","correct":false}]},{"q":"<p>You are training a support vector machine using the <strong>SVMWithSGD</strong> class in PySpark. Which of these parameters will you use to specify whether the bias features are activated?</p>","a":[{"id":1181861,"option":"convergenceTol ","correct":false},{"id":1181862,"option":"miniBatchFraction ","correct":false},{"id":1181863,"option":"regType","correct":false},{"id":1181864,"option":"intercept ","correct":true}]},{"q":"<p>What is the default type of regularizer used while training a support vector machine in PySpark 2.4.4?</p>","a":[{"id":1181857,"option":"l1","correct":false},{"id":1181858,"option":"l2","correct":true},{"id":1181859,"option":"l3","correct":false},{"id":1181860,"option":"None of these","correct":false}]},{"q":"<p>You are working with the following method in PySpark 2.4.4. Which of these values of the <strong>numRows</strong> parameter causes the number of rows to be calculated when <strong>numRows</strong> is invoked:</p>\n\n<p><strong>Method</strong></p>\n\n<pre class=\"prettyprint\"><code>pyspark.mllib.linalg.distributed.BlockMatrix(blocks, rowsPerBlock, colsPerBlock, numRows=0, numCols=0)</code></pre>\n\n<p> </p>","a":[{"id":1181849,"option":"-1","correct":false},{"id":1181850,"option":"0","correct":false},{"id":1181851,"option":"1","correct":false},{"id":1181852,"option":"-1 and 0","correct":true}]},{"q":"<p>You want to convert a matrix to a new MLlib local representation while working with <strong>pyspark.mllib.linalg.matrix</strong> without copying data. Which of these methods will you use to perform this task?</p>","a":[{"id":1181845,"option":"asML()","correct":true},{"id":1181846,"option":"transform()","correct":false},{"id":1181847,"option":"convert()","correct":false},{"id":1181848,"option":"stringify()","correct":false}]},{"q":"<p>You are using the <strong>dot</strong> function in <strong>mllib.linalg.Vector</strong> while working with PySpark 2.4.4. Which of the following is supported in this context:</p>\n\n<ol>\n\t<li>Numpy Array</li>\n\t<li>List</li>\n\t<li>Sparse Vector</li>\n</ol>","a":[{"id":1181841,"option":"1 and 2","correct":false},{"id":1181842,"option":"2 and 3","correct":false},{"id":1181843,"option":"1 and 3","correct":false},{"id":1181844,"option":"All of these","correct":true}]},{"q":"<p>Which of the following data types are used by <strong>PySpark MLlib</strong> for linear algebra operations that involve dense vectors?</p>","a":[{"id":1181837,"option":"NumPy array type","correct":true},{"id":1181838,"option":"Scipy array type","correct":false},{"id":1181839,"option":"Scipy List type","correct":false},{"id":1181840,"option":"NumPy List type","correct":false}]},{"q":"<p>Which of the following modules will you use to mine frequent itemsets in PySpark 2.4.4?</p>","a":[{"id":1181833,"option":"fpm ","correct":true},{"id":1181834,"option":"features ","correct":false},{"id":1181835,"option":"classification ","correct":false},{"id":1181836,"option":"normalizer ","correct":false}]},{"q":"<p>Which of the following parameters must be supplied to create an <strong>FP-Growth</strong> model using the <strong>fpm</strong> module in PySpark 2.4.4?</p>","a":[{"id":1181829,"option":"data","correct":false},{"id":1181830,"option":"minSupport ","correct":false},{"id":1181831,"option":"numPartitions","correct":false},{"id":1181832,"option":"All of these","correct":true}]},{"q":"<p>You want to create a vector representation of words in a text corpus while working with PySpark 2.4.4. Which of these classes in the <strong>features</strong> module can be used to perform this task?</p>","a":[{"id":1181821,"option":"Word2Vec","correct":true},{"id":1181822,"option":"Text2Vec","correct":false},{"id":1181823,"option":"Corpus2Vec","correct":false},{"id":1181824,"option":"conv2Vec","correct":false}]},{"q":"<p>You want to map a sequence of terms to their term frequencies using the hashing trick while working in Pyspark 2.4.4. Which of the following modules will you use to perform this task?</p>","a":[{"id":1181817,"option":"pyspark.mllib.clustering ","correct":false},{"id":1181818,"option":"pyspark.mllib.feature","correct":true},{"id":1181819,"option":"pyspark.mllib.Streaming","correct":false},{"id":1181820,"option":"pyspark.mllib.Hashing","correct":false}]},{"q":"<p>You are using the following PySpark method to perform an online update of centroids while working with a clustering model. What is the appropriate value of the decay factor to ensure that during the update, the weighted mean of the previous and new data is considered:</p>\n\n<p><strong>Method</strong></p>\n\n<pre class=\"prettyprint\"><code>StreamingKMeansModel(clusterCenters, clusterWeights)</code></pre>\n\n<p> </p>","a":[{"id":1181813,"option":"-1","correct":false},{"id":1181814,"option":"0","correct":false},{"id":1181815,"option":"1","correct":true},{"id":1181816,"option":"2","correct":false}]},{"q":"<p>You are training a <strong>K-means</strong> clustering model in PySpark 2.4.4. Which of the following parameters can be used to control the distance threshold within which a center is considered to have converged?</p>","a":[{"id":1181809,"option":"maxIterations","correct":false},{"id":1181810,"option":"k","correct":false},{"id":1181811,"option":"epsilon","correct":true},{"id":1181812,"option":"seed","correct":false}]},{"q":"<p>You are using the <strong>K-means</strong> class in the clustering module of the Pysparks 2.4.4 API to perform some operations. Which of these are the valid values for the initialization algorithm parameter:</p>\n\n<ol>\n\t<li>K-means</li>\n\t<li>Random</li>\n\t<li>Euclidean</li>\n</ol>","a":[{"id":1181805,"option":"1 and 2","correct":true},{"id":1181806,"option":"2 and 3","correct":false},{"id":1181807,"option":"1 and 3","correct":false},{"id":1181808,"option":"All of these","correct":false}]},{"q":"<p>You want to use a partition as an array while working with a DStream in PySpark 2.4.4. Which of these functions can be used to do perform this task?</p>","a":[{"id":1181789,"option":"combine()","correct":false},{"id":1181790,"option":"glom()","correct":true},{"id":1181791,"option":"persist()","correct":false},{"id":1181792,"option":"rowSet()","correct":false}]},{"q":"<p>Which of the following properties can be used to characterize the DStream class internally:</p>\n\n<ol>\n\t<li>List of other DStreams that the DStream depends on</li>\n\t<li>Time interval at which the DStream generates an RDD</li>\n\t<li>Function that is used to generate an RDD after each time interval</li>\n</ol>","a":[{"id":1181781,"option":"1 and 2","correct":false},{"id":1181782,"option":"2 and 3","correct":false},{"id":1181783,"option":"1 and 3","correct":false},{"id":1181784,"option":"All of these","correct":true}]},{"q":"<p>You find the requirements of a schema inference while working with PySpark 2.4.4. Which of these will you use to determine the ratio of rows that are used for schema inference?</p>","a":[{"id":1181777,"option":"samplingRatio ","correct":true},{"id":1181778,"option":"Catalog","correct":false},{"id":1181779,"option":"SchemaRatio","correct":false},{"id":1181780,"option":"DTDSchema","correct":false}]},{"q":"<p>You are working with PySpark 2.4.4. You use the <strong>config</strong> option in a builder class to modify certain configuration parameters. Which of the following classes automatically propagates the changes that are done by using this approach: </p>\n\n<ol>\n\t<li>SparkConf</li>\n\t<li>SparkSession configuration</li>\n</ol>","a":[{"id":1181773,"option":"1","correct":false},{"id":1181774,"option":"2","correct":false},{"id":1181775,"option":"Both of these","correct":true},{"id":1181776,"option":"None of these","correct":false}]},{"q":"<p>Which of the following can you use to process data using PySpark SQL:</p>\n\n<ol>\n\t<li>PySQL</li>\n\t<li>SQL</li>\n\t<li>HiveQL</li>\n</ol>","a":[{"id":1181765,"option":"1","correct":false},{"id":1181766,"option":"2","correct":false},{"id":1181767,"option":"3","correct":false},{"id":1181768,"option":"2 and 3","correct":true}]},{"q":"<p>The <strong>PySpark MLlib</strong> API exposes three core Machine Learning functionalities. Which of the following is not one of these three functionalities?</p>","a":[{"id":1181745,"option":"Machine Learning algorithms","correct":false},{"id":1181746,"option":"Live dashboards","correct":true},{"id":1181747,"option":"Data preparation","correct":false},{"id":1181748,"option":"Utilities","correct":false}]},{"q":"<p>Which of the following is a feature of a PySpark DataFrame:</p>\n\n<ol>\n\t<li>They are distributed in nature</li>\n\t<li>They are immutable in nature</li>\n\t<li>Execution starts before an action is triggered</li>\n</ol>\n\n<p> </p>","a":[{"id":1181741,"option":"1","correct":false},{"id":1181742,"option":"2","correct":false},{"id":1181743,"option":"1 and 2","correct":true},{"id":1181744,"option":"All of these","correct":false}]},{"q":"<p>You have specified the hint parameter in PySpark SQL to optimize planning decisions. What does the hint parameter influence in this scenario:</p>\n\n<ol>\n\t<li>The selection of join strategies.</li>\n\t<li>The repartitioning of the data.</li>\n\t<li>The selection of join strategies and repartitioning of the data.</li>\n</ol>","a":[{"id":1179369,"option":"1","correct":false},{"id":1179370,"option":"2","correct":false},{"id":1179371,"option":"3","correct":true},{"id":1179372,"option":"None of these","correct":false}]},{"q":"<p>Which of the following is true about Datasets in PySpark?</p>\n\n<ol>\n\t<li>Datasets are distributed collection of items and a primary abstraction</li>\n\t<li>Datasets can be created from Hadoop InputFormats</li>\n</ol>","a":[{"id":1179333,"option":"1","correct":false},{"id":1179334,"option":"2","correct":false},{"id":1179335,"option":"Both of these","correct":true},{"id":1179336,"option":"None of these","correct":false}]},{"q":"<p>Which of the following is a valid method to split each line of input when working with PySpark Shell?</p>","a":[{"id":1179325,"option":"line.split(' ')","correct":false},{"id":1179326,"option":"line.split(\"\\n\")","correct":false},{"id":1179327,"option":"line.split(\" \")","correct":true},{"id":1179328,"option":"line.split(\\n)","correct":false}]},{"q":"<p>Which of the following operations on an RDD is an example of transformations?</p>\n\n<ol>\n\t<li>groupBy</li>\n\t<li>Computations</li>\n\t<li>Filter</li>\n</ol>","a":[{"id":1179301,"option":"1","correct":false},{"id":1179302,"option":"2","correct":false},{"id":1179303,"option":"1 and 3","correct":true},{"id":1179304,"option":"2 and 3","correct":false}]},{"q":"<p>In PySpark, you used <strong>enableHiveSupport()</strong> to enable Hive support in SparkSession while working with the pyspark.sql module. The support to which of these options is also enabled in this scenario?</p>","a":[{"id":1179233,"option":"Hive SerDes","correct":false},{"id":1179234,"option":"Hive user-defined functions","correct":false},{"id":1179235,"option":"Hive sessions","correct":false},{"id":1179236,"option":"Hive SerDes and Hive user-defined functions","correct":true}]},{"q":"You are using the Co-grouped map operations with Pandas instances to cogroup PySpark DataFrames with a common key. When doing so, you are applying a function to each cogroup. Which of the following is an optional input to the function?","a":[{"id":1181267,"option":"A tuple representing the key","correct":true},{"id":1181268,"option":"pandas.DataFrame","correct":false},{"id":1181269,"option":"PySpark DataFrame","correct":false},{"id":1181270,"option":"StructType object","correct":false}]},{"q":"You want to create a Pandas UDF similar to PySpark’s aggregate functions. Which of the following type hints can be used for this purpose?","a":[{"id":1181255,"option":"pandas.Series, … -> Any","correct":true},{"id":1181256,"option":"Iterator[pandas.Series] -> Iterator[pandas.Series].","correct":false},{"id":1181257,"option":"pandas.Series, … -> Var","correct":false},{"id":1181258,"option":"pandas.Series, … -> All","correct":false}]},{"q":"Analyze the type hint shown alongside. If the given type hint is used with a pandas udf what operation is likely to be performed?","a":[{"id":1181251,"option":"It creates a Pandas UDF where the given function takes an iterator of a tuple of multiple pandas.Series and outputs an iterator of pandas.Series.","correct":true},{"id":1181252,"option":"It creates a Pandas UDF where the given function takes an iterator of pandas.Series and outputs an iterator of pandas.Series. ","correct":false},{"id":1181253,"option":" It creates a Pandas UDF where the given function takes one or more pandas.Series and outputs one pandas.Series. ","correct":false},{"id":1181254,"option":"None of these","correct":false}]},{"q":"You are defining a custom profiler to expose spark stages. In the given context which of these methods would you need to define?","a":[{"id":1181239,"option":"profile","correct":false},{"id":1181240,"option":"dump","correct":false},{"id":1181241,"option":"add","correct":false},{"id":1181242,"option":"All of these","correct":true}]},{"q":"You want to use local checkpointing along with dynamic allocation while working in pySpark. Which of the given practices should be followed while doing so?","a":[{"id":1181227,"option":"set spark.dynamicAllocation.cachedExecutorIdleTimeout to a high value.","correct":true},{"id":1181228,"option":"set spark.dynamicAllocation.MaxParallelExecutors to a high value.","correct":false},{"id":1181229,"option":"set spark.dynamicAllocation.cacheBatchSize to a high value.","correct":false},{"id":1181230,"option":"set spark.dynamicAllocation.executorSize to a high value.","correct":false}]},{"q":"Analyze the UDF function used in pySpark given alongside. Which of the following statements regarding this code are valid?","a":[{"id":1181215,"option":"This type of UDF does not support partial aggregation","correct":false},{"id":1181216,"option":"all data for a group or window will be loaded into memory by this code.","correct":false},{"id":1181217,"option":"Only unbounded window is supported by this code","correct":false},{"id":1181218,"option":"All of these","correct":true}]},{"q":"You want to compute a pair-wise frequency table of certain columns. Which of these methods in the pyspark.sql.DataFrameStatFunctions module will you use to do so?","a":[{"id":1181219,"option":"cov(col1, col2)","correct":false},{"id":1181220,"option":"corr(col1, col2, method=None)","correct":false},{"id":1181221,"option":"crosstab(col1, col2)","correct":true},{"id":1181222,"option":"freqItems(cols, support=None)","correct":false}]},{"q":"You are asked to refactor the computation given alongside directly using the aggregateMessages operator. Which of these reasons justify the validation of this request?","a":[{"id":1181211,"option":"The given code is deserialized and prone to error","correct":false},{"id":1181212,"option":"The given code is costly due to the substantial communication required","correct":false},{"id":1181213,"option":"The given code is costly due to the duplicate information generated","correct":false},{"id":1181214,"option":"Both 2,3","correct":true}]},{"q":"You want to obtain a model which can transform categorical features to use 0-based indices. Which of these classes in pySpark.ml library will you use to do so?","a":[{"id":1181203,"option":"pyspark.ml.feature.VectorIndexer(maxCategories=20, inputCol=None, outputCol=None, handleInvalid='error')","correct":true},{"id":1181204,"option":" pyspark.ml.feature.VectorAssembler(inputCols=None, outputCol=None, handleInvalid='error')","correct":false},{"id":1181205,"option":"pyspark.ml.feature.StringIndexer(maxCategories=20, inputCol=None, outputCol=None, handleInvalid='error')","correct":false},{"id":1181206,"option":"pyspark.ml.feature.RegexIndexer(maxCategories=20, inputCol=None, outputCol=None, handleInvalid='error')","correct":false}]},{"q":"You are collecting neighboring vertices and their attributes at each vertex for a directed graph in pySpark. In the given context which of these operators can aid you to do so?<br><br>1. collectNeighborIds<br>2. collectNeighbors ","a":[{"id":1181207,"option":"Only 1","correct":false},{"id":1181208,"option":"Only 2","correct":false},{"id":1181209,"option":"Both 1,2","correct":true},{"id":1181210,"option":"None of these","correct":false}]},{"q":"You are using the createPollingStream() of the flume model to create an input stream. What does the parameter maxBatchSize of this method contain?","a":[{"id":1180068,"option":"The maximum number of events to be pulled from Storage","correct":false},{"id":1180069,"option":"The maximum number of events to be pulled from the Spark sink in a single RPC call","correct":true},{"id":1180070,"option":"The maximum number of concurrent requests of this stream","correct":false},{"id":1180071,"option":"The maximum number of events running in the Spark Sink","correct":false}]},{"q":"Which of the following assigns points to the nearest center in the K-means clustering method?<br>1. E-step<br>2. M-step","a":[{"id":1179269,"option":"Only 1","correct":true},{"id":1179270,"option":"Only 2","correct":false},{"id":1179271,"option":"Both 1 and 2","correct":false},{"id":1179272,"option":"Neither 1 nor 2","correct":false}]},{"q":"Assume that you have set the enforceSchema parameter of the csv function in pyspark.sql.DataFrameReader(spark) interface.<br><br>To which of these is the specified schema forcibly applied to?","a":[{"id":1179261,"option":"Datasource files","correct":false},{"id":1179262,"option":"Headers in CSV files","correct":false},{"id":1179263,"option":"First header in RDD","correct":false},{"id":1179264,"option":"Both 1 and 2","correct":true}]},{"q":"<p>John is working on SQL in PySpark. he wants to know the main entry point for DataFrame.Help him to find which of the following classes will be used?</p>","a":[{"id":1181769,"option":"pyspark.sql.Window ","correct":false},{"id":1181770,"option":"pyspark.sql.Column ","correct":false},{"id":1181771,"option":"pyspark.sql.DataFrame ","correct":false},{"id":1181772,"option":"pyspark.sql.SparkSession ","correct":true}]},{"q":"<p>You want to calculate the approximate quantiles of numerical columns of a DataFrame when working with PySpark SQL. To do so, you implement the approxQuantile algorithm. Given that</p>\n\n<ol>\n\t<li>The DataFrame has N elements</li>\n\t<li>You have requested the quantile at probability p up to error err.</li>\n</ol>\n\n<p>In the given context, what will the algorithm return?</p>","a":[{"id":1180156,"option":"A sample x from the DataFrame","correct":true},{"id":1180157,"option":"The exact rank of the sample x from the DataFrame","correct":false},{"id":1180158,"option":"The exact rank of all samples in the DataFrame","correct":false},{"id":1180159,"option":"All samples in the DataFrame","correct":false}]},{"q":"<p>In which of the following scenarios is pulling data sets into a cluster-wide in-memory cache useful:</p>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>Data is accessed repeatedly</li>\n\t<li>Data is accessed from contiguous memory locations</li>\n\t<li>Transforming a dataset</li>\n</ol>","a":[{"id":1179349,"option":"1","correct":false},{"id":1179350,"option":"2","correct":false},{"id":1179351,"option":"3","correct":false},{"id":1179352,"option":"1 and 2","correct":true}]},{"q":"<p>Assume that you have used a standalone Python script by creating a SparkContext to utilize PySpark. Which of the following objects can be used to add configuration properties in the given context?</p>","a":[{"id":1180100,"option":"binConf","correct":false},{"id":1180101,"option":"Sessions ","correct":false},{"id":1180102,"option":"Contexts","correct":false},{"id":1180103,"option":"SparkConf ","correct":true}]},{"q":"<p>In PySpark, you are using a method to determine the approximate quantiles of numerical columns of a DataFrame. If the input parameter <strong>col</strong> is a string then which of the following does this method return?</p>","a":[{"id":1179241,"option":"List","correct":false},{"id":1179242,"option":"List of floats","correct":true},{"id":1179243,"option":"String","correct":false},{"id":1179244,"option":"List of list of floats","correct":false}]},{"q":"<p>You have optimized the conversion of Spark DataFrame to Pandas DataFrame using Arrow. Which of the following can be used to deal with an error that occurs before the actual computation within Spark?</p>","a":[{"id":1182129,"option":"spark.sql.execution.arrow.fallback","correct":true},{"id":1182130,"option":"spark.sql.execution.arrow.callback","correct":false},{"id":1182131,"option":"spark.sql.execution.arrow.tail","correct":false},{"id":1182132,"option":"spark.sql.execution.arrow.failback","correct":false}]},{"q":"<p>Which of the following is true with respect to executing jobs in Spark?</p>\n\n<ol>\n\t<li>Spark breaks up the processing of RDD operations into tasks and each of it executed by an executor.</li>\n\t<li>Spark computes the task’s closure after the execution.<br>\n\t </li>\n</ol>","a":[{"id":1179377,"option":"1","correct":true},{"id":1179378,"option":"2","correct":false},{"id":1179379,"option":"Both of these","correct":false},{"id":1179380,"option":"None of these","correct":false}]},{"q":"<p>In PySpark, which of the following tasks can be accomplished using a SparkSession while working with the pyspark.sql module?</p>\n\n<ol>\n\t<li>A DataFrame can be created</li>\n\t<li>A DataFrame can be registered as a table</li>\n</ol>","a":[{"id":1179229,"option":"Both of these","correct":true},{"id":1179230,"option":"1","correct":false},{"id":1179231,"option":"2","correct":false},{"id":1179232,"option":"None of these","correct":false}]},{"q":"Which of the following methods can be used to return number of rows in a DataFrame in PySpark v 1.3?<br><br>[Assume that df is the newly created DataFrame]","a":[{"id":1179249,"option":"df.count()","correct":true},{"id":1179250,"option":"df.count(c) \n\nWhere c is comma separated names of all columns ","correct":false},{"id":1179251,"option":"df.collect()","correct":false},{"id":1179252,"option":"df.collect(c) \n\nWhere c is comma separated names of all columns ","correct":false}]},{"q":"You are using the csv function in pyspark.sql.DataFrameReader(spark) interface to load a CSV file and returns the result as a DataFrame.<br><br>When will this function go through the input once to determine the input schema?","a":[{"id":1179253,"option":"If inferSchema is enabled and the schema is explicitly specified using \"schema\"","correct":false},{"id":1179254,"option":"If inferSchema is disabled","correct":false},{"id":1179255,"option":"If the schema is explicitly specified using \"schema\"","correct":false},{"id":1179256,"option":"If inferSchema is enabled","correct":true}]},{"q":"<p>You have created a DataFrame from the text of the FRAME file in the Spark source directory as shown alongside.Which of these codes can be used to get the number of rows in this DataFrame?</p>\n\n<p>Code:</p>\n\n<pre class=\"prettyprint\"><code>textFile = spark.read.text(\"FRAME.md\")</code></pre>","a":[{"id":1179417,"option":"textFile.row()","correct":false},{"id":1179418,"option":"textFile.count()","correct":true},{"id":1179419,"option":"textFile.count(row)","correct":false},{"id":1179420,"option":"textFile.row(count)","correct":false}]},{"q":"<p>You are using the predict(x) method of the clustering model derived from the k means method. The parameter x is a data point or RDD of points to determine the cluster index. What does the predict(x) return in the given context:</p>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>Predicted cluster index</li>\n\t<li>An RDD of predicted cluster indices&nbsp;</li>\n</ol>","a":[{"id":1180112,"option":"1","correct":false},{"id":1180113,"option":"2","correct":false},{"id":1180114,"option":"Both of these","correct":true},{"id":1180115,"option":"None of these","correct":false}]},{"q":"<p>In PySpark, what is the correct solution of the following exception that is thrown in the&nbsp;'main' org.apache.spark.sql.AnalysisException.</p>\n\n<p><strong>Exception:</strong></p>\n\n<p>resolved attribute(s) surname#20 missing from id#0,birthDate#3,name#10,surname#7 in operator !Project [id#0,birthDate#3,name#10,surname#20,UDF(birthDate#3) AS birthDate_cleaned#8];</p>\n\n<p><strong>Options:</strong>.</p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>left = left.select(specific_coluns_from_left)\nleft.cache()\nright = left.select(specific_coluns_from_left)\nright.cache()\nleft.join(right, ['column_to_join'])</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>left = left.select(specific_coluns_from_left)\nleft.cache()\nright = right.select(specific_coluns_from_left)\nright.cache()\nleft.join(right, ['column_to_join'])</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>right = left.select(specific_coluns_from_left)\nleft.cache()\nleft = right.select(specific_coluns_from_left)\nright.cache()\nleft.join(right, ['column_to_join'])</code></pre>\n\t</li>\n</ol>","a":[{"id":1182065,"option":"1","correct":false},{"id":1182066,"option":"2","correct":true},{"id":1182067,"option":"3","correct":false},{"id":1182068,"option":"None of these","correct":false}]},{"q":"<p>You want to enable arrow-based columnar data transfers while working with PySpark. Which of the following code&nbsp;snippets will allow you to perform this action?</p>","a":[{"id":1182053,"option":"spark.conf.set(\"spark.pandas.arrow.enabled\", \"true\")","correct":false},{"id":1182054,"option":"spark.conf.job(\"spark.execution.arrow.enabled\", \"true\")","correct":false},{"id":1182055,"option":"spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")","correct":true},{"id":1182056,"option":"spark.conf.set(\"pandas.convert.arrow.enabled\", \"true\")","correct":false}]},{"q":"<p>When working with PySpark RDDs, you want to use the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file. Which of the following code snippets is the right way to do so?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>lines=sc.textFile(\"data.txt\")\npairs=map(lambda(lines,1))\ncounts=pairs.reduceByKey(lambda a,b:a+b)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>lines=sc.textFile(\"data.txt\")\npairs=lines.map(lambda s:(s,1))\ncounts=pairs.reduceByKey(lambda a,b:a+b)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>lines=sc.textFile(\"data.txt\")\npairs=map(lambda sc:(sc,1)).lines\ncounts=pairs.reduceByKey(lambda a,b:a+b)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>lines=sc.textFile(\"data.txt\")\npairs=map(lambda sc:(sc,1)).lines\ncounts=reduceByKey(lambda a,b:a+b).pairs</code></pre>\n\t</li>\n</ol>\n\n<p>&nbsp;</p>","a":[{"id":1179389,"option":"1","correct":false},{"id":1179390,"option":"2","correct":true},{"id":1179391,"option":"3","correct":false},{"id":1179392,"option":"4","correct":false}]},{"q":"<p>You are given RDD of ‘<strong>implicit preferences</strong>’ of users for a subset of products and asked to train a matrix factorization model using PySpark 2.4.4. Which of the following methods in <strong>pyspark.mllib.recommendation</strong> will you use?</p>","a":[{"id":1182133,"option":"train()","correct":false},{"id":1182134,"option":"trainImplicit()","correct":true},{"id":1182135,"option":"trainExplicit()","correct":false},{"id":1182136,"option":"Rating()","correct":false}]},{"q":"<p>You want to integrate HappyBase with Python for interactions with HBase. Which of the following steps must be taken to take advantage of the distributed nature of Spark?</p>","a":[{"id":1182109,"option":"Providing explicit definition for the parallel computations","correct":true},{"id":1182110,"option":"Providing explicit cache utilization","correct":false},{"id":1182111,"option":"Providing explicit thread utilization thresholds","correct":false},{"id":1182112,"option":"Providing explicit definition for DataFrame standard formats","correct":false}]},{"q":"<p>You want to print raw prediction scores while working with the <strong>SVMModel </strong>class in PySpark. Which of these methods will you use while performing this task?</p>","a":[{"id":1181969,"option":"clearThreshold()","correct":true},{"id":1181970,"option":"Span()","correct":false},{"id":1181971,"option":"Truncate()","correct":false},{"id":1181972,"option":"Tree()","correct":false}]},{"q":"<p>The availability of Python is required by PySpark on the <strong>System PATH</strong> to run programs by default. Which of the following environment variables must be set in order to perform this action?</p>","a":[{"id":1181941,"option":"PysparkPython","correct":false},{"id":1181942,"option":"Pyspark_Python","correct":false},{"id":1181943,"option":"PYSPARK_PYTHON","correct":true},{"id":1181944,"option":"PYSPARK PYTHON","correct":false}]},{"q":"When working with PySpark SQL, you want to return a column which is a substring of the column. Which of the following is a correct way to implement this?","a":[{"id":1180064,"option":"df.collect(df.name.substr(1, 3).alias(\"col\"))","correct":false},{"id":1180065,"option":"df.select(df.name.substr(1, 3).alias(\"col\"))","correct":false},{"id":1180066,"option":"df.select(df.name.substr(1, 3).collect()","correct":false},{"id":1180067,"option":"df.select(df.name.substr(1, 3).alias(\"col\")).collect()","correct":true}]},{"q":"How is the csv file encoded if the encoding parameter in the csv function of pyspark.sql.DataFrameReader(spark) interface is set to none?","a":[{"id":1179257,"option":"Using UTF-8","correct":true},{"id":1179258,"option":"Using UTF-16BE","correct":false},{"id":1179259,"option":"Using IEC 2022","correct":false},{"id":1179260,"option":"None of these","correct":false}]},{"q":"<p>John is working on SQL in PySpark. He is using the utility functions provided in class <strong>pyspark.sql.Window</strong> to define a window in DataFrames and the Ordering is defined. Which of the following will happens in this scenario?</p>","a":[{"id":1180128,"option":" A growing window frame is used by default.","correct":false},{"id":1180129,"option":"An unbounded window frame is used by default","correct":true},{"id":1180130,"option":"A range window frame is used by default.\r\n","correct":false},{"id":1180131,"option":"None of the above","correct":false}]},{"q":"<p>What happens when you execute the query given alongside?</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE trial (a.b int);\norg.apache.spark.sql.catalyst.parser.ParseException:\nno viable alternative at input 'CREATE TABLE trial (a.'(line 1, pos 20)</code></pre>","a":[{"id":1179405,"option":"A table trial is created","correct":false},{"id":1179406,"option":"CREATE TABLE fails with a ParseException because of the illegal identifier name a.b","correct":true},{"id":1179407,"option":"CREATE TABLE fails with ParseException because special character ` is not escaped","correct":false},{"id":1179408,"option":"A table trial is not created and no exception is thrown","correct":false}]},{"q":"<p>You want to process data by using&nbsp;SQL and HiveQL. Which of the following can you use for this?</p>","a":[{"id":1181749,"option":"PySpark SQL","correct":true},{"id":1181750,"option":"Dataframe","correct":false},{"id":1181751,"option":"PySpark Core","correct":false},{"id":1181752,"option":"RDD","correct":false}]},{"q":"<p>You are defining a custom profiler to expose spark stages. In the given context which of these methods would you need to define?</p>","a":[{"id":1182177,"option":"profile","correct":false},{"id":1182178,"option":"dump","correct":false},{"id":1182179,"option":"add","correct":false},{"id":1182180,"option":"All of these","correct":true}]},{"q":"<p>Consider the Python code given alongside. What should be used in place of XXX such that output is as follows?</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>df = spark.createDataFrame([(1, 400), (2, 300)], (\"\"id\"\", \"\"age\"\"))\n\ndef filter_func(iterator):\n    for pdf in iterator:\n     XXX\n\ndf.mapInPandas(filter_func, schema=df.schema).show()</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre class=\"prettyprint\"><code>id price\n1 400</code></pre>","a":[{"id":1182161,"option":"   yield pdf[pdf.id == 1]","correct":true},{"id":1182162,"option":"   yield.pdf[pdf.id == 1]","correct":false},{"id":1182163,"option":"if (pdf.id == 1)","correct":false},{"id":1182164,"option":"   yield pdf[ id == 1]","correct":false}]},{"q":"<p>When working in PySpark, you observe that the objects are serialized in batches by default. You want to control the batch size through the SparkContext's batchSize parameter. Which of the following is a valid way to implement this?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>sc = SparkContext('local', 'test', batchSize=2)\nrdd = sc.parallelize(range(16), 4).map;</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>sc = SparkContext('local', 'test')\nrdd = sc.map(batchSize=2).parallelize(range(16), 4)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>sc = SparkContext('local', 'test', batchSize=2)\nrdd = sc.parallelize(range(16), 4).map(lambda x: x)</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>sc = SparkContext('local', 'test', batchSize=2)\nrdd = sc.map(lambda x: x).parallelize(range(16), 4)</code></pre>\n\t</li>\n</ol>","a":[{"id":1180136,"option":"1","correct":false},{"id":1180137,"option":"2","correct":false},{"id":1180138,"option":"3","correct":true},{"id":1180139,"option":"4","correct":false}]},{"q":"<p>Which of the following commands can be run to use the bin/pyspark shell with a standalone Spark cluster?</p>","a":[{"id":1180076,"option":"$ IPYTHON=1 ./bin/pyspark","correct":true},{"id":1180077,"option":"help(pyspark)","correct":false},{"id":1180078,"option":"$ MASTER=spark://IP:PORT ./bin/pyspark","correct":false},{"id":1180079,"option":"spark://IP:PORT ./bin/pyspark","correct":false}]},{"q":"<p>In PySpark, what is&nbsp;the cartesian product of the following two DataFrames:</p>\n\n<p><strong>DataFrames:</strong></p>\n\n<ul>\n\t<li>df.select(\"age\", \"name\").collect()</li>\n\t<li>df2.select(\"name\", \"height\").collect()</li>\n</ul>","a":[{"id":1181989,"option":"df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()","correct":true},{"id":1181990,"option":"df.crossJoin(df.select(\"height\")).select(\"age\", \"name\").collect()","correct":false},{"id":1181991,"option":"df.crossJoin(df2.select(\"name\",\"height\")).select(\"age\", \"name\").collect()","correct":false},{"id":1181992,"option":"df.crossJoin(df2.select(\"name\",\"height\")).select(\"age\", \"name\").collect()","correct":false}]},{"q":"<p>In RDD, the number of unique elements is found to be 800000. If you are given&nbsp;the time that it takes to count all the elements in an RDD, then which of the following statements is most suitable to get the desired output in optimal time?</p>","a":[{"id":1182077,"option":"Use CountApprox to get the results within a Timeout","correct":true},{"id":1182078,"option":"Use CountApproxDistinct to get the distinct records","correct":false},{"id":1182079,"option":"Use CountByKey to fetch the elements for each key","correct":false},{"id":1182080,"option":"Use Filter to get the desired result","correct":false}]},{"q":"<p>In PySpark, Which of these&nbsp;is the cause&nbsp;the following error occurs, while calling o206.showString.</p>\n\n<p><strong>Error:</strong></p>\n\n<p>org.apache.spark.SparkException: Job 25 canceled because SparkContext was shut down</p>\n\n<p>&nbsp;</p>","a":[{"id":1182069,"option":"Null on the join field","correct":false},{"id":1182070,"option":"OutOfMemoryError","correct":false},{"id":1182071,"option":"Too many rows on join","correct":false},{"id":1182072,"option":"All of these","correct":true}]},{"q":"<p>In PySpark, which of these options can be the cause of the following&nbsp;error in the 'main'&nbsp;org.apache.spark.sql.AnalysisException.</p>\n\n<p><strong>Exception:</strong></p>\n\n<p>resolved attribute(s) surname#20 missing from id#0,birthDate#3,name#10,surname#7 in operator !Project [id#0,birthDate#3,name#10,surname#20,UDF(birthDate#3) AS birthDate_cleaned#8];</p>\n\n<p><strong>Options:</strong></p>\n\n<p>1<strong>.</strong>This error usually happens when there are two data frames and you apply UDF on some columns to transfer,aggregate, and rejoin to add as new fields on a new data frame.</p>\n\n<p>2.This error usually happens when there are two data frames and you apply UDF on some columns to transfer and add new fields on a new data frame.</p>\n\n<p>3.This error usually happens when there are two data frames and you apply UDF on some columns and add new fields on a new data frame.</p>","a":[{"id":1182061,"option":"1","correct":true},{"id":1182062,"option":"2","correct":false},{"id":1182063,"option":"3","correct":false},{"id":1182064,"option":"Cannot be determined","correct":false}]},{"q":"<p>You are using AWS service and PySpark together. Which of these&nbsp;actions must be performed to validate the following statement:</p>\n\n<p><strong>Statement:</strong></p>\n\n<p>The given AWS credentials&nbsp;get saved in DStream checkpoints if checkpointing is enabled. Make sure that your checkpoint directory is secure.</p>","a":[{"id":1182029,"option":"The checkpoint directory should be encrypted.","correct":false},{"id":1182030,"option":"The checkpoint directory should be password protected.","correct":false},{"id":1182031,"option":"Hide the checkpoint credentials directory.","correct":false},{"id":1182032,"option":"All of these","correct":true}]},{"q":"<p>From the given statements choose the correct option.</p>\n\n<ol>\n\t<li>The number of cores allocated to the Spark Streaming application must be more than the number of receivers.</li>\n\t<li>When running a Spark Streaming program locally, use local as the master URL.</li>\n</ol>","a":[{"id":1179293,"option":"1","correct":true},{"id":1179294,"option":"2","correct":false},{"id":1179295,"option":"Both of these","correct":false},{"id":1179296,"option":"None of these","correct":false}]},{"q":"<p>You have created Parallelized collections by calling SparkContext’s parallelize method on an existing collection in your driver program. What is the right way to manually set 3 partitions in this scenario?</p>","a":[{"id":1179305,"option":"sc.parallelize(data, 3)","correct":true},{"id":1179306,"option":"sc.parallelize(3, data)","correct":false},{"id":1179307,"option":"sc.parallelize(3)","correct":false},{"id":1179308,"option":"sc.parallel(3, data)","correct":false}]},{"q":"<p>Which of the following PySpark Shell actions can you use to see the contents of an RDD?</p>","a":[{"id":1181933,"option":"RDD. Collect ()","correct":false},{"id":1181934,"option":"RDDread. Collect ()","correct":true},{"id":1181935,"option":"RDDread. ContentCollect ()","correct":false},{"id":1181936,"option":"RDDread. Content ()","correct":false}]},{"q":"<p>During the installation of PySpark, what is the value of the <strong>User </strong>and <strong>System </strong>variables?</p>","a":[{"id":1181929,"option":"User Variable: SPARK_HOME\r\nSystem Variable: PATH","correct":true},{"id":1181930,"option":"User Variable: HOME\r\nSystem Variable: PATH","correct":false},{"id":1181931,"option":"User Variable: HOME\r\nSystem Variable: SPARK_PATH","correct":false},{"id":1181932,"option":"User Variable: SPARK_HOME\r\nSystem Variable: SPARK_PATH","correct":false}]},{"q":"<p>PySpark RDD performs two primary types of operations. What are the names of these two operations?</p>","a":[{"id":1181885,"option":"1. Transformations\r\n2. Actions","correct":true},{"id":1181886,"option":"1. Transactions\r\n2. Actions","correct":false},{"id":1181887,"option":"1. Transactions\r\n2. Transformations","correct":false},{"id":1181888,"option":"1. Streaming\r\n2. Actions","correct":false}]},{"q":"<p>In PySpark Streaming, how does Spark Engine process the broken downstream data that is forwarded to Engine?</p>","a":[{"id":1181881,"option":"Spark Engine processes these data batches using complex algorithms expressed with high-level functions such as map, reduce, join, and window","correct":true},{"id":1181882,"option":"Spark Engine processes these data batches using sources such as Apache Flume, Kinesis, Kafka, and TCP sockets etc.","correct":false},{"id":1181883,"option":"Both of these","correct":false},{"id":1181884,"option":"None of these","correct":false}]},{"q":"<p>You want to create a clustering model derived from the <strong>bisecting K-means</strong> method while working with Pysparks API 2.4.4. Which of the following packages will you consider to use the functions that are available in these packages to perform this task?</p>","a":[{"id":1181797,"option":"pyspark.mllib.clustering ","correct":true},{"id":1181798,"option":" pyspark.mllib.classification","correct":false},{"id":1181799,"option":"pyspark.mllib.Streaming","correct":false},{"id":1181800,"option":"pyspark.mllib.Means","correct":false}]},{"q":"<p>Which of these methods is recommended to pass functions in the driver program to run on the cluster in Spark?<br>\n </p>","a":[{"id":1179373,"option":"Use Local defs inside the function calling into Spark","correct":false},{"id":1179374,"option":"Use Lambda expressions","correct":false},{"id":1179375,"option":"Use Lambda expressions and  Local defs inside the function calling into Spark","correct":true},{"id":1179376,"option":"Use Local defs in a module","correct":false}]},{"q":"<p>The syntax given alongside is of a Binary Literal in PySpark SQL. What does the parameter num represent here?</p>\n\n<pre class=\"prettyprint\"><code>X { 'num [ ... ]' | \"num [ ... ]\" }</code></pre>\n\n<p> </p>","a":[{"id":1179345,"option":"Any hexadecimal number from 0 to F","correct":true},{"id":1179346,"option":"Any integer from 0 to 9","correct":false},{"id":1179347,"option":"Any integer from -1 to 9","correct":false},{"id":1179348,"option":"None of these","correct":false}]},{"q":"Assume that you encounter the exception shown alongside while working with the isStreaming function in spark. Which of the following can be a valid cause for the same? ","a":[{"id":1181247,"option":"You are using a method that returns a single answer,","correct":true},{"id":1181248,"option":"You have not defined the streaming source","correct":false},{"id":1181249,"option":"Your spark streaming components have encountered a version mismatch","correct":false},{"id":1181250,"option":"Your connection to the streaming component source has timed out","correct":false}]},{"q":"Which of the following can be used to create a Column instance by selecting a column out of a DataFrame?","a":[{"id":1180048,"option":"\ndf[\"colName\"]","correct":false},{"id":1180049,"option":"df.colName + 1\n1 / df.colName","correct":false},{"id":1180050,"option":"df.colName\ndf[\"colName\"]","correct":true},{"id":1180051,"option":"df.colName","correct":false}]},{"q":"<p>Which of the following command should you run in the terminal window to start PySpark Shell?</p>","a":[{"id":1181761,"option":"~$ pyspark","correct":true},{"id":1181762,"option":"~$ pysparkshell","correct":false},{"id":1181763,"option":"~$ sparkshell","correct":false},{"id":1181764,"option":"~$ pyshell","correct":false}]},{"q":"<p>John is working on SQL in PySpark. He was using the nulls_sort_order parameter in the ORDER BY statement and NULLS LAST is specified. Help him to find which of the following will happens.</p>","a":[{"id":1179401,"option":"NULL values are returned first regardless of the sort order","correct":false},{"id":1179402,"option":"NULL values are returned last regardless of the sort order","correct":true},{"id":1179403,"option":"Nothing returns","correct":false},{"id":1179404,"option":"None of the above","correct":false}]},{"q":"<p>You want to be able to use scalar Pandas UDF while working on PySpark. Which of the following requirements must be satisfied.</p>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>The size of the input and output series must be the same.</li>\n\t<li>The result of the user-defined function must be independent of the splitting.</li>\n</ol>","a":[{"id":1182121,"option":"1","correct":false},{"id":1182122,"option":"2","correct":false},{"id":1182123,"option":"Both of these","correct":true},{"id":1182124,"option":"None of these","correct":false}]},{"q":"<p>You are using the <strong>pyspark.sql.WindowSpec(jspec)</strong> class while working in PySpark. Which of the following methods are recommended for specifying the boundary values?</p>","a":[{"id":1182117,"option":" Window.unboundedPreceding","correct":false},{"id":1182118,"option":"Window.unboundedFollowing","correct":false},{"id":1182119,"option":"Window.currentRow","correct":false},{"id":1182120,"option":"All of these","correct":true}]},{"q":"<p>You want the following&nbsp;method&nbsp;to throw an exception when it encounters corrupted records. Which of these values should be set for the mode configuration parameter:</p>\n\n<p><strong>Method</strong></p>\n\n<pre class=\"prettyprint\"><code>pyspark.sql.DataFrameReader(spark)</code></pre>","a":[{"id":1182113,"option":"Permissive","correct":false},{"id":1182114,"option":"DropMalformed","correct":false},{"id":1182115,"option":"Failfast","correct":true},{"id":1182116,"option":"Abrasive","correct":false}]},{"q":"<p>Analyze the snippet given alongside and select the valid risks to be considered while executing the same.</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>df = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"\"id\"\", \"\"v\"\"))  \ndef mean_func(key, pdf):\n    return pd.DataFrame([key + (pdf.v.mean(),)])\ndf.groupby('id').applyInPandas(\n    mean_func, schema=\"\"id long, v double\"\").show()  \ndef sum_func(key, pdf):\n       return pd.DataFrame([key + (pdf.v.sum(),)])\ndf.groupby(df.id, ceil(df.v / 2)).applyInPandas(\"</code></pre>","a":[{"id":1182173,"option":"potential corruption risk if data is repeated","correct":false},{"id":1182174,"option":"potential ordering risk if data is redundant","correct":false},{"id":1182175,"option":"potential OOM risk if data is skewed ","correct":true},{"id":1182176,"option":"All of these","correct":false}]},{"q":"<p>You want to use the groupBy().applyInPandas() available in a grouped map while working with PySpark. Which of these prerequisites would you need to meet while doing so?</p>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>A Python function that defines the computation for each group needs to be defined.</li>\n\t<li>A StructType object or a string that defines the schema of the output PySpark DataFrame needs to be defined.</li>\n</ol>","a":[{"id":1182169,"option":"1","correct":false},{"id":1182170,"option":"2","correct":false},{"id":1182171,"option":"Both of these","correct":true},{"id":1182172,"option":"None of these","correct":false}]},{"q":"<p>You want to use local checkpointing along with dynamic allocation while working in pySpark. Which of the given practices should be followed while doing so?</p>","a":[{"id":1182165,"option":"set spark.dynamicAllocation.cachedExecutorIdleTimeout to a high value","correct":true},{"id":1182166,"option":"set spark.dynamicAllocation.MaxParallelExecutors to a high value","correct":false},{"id":1182167,"option":"set spark.dynamicAllocation.cacheBatchSize to a high value","correct":false},{"id":1182168,"option":"set spark.dynamicAllocation.executorSize to a high value","correct":false}]},{"q":"<p>You want to compute a pair-wise frequency table of certain columns. Which of these methods in the pyspark.sql.DataFrameStatFunctions module will you use to do so?</p>","a":[{"id":1182157,"option":"cov(col1, col2)","correct":false},{"id":1182158,"option":"corr(col1, col2, method=None)","correct":false},{"id":1182159,"option":"crosstab(col1, col2)","correct":true},{"id":1182160,"option":"freqItems(cols, support=None)","correct":false}]},{"q":"<p>In PySpark, which of the following use cases allows you to use an <strong>Estimator</strong> class?</p>","a":[{"id":1182001,"option":"To get the parameters and their coefficents","correct":false},{"id":1182002,"option":"To estimate the predictions","correct":false},{"id":1182003,"option":"To fit a model with the dataset","correct":true},{"id":1182004,"option":"All of these","correct":false}]},{"q":"<p>The items in RDD are partitioned into <span class=\"mathjax-latex\">\\(j\\)</span>, <span class=\"mathjax-latex\">\\(j+k\\)</span>, <span class=\"mathjax-latex\">\\(2\\times j+k\\)</span>&nbsp;where <span class=\"mathjax-latex\">\\(j\\)</span> is the number of partitions. For a Spark job to be executed, the index must be unique. In this partition,there exists a gap&nbsp;so that the Spark job does not get triggered. Find the corresponding method that can be added in the following code to solve this issue:</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).______.collect()</code></pre>","a":[{"id":1182089,"option":"zip()","correct":false},{"id":1182090,"option":"zipWithIndex()","correct":false},{"id":1182091,"option":"zipWithUniqueId()","correct":true},{"id":1182092,"option":"set(zip())","correct":false}]},{"q":"<p>Two jobs (A and B) are running simultaneously. To understand the status of both the jobs, you are using the StatusCheck class to get the information of both the jobs. While fetching the info of job A, it successfully returns 'Status Completed' whereas job B does not return anything. Which of the following issues can&nbsp;occur while fetching job information of B since both use&nbsp;the same API?</p>","a":[{"id":1182085,"option":"Job B can collected garbage","correct":false},{"id":1182086,"option":"API could not retain staged jobs, therefore, it cannot get information of Job B","correct":false},{"id":1182087,"option":"Job B can fail","correct":false},{"id":1182088,"option":"All of these","correct":true}]},{"q":"<p>You want to add spark-streaming-kinesis-asl_2.12 and its dependencies while working on PySpark. Which of the following options will you use to perform this task?</p>","a":[{"id":1182041,"option":"--packages","correct":true},{"id":1182042,"option":"--dependencies","correct":false},{"id":1182043,"option":"--environment","correct":false},{"id":1182044,"option":"--config","correct":false}]},{"q":"<p>Which of these PySpark SQL queries can be used to sort rows in ascending manner keeping null values to be first:</p>\n\n<ol>\n\t<li>SELECT name, age FROM person ORDER BY age;</li>\n\t<li>SELECT name, age FROM person ORDER BY age ASC NULLS FIRST;</li>\n</ol>","a":[{"id":1179397,"option":"1","correct":false},{"id":1179398,"option":"2","correct":false},{"id":1179399,"option":"Both of these","correct":true},{"id":1179400,"option":"None of these","correct":false}]},{"q":"<p>Which of the following streaming flume module method can be used to create an input stream that must be used with Spark Sink&nbsp;deployed on a Flume agent?</p>","a":[{"id":1182037,"option":"createDStream()","correct":false},{"id":1182038,"option":"createStream()","correct":false},{"id":1182039,"option":"\r\ncreatePollingStream()","correct":true},{"id":1182040,"option":"None of these","correct":false}]},{"q":"<p>Most of the Big Data applications spend a great time doing read-write operations such as performing DISK IO operations, data replication, and serialization. Assume that you are required to query data from five different systems in a distributed network, which of the following options will you select for faster computation?</p>","a":[{"id":1181977,"option":"Resilient Distributed Datasets (RDD)","correct":true},{"id":1181978,"option":"Distributed Files System (DFS)","correct":false},{"id":1181979,"option":"Hadoop Distributed File System (HDFS)","correct":false},{"id":1181980,"option":"None of these","correct":false}]},{"q":"<p>You want to read a '<strong>PS.txt</strong>' file from the Spark folder in the interactive session using PySpark Shell. Which of these code statements can be used to perform this task?</p>","a":[{"id":1181937,"option":"RDDread = sc.textFile (\"file:///opt/spark/PS.txt\")","correct":true},{"id":1181938,"option":"RDDread = sc.textFile (\"file:///spark/PS.txt\")","correct":false},{"id":1181939,"option":"RDDread = sc.File (\"file:///opt/spark/PS.txt\")","correct":false},{"id":1181940,"option":"RDDrd = sc.textFile (\"file:///opt/spark/PS.txt\")","correct":false}]},{"q":"<p>You have created a <strong>Creates a Chi-squared</strong> feature selector using the features library in PySpark 2.4.4. Which of these selection methods must be supported by such a selector?</p>","a":[{"id":1181825,"option":"numTopFeatures","correct":false},{"id":1181826,"option":"percentile","correct":false},{"id":1181827,"option":"fpr","correct":false},{"id":1181828,"option":"All of these","correct":true}]},{"q":"<p>You have used the explode function as shown alongside when working with Spark to implement MapReduce. Which of these deductions is valid with respect to the given context?</p>\n\n<pre class=\"prettyprint\"><code>wordCounts = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()</code></pre>\n\n<p> </p>","a":[{"id":1179409,"option":"The explode function transforms a Dataset of lines to a Dataset of words","correct":true},{"id":1179410,"option":"The explode function transforms a Dataset of words to a Dataset of lines","correct":false},{"id":1179411,"option":"The explode function transforms a Dataset of Frames to a Dataset of lines","correct":false},{"id":1179412,"option":"The explode function collects the word count in the textFile","correct":false}]},{"q":"Assume that you are working with a Spark RDD on a certain computation. If you expect the result of the computation to be a suitably large array, then, the usage of which of these methods should be avoided in the computation?<br><br>1. subtractByKey(other, numPartitions=None)<br>2.take(num)<br>3.top(num, key=None)","a":[{"id":1181243,"option":"Only 1","correct":false},{"id":1181244,"option":"Only 2","correct":false},{"id":1181245,"option":"Only 3","correct":false},{"id":1181246,"option":"Both 2,3","correct":true}]},{"q":"You want to use the groupBy().applyInPandas() available in a grouped map while working with PySpark. Which of these prerequisites would you need to meet while doing so?<br>1. A Python function that defines the computation for each group needs to be defined.<br>2.A StructType object or a string that defines the schema of the output PySpark DataFrame needs to be defined.","a":[{"id":1181231,"option":"Only 1","correct":false},{"id":1181232,"option":"Only 2","correct":false},{"id":1181233,"option":"Both 1,2","correct":true},{"id":1181234,"option":"None of these","correct":false}]},{"q":"You have created too many partitions in parallel on a large cluster when using the pyspark.sql.DataFrameReader(spark) interface.<br><br>What does Spark do in the given scenario?","a":[{"id":1179265,"option":"It might crash the external database systems.","correct":true},{"id":1179266,"option":"It used the minimum value of columns to decide the partition stride","correct":false},{"id":1179267,"option":"It uses the maximum value of columns to decide the partition stride","correct":false},{"id":1179268,"option":"A minimum number of partitions are created","correct":false}]},{"q":"<p>Analyze the UDF function used in pySpark given alongside. Which of the following statements regarding this code are valid:</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql import Window\n\ndf = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"\"id\"\", \"\"v\"\"))\n@pandas_udf(\"\"double\"\")\ndef mean_udf(v: pd.Series) -&gt; float:\n    return v.mean()\ndf.select(mean_udf(df['v'])).show()\ndf.groupby(\"\"id\"\").agg(mean_udf(df['v'])).show()\nw = Window \\\n    .partitionBy('id') \\\n    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\ndf.withColumn('mean_v', mean_udf(df['v']).over(w)).show()</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>This type of UDF does not support partial aggregation</li>\n\t<li>All data for a group or window will be loaded into memory by this code</li>\n\t<li>Only unbounded window is supported by this code</li>\n</ol>","a":[{"id":1182153,"option":"1","correct":false},{"id":1182154,"option":"2","correct":false},{"id":1182155,"option":"3","correct":false},{"id":1182156,"option":"All of these","correct":true}]},{"q":"<p>You are collecting neighboring vertices and their attributes at each vertex for a directed graph in PySpark. In the given context which of these operators can aid you to do so?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>collectNeighborIds</li>\n\t<li>collectNeighbors</li>\n</ol>","a":[{"id":1182145,"option":"1","correct":false},{"id":1182146,"option":"2","correct":false},{"id":1182147,"option":"Both of these","correct":true},{"id":1182148,"option":"None of these","correct":false}]},{"q":"<p>What is the output of the following PySpark code:</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>data = [LabeledPoint(0.0, [0.0]), LabeledPoint(1.0, [1.0]), LabeledPoint(1.0, [2.0]),LabeledPoint(1.0,[3.0])]\nsvm = SVMWithSGD.train(sc.parallelize(data), iterations=10)\nsvm.predict(sc.parallelize([[1.0]])).collect()\nsvm.clearThreshold()</code></pre>","a":[{"id":1180148,"option":"[1.44]","correct":false},{"id":1180149,"option":"[2]","correct":false},{"id":1180150,"option":"[1]","correct":true},{"id":1180151,"option":"[2.44]","correct":false}]},{"q":"<p>The method needConversion() can be used to avoid unnecessary conversions between which of these types in spark?</p>\n\n<p><strong>Options:</strong></p>\n\n<ol>\n\t<li>Array type</li>\n\t<li>Map type</li>\n\t<li>Struct type</li>\n</ol>","a":[{"id":1180056,"option":"1 and 2","correct":false},{"id":1180057,"option":"2 and 3","correct":false},{"id":1180058,"option":"1 and 3","correct":false},{"id":1180059,"option":"All of these","correct":true}]},{"q":"<p>You are using the Python package for statistical functions in MLlib and working with the static chiSqTest(observed, expected=None). What happens if the observed parameter is matrix?</p>","a":[{"id":1180116,"option":"Conduct Pearson’s chi-squared goodness of fit test of the observed data.","correct":false},{"id":1180117,"option":"Conduct Pearson’s independence test on the input contingency matrix.","correct":true},{"id":1180118,"option":"Conduct Pearson’s independence test for every feature against the label across the input RDD","correct":false},{"id":1180119,"option":"None of the Above","correct":false}]},{"q":"<p>What is the output of the following PySpark code:</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>df = spark.createDataFrame([(\"x y z\",)], [\"text\"])\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\ntokenizer.transform(df).head()</code></pre>","a":[{"id":1182013,"option":"Row(text='x y z', words=['xy','yz','zx'])","correct":true},{"id":1182014,"option":"Row(text='x y z', words=[ 'x',  'y',  'z'])","correct":false},{"id":1182015,"option":"Row(text='x y z', words=[\"x\", \"y\", \"z'\"])","correct":false},{"id":1182016,"option":"Given input text cannot be tokenized","correct":false}]},{"q":"<p>You are given a set of training data. You are required to do transformation and estimation on it. This step must be repeated for another separate part of the training data. Which of the following options is suitable to eliminate redundant execution of steps?</p>","a":[{"id":1182005,"option":"Create a pipeline with transformer and estimator","correct":true},{"id":1182006,"option":"Repeat transformer and estimatior for other bunch of training data","correct":false},{"id":1182007,"option":"Loop over the entire training bunch and do transformation and estimation","correct":false},{"id":1182008,"option":"Eliminate rest of the training data and do transformation and estimation for one training set ","correct":false}]},{"q":"<p>The following SQL Context is used to create a data frame that reads data from skills and people in the form of <strong>parquet</strong> files.&nbsp;Assume that you have performed multiple operations over this data frame. To get to a particular version of the dataset, which of the following options will you use?</p>\n\n<p><strong>Code:</strong></p>\n\n<pre class=\"prettyprint\"><code>people = sqlContext.read.parquet(\"people.parquet\")\nskills = sqlContext.read.parquet(\"skills.parquet\")</code></pre>","a":[{"id":1181985,"option":"history","correct":false},{"id":1181986,"option":"tempview","correct":false},{"id":1181987,"option":"checkpoint","correct":true},{"id":1181988,"option":"localcache","correct":false}]},{"q":"<p>John is working on External Datasets in PySpark RDDs. He used the batching on pickle serialization and he want to know the dafault batch size in this case.which of the following is the dafault batch size.</p>","a":[{"id":1179321,"option":"5","correct":false},{"id":1179322,"option":"20","correct":false},{"id":1179323,"option":"10","correct":true},{"id":1179324,"option":"15","correct":false}]},{"q":"<p>John is working on SparkUi in Pyspark. He started a streaming job and get to see a Streaming tab in the SparkUI of the attached cluster. Which of the following is true in the given context?</p>","a":[{"id":1179277,"option":"You can view the driver logs in this cluster","correct":false},{"id":1179278,"option":"No streaming job is running in this cluster","correct":false},{"id":1179279,"option":"A streaming job is running in this cluster","correct":true},{"id":1179280,"option":"You can view the driver logs and streaming jobs in this cluster ","correct":false}]},{"q":"<p>In PySpark, analyze these statements regarding the following method and select the appropriate choice.</p>\n\n<p><strong>Method</strong></p>\n\n<pre class=\"prettyprint\"><code>approxNearestNeighbors(dataset,key,numNearestNeighbors,distCol='distCol')</code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>If outputCol is missing, then the method transforms the data.</li>\n\t<li>This method allows caching of the transformed data when necessary.</li>\n</ol>","a":[{"id":1182097,"option":"1 is correct","correct":false},{"id":1182098,"option":"2 is correct","correct":false},{"id":1182099,"option":"Both 1 and 2 are correct and 1 is the correct explanation for 2","correct":true},{"id":1182100,"option":"Both 1 and 2 are correct and 1 is not the correct explanation for 2","correct":false}]},{"q":"<p>In PySpark, which of the following accurately depicts the number of buckets for a model fitted by the BucketedRandomProjectionLSH class&nbsp;where multiple random vectors are stored?</p>","a":[{"id":1182093,"option":"Maximum L2 norm of input vectors or bucketLength","correct":true},{"id":1182094,"option":"Maximum L1 norm of input vectors or bucketLength","correct":false},{"id":1182095,"option":"Minimum L2 norm of input vectors or bucketLength","correct":false},{"id":1182096,"option":"Minimum L1 norm of input vectors or bucketLength","correct":false}]},{"q":"<p>You are given the host address and the port of the Kafka service. Which of the following&nbsp;PySpark methods&nbsp;can be used to fetch the connection of the Kafka service?</p>","a":[{"id":1182025,"option":"pyspark.streaming.kafka.Brokers","correct":false},{"id":1182026,"option":"pyspark.streaming.kafka.Broker","correct":true},{"id":1182027,"option":"pyspark.streaming.kafka.getBroker","correct":false},{"id":1182028,"option":"None of the above","correct":false}]},{"q":"<p>Assume that you encounter the error shown alongside while working with PySpark. The issue can be considered indicative of which of these errors?</p>\n\n<p><strong>Error:</strong></p>\n\n<pre class=\"prettyprint\"><code>8/05/08 13:52:43 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\njava.io.InvalidClassException: org.apache.spark.rpc.RpcEndpointRef; local class incompatible: stream classdesc serialVersionUID = -1329125091869941550, local class serialVersionUID = 1835832137613908542</code></pre>\n\n<p>&nbsp;</p>","a":[{"id":1180096,"option":"A Spark version mismatch between components","correct":true},{"id":1180097,"option":"Incorrectly configured permission sets between nodes","correct":false},{"id":1180098,"option":"Incorrect class hierarchy in configuration","correct":false},{"id":1180099,"option":"None  of these","correct":false}]},{"q":"<p>In PySpark which of the following represents the list of available data types?</p>","a":[{"id":1179313,"option":"pyspark.sql.Window","correct":false},{"id":1179314,"option":"pysark.sql.functions","correct":false},{"id":1179315,"option":"pyspark.sql.types","correct":true},{"id":1179316,"option":"None of these","correct":false}]},{"q":"<p>You want to obtain a model which can transform categorical features to use 0-based indices. Which of these following in pySpark.ml library will you use to do so?</p>\n\n<ol>\n\t<li>\n\t<pre class=\"prettyprint\"><code>pyspark.ml.feature.VectorAssembler(inputCols=None, outputCol=None, handleInvalid='error')</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>pyspark.ml.feature.VectorIndexer(maxCategories=20,inputCol=None,outputCol=None,\nhandleInvalid='error')</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>pyspark.ml.feature.StringIndexer(maxCategories=20,inputCol=None,outputCol=None,\nhandleInvalid='error')</code></pre>\n\t</li>\n\t<li>\n\t<pre class=\"prettyprint\"><code>pyspark.ml.feature.RegexIndexer(maxCategories=20,inputCol=None,outputCol=None,\nhandleInvalid='error')</code></pre>\n\t</li>\n</ol>\n\n<p>&nbsp;</p>","a":[{"id":1182141,"option":"1","correct":false},{"id":1182142,"option":"2","correct":true},{"id":1182143,"option":"3","correct":false},{"id":1182144,"option":"4","correct":false}]},{"q":"<p>In PySpark, you are working on the PySpark API. If you are required to connect to a Spark Cluster, then which of the following conditions must be met in this scenario:</p>\n\n<ol>\n\t<li>Handle authentication</li>\n\t<li>Information specific to your cluster</li>\n\t<li>Information specific to all clusters</li>\n</ol>","a":[{"id":1181913,"option":"1","correct":false},{"id":1181914,"option":"2","correct":false},{"id":1181915,"option":"3","correct":false},{"id":1181916,"option":"1 and 2","correct":true}]},{"q":"<p>Which of the following statements is correct if you start <strong>Python Spark Shells</strong> without options?</p>","a":[{"id":1181873,"option":"It can kill the shell instance.","correct":true},{"id":1181874,"option":"It can kill SparkContext in the shell.","correct":false},{"id":1181875,"option":"It can kill the Spark master on local.","correct":false},{"id":1181876,"option":"It may kill the shell instance and the Spark master on local.","correct":false}]},{"q":"<p>You want to train a support vector machine based on certain data provided to you while working with PySpark 2.4.4. Which of these parameters will you be able to specify while training the machine:</p>\n\n<ol>\n\t<li>regParam</li>\n\t<li>miniBatchFraction</li>\n\t<li>step</li>\n</ol>","a":[{"id":1181853,"option":"1","correct":false},{"id":1181854,"option":"2","correct":false},{"id":1181855,"option":"3","correct":false},{"id":1181856,"option":"All of these","correct":true}]},{"q":"<p>You are using a method that is similar to the following method while working with streams in Pyspark 2.4.4. Which of these statements is correct:</p>\n\n<p><strong>Method</strong></p>\n\n<pre class=\"prettyprint\"><code>static createDirectStream(ssc, topics, kafkaParams, fromOffsets=None, keyDecoder=&lt;function utf8_decoder&gt;, valueDecoder=&lt;function utf8_decoder&gt;, messageHandler=None)</code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>The displayed method is not a receiver-based Kafka input stream.</li>\n\t<li>The ZooKeeper software is not used to store offsets in this scenario.</li>\n</ol>","a":[{"id":1181793,"option":"1","correct":false},{"id":1181794,"option":"2","correct":false},{"id":1181795,"option":"Both of these","correct":true},{"id":1181796,"option":"None of these","correct":false}]},{"q":"<p>You are using the DESCRIBE DATABASE statement to return the metadata of an existing database. Which of these information does the metadata contain in the given context?</p>","a":[{"id":1179329,"option":"Database name and Database location","correct":true},{"id":1179330,"option":"Database name and Database properties","correct":false},{"id":1179331,"option":"Database location and Database properties","correct":false},{"id":1179332,"option":"Database location","correct":false}]},{"q":"Analyze the snippet given alongside and select the valid risks to be considered while executing the same.","a":[{"id":1181235,"option":"potential OOM risk if data is skewed ","correct":true},{"id":1181236,"option":"potential ordering risk if data is redundant","correct":false},{"id":1181237,"option":"potential corruption risk if data is repeated","correct":false},{"id":1181238,"option":"All of these","correct":false}]},{"q":"Consider the Python code given alongside. What should be used in place of XXX such that output is as follows?<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>id   price<br>1     400","a":[{"id":1181223,"option":"   yield pdf[pdf.id == 1]","correct":true},{"id":1181224,"option":"   yield.pdf[pdf.id == 1]","correct":false},{"id":1181225,"option":"if (pdf.id == 1)","correct":false},{"id":1181226,"option":"   yield pdf[ id == 1]","correct":false}]},{"q":"Predict the output of the code snippet given alongside.","a":[{"id":1180132,"option":"[Row(name='Angela'), Row(name='Kerby'), Row(name=None]","correct":false},{"id":1180133,"option":"[ Row(name='Angela'), Row(name='Kerby')]","correct":false},{"id":1180134,"option":"[Row(name=None), Row(name='Angela'), Row(name='Kerby')]","correct":true},{"id":1180135,"option":"None of these","correct":false}]},{"q":"<p>You are using the Co-grouped map operations with Pandas instances to cogroup PySpark DataFrames with a common key. When doing so, you are applying a function to each cogroup. Which of the following is an optional input to the function?</p>","a":[{"id":1182205,"option":"A tuple representing the key","correct":true},{"id":1182206,"option":"pandas.DataFrame","correct":false},{"id":1182207,"option":"PySpark DataFrame","correct":false},{"id":1182208,"option":"StructType object","correct":false}]},{"q":"<p>You are given a dataset of 20M records of numeric data. You are required to perform the clustering algorithm. By default, the number of clusters is<strong> </strong>4. Which of the following clustering algorithms can be used to get the optimal output in less time?</p>","a":[{"id":1182021,"option":"K-means","correct":false},{"id":1182022,"option":"Bisecting K-means","correct":true},{"id":1182023,"option":"LDA","correct":false},{"id":1182024,"option":"Cannot be determined with provided information","correct":false}]},{"q":"<p>You receive the java.lang.NullPointerException&nbsp;error when spark attempts to save data to a postgres database. Which of these can be considered a valid issue behind this?</p>\n\n<p>&nbsp;</p>","a":[{"id":1180080,"option":"Mismatch between nodes and workers","correct":true},{"id":1180081,"option":"You forgot to set the driver","correct":false},{"id":1180082,"option":"Incorrectly configured Jar files","correct":false},{"id":1180083,"option":"Incorrect mount point used","correct":false}]},{"q":"<p>There are<strong> </strong>N PySpark jobs currently running&nbsp;and N files must be downloaded by this Spark job. If each job is assigned to download a file, then which of the following circumstances hinders the download of the file?</p>","a":[{"id":1182081,"option":"Recursive option is set to false","correct":false},{"id":1182082,"option":"File in HDFS is passed as a path ","correct":false},{"id":1182083,"option":"Wrong path specified","correct":true},{"id":1182084,"option":"All of these","correct":false}]},{"q":"<p>You have integrated PySpark with a scala object for interactions with HBase. Which of the following configuration parameters specifies the HBase Zookeeper quorum?</p>","a":[{"id":1182105,"option":"spark.hbase.host ","correct":true},{"id":1182106,"option":"spark.hbase.port","correct":false},{"id":1182107,"option":"spark.hbase.cluster.config","correct":false},{"id":1182108,"option":"spark.hbase.config","correct":false}]},{"q":"<p>Assume that you are aggregating the data and finding the mean of the data frame. Since you are connected to a remote system, you should not save your result as a file in the remote storage. You must find a way to save it in your local drive. Which of the options will you use to perform this task?</p>","a":[{"id":1181997,"option":"Use df.write() after exiting the remote system.","correct":false},{"id":1181998,"option":"Use df.writeStream() to store the data in buffer after exiting the remote system.","correct":false},{"id":1181999,"option":"Use df.writeStream() to store the data in buffer. After exiting the remote system use df.write() to save the buffer data.","correct":false},{"id":1182000,"option":"Cannot be determined with provided information","correct":true}]},{"q":"You want to create a Pandas UDF similar to PySpark’s aggregate functions. Which of the following type hints can be used for this purpose?","a":[{"id":1182193,"option":"pandas.Series, … -> Any","correct":true},{"id":1182194,"option":"Iterator[pandas.Series] -> Iterator[pandas.Series].","correct":false},{"id":1182195,"option":"pandas.Series, … -> Var","correct":false},{"id":1182196,"option":"pandas.Series, … -> All","correct":false}]}]