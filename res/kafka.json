[{"q":"<p>In Kafka, you have created the following topic and it has one partition.&nbsp;Which of these&nbsp;statements about this scenario are correct?</p>\n\n<p><strong>Topic:</strong></p>\n\n<pre class=\"prettyprint\"><code>&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-sample-topic\nTopic:my-sample-topic        PartitionCount:1        ReplicationFactor:3        Configs:\nTopic: my-sample-topic        Partition: 0      Leader: 1      Replicas: 1,2,0    Isr:1,2,0</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>Leader&nbsp;is the node that is responsible for all the reads and writes for the given partition, where each node is the leader for a randomly selected portion of the partitions.</li>\n\t<li>Replicas&nbsp;is the list of nodes that replicate the log for this partition irrespective of whether they are the leader or they are currently active.&nbsp;</li>\n\t<li>Isr&nbsp;is the set of in-sync&nbsp;replicas that represents a set of the replicas that are currently inactive.</li>\n</ol>","a":[{"id":1166763,"option":"1 and 2","correct":true},{"id":1166764,"option":"2 and 3","correct":false},{"id":1166765,"option":"1 and 3","correct":false},{"id":1166766,"option":"All of these","correct":false}]},{"q":"<p>In Apache Kafka, you are working on its broker configurations. If you are required to obtain the number of requests that can be queued up for processing by the I/O threads before the network threads stop reading in new requests, then which of the following properties are used to perform this action?</p>","a":[{"id":1420990,"option":"queued.requests","correct":false},{"id":1420991,"option":"queued.message.io.requests","correct":false},{"id":1420992,"option":"queued.min.requests","correct":false},{"id":1420993,"option":"queued.max.requests","correct":true}]},{"q":"<p>In Kafka, if the <strong>auto.leader.rebalance.enable</strong> property is enabled, then which of the following statements is correct:</p>\n\n<ol>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by periodically returning the leadership to the preferred replica for each partition, if available.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to any of the replicas that are available.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to the preferred replica for each partition irrespective of its availability.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to any of the replicas for each partition irrespective of its availability.</li>\n</ol>\n\n<p> </p>","a":[{"id":1166921,"option":"1","correct":true},{"id":1166922,"option":"2","correct":false},{"id":1166923,"option":"3","correct":false},{"id":1166924,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, you want to write data to an external system. You create a connection object in the Spark driver as displayed in the following code. You encounter an error. Which of these statements is a valid reason for this error:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>  dstream.foreachRDD(rdd =&gt; {\n      val connection = createNewConnection()  \n      rdd.foreach(record =&gt; {\n          connection.send(record) \n      })\n  })</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>The connection object must be parallelized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the worker to the driver. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>There is no error. The transferability of the connection object across machines is successful.</li>\n</ol>","a":[{"id":1167806,"option":"1","correct":false},{"id":1167807,"option":"2","correct":true},{"id":1167808,"option":"3","correct":false},{"id":1167809,"option":"4","correct":false}]},{"q":"<p>Consider that you are using the <strong>Azure Databricks</strong> interactive notebook to run the following code. If you want to send data to Slack to generate feedback from users through Slack, then which of these statements represents the task performed by the following code:</p>\n\n<p><strong>Code </strong></p>\n\n<pre class=\"prettyprint\"><code>val superSecretSlackToken = \"xoxb-...\" superSecretSlackToken: String = xoxb-... </code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>Defines the Slack Bot API token</li>\n\t<li>Defines the functions used by Slack</li>\n\t<li>Prepares the Kafka producer to send messages to Slack</li>\n\t<li>Initializes the Slack channel</li>\n</ol>","a":[{"id":1166933,"option":"1","correct":true},{"id":1166934,"option":"2","correct":false},{"id":1166935,"option":"3","correct":false},{"id":1166936,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, if you increase the number of topic-specific partitions in <strong>KafkaUtils.createStream()</strong>, then<strong> </strong>which of the following occurs:</p>\n\n<ol>\n\t<li>The number of threads using which the topics are consumed within multiple receivers increases.</li>\n\t<li>The number of threads using which the topics are consumed within a single receiver increases.</li>\n\t<li>The parallelism property of Spark does not increase while processing data.</li>\n\t<li>The parallelism property of Spark decreases significantly while processing data.</li>\n</ol>","a":[{"id":1166929,"option":"1 and 2","correct":false},{"id":1166930,"option":"2 and 3","correct":true},{"id":1166931,"option":"3 and 4","correct":false},{"id":1166932,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these tasks is performed by the following artifact for Scala/Java applications using SBT/Maven project definitions:</p>\n\n<pre class=\"prettyprint\"><code>NameID = org.apache.spark \nitemId = spark-streaming-kafka-0-8_2.11 \nv = 2.2.0 </code></pre>\n\n<p><strong>Tasks</strong> </p>\n\n<ol>\n\t<li>It removes your Kafka streaming application.</li>\n\t<li>It links your Kafka streaming application.</li>\n\t<li>It starts your Kafka streaming application.</li>\n</ol>","a":[{"id":1166783,"option":"1","correct":false},{"id":1166784,"option":"2","correct":true},{"id":1166785,"option":"3","correct":false},{"id":1166786,"option":"None of these","correct":false}]},{"q":"<p>In Kafka, you have created the following topic named <strong>sample</strong>. Which of these statements represent the properties of this topic correctly:</p>\n\n<pre class=\"prettyprint\"><code>&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sample </code></pre>\n\n<p><strong>Properties </strong></p>\n\n<ol>\n\t<li>It is a topic with a single partition and only one replica.</li>\n\t<li>It is a topic with multiple partitions and many replicas.</li>\n\t<li>It is a topic with a single partition and many replicas.</li>\n\t<li>It is a topic with multiple partitions and only one replica.</li>\n</ol>","a":[{"id":1166759,"option":"1","correct":true},{"id":1166760,"option":"2","correct":false},{"id":1166761,"option":"3","correct":false},{"id":1166762,"option":"4","correct":false}]},{"q":"<p>In Kafka, which of the following statements about the <strong>consumer </strong>API is correct:</p>\n\n<ol>\n\t<li>It allows applications to subscribe to one or more topics and process the produced stream of records.</li>\n\t<li>It allows applications to subscribe to topics only and process the produced stream of records.</li>\n\t<li>It allows applications to subscribe to two topics only and process the produced stream of records.</li>\n</ol>","a":[{"id":1166751,"option":"1","correct":true},{"id":1166752,"option":"2","correct":false},{"id":1166753,"option":"3","correct":false},{"id":1166754,"option":"None of these","correct":false}]},{"q":"<p>In Kafka, which of the following statements about <strong>topic</strong> are correct:</p>\n\n<ol>\n\t<li>It is a feed name to which records are published.</li>\n\t<li>They are not multi-subscriber. Each topic can have only one consumer that can subscribe to the data.</li>\n</ol>","a":[{"id":1166743,"option":"1","correct":true},{"id":1166744,"option":"2","correct":false},{"id":1166745,"option":"All of these","correct":false},{"id":1166746,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of the following statements about a <strong>rebalancing</strong> event is correct:</p>\n\n<ol>\n\t<li>It is a lifecycle event that occurs when consumers join or leave a consumer group.</li>\n\t<li>It is a lifecycle event that occurs when consumers are unable to enter a consumer group.</li>\n\t<li>It is a lifecycle event that occurs when a consumer deletes a consumer group.</li>\n</ol>","a":[{"id":1166747,"option":"1 ","correct":true},{"id":1166748,"option":"2 ","correct":false},{"id":1166749,"option":"3","correct":false},{"id":1166750,"option":"None of these","correct":false}]},{"q":"<p>Which of the following are core APIs of Kafka:</p>\n\n<ol>\n\t<li>Producer</li>\n\t<li>Consumer</li>\n\t<li>Connector</li>\n\t<li>Stream</li>\n</ol>","a":[{"id":1166739,"option":"1, 2, and 3","correct":false},{"id":1166740,"option":"2, 3, and 4","correct":false},{"id":1166741,"option":"1, 3, and 4","correct":false},{"id":1166742,"option":"All of these","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following artifact is correct:</p>\n\n<p><strong>Artifact </strong></p>\n\n<pre class=\"prettyprint\"><code>ConsumerStrategies.Subscribe </code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>It allows you to subscribe to a fixed collection of topics.</li>\n\t<li>It allows you to use a regex to specify topics of interest.</li>\n\t<li>It allows you to distribute partitions evenly across available executors.</li>\n</ol>","a":[{"id":1167600,"option":"1","correct":true},{"id":1167601,"option":"2","correct":false},{"id":1167602,"option":"3","correct":false},{"id":1167603,"option":"None of these","correct":false}]},{"q":"<p>In Spark streaming integrated with Kafka, which of the following artifacts is used to change this setting to handle more than (64 * number of executors) Kafka partitions?</p>","a":[{"id":1166925,"option":"spark.streaming.kafka.consumer.cache.maxCapacity","correct":true},{"id":1166926,"option":"spark-streaming-kafka-0-10_2.11","correct":false},{"id":1166927,"option":"LocationStrategies.PreferConsistent","correct":false},{"id":1166928,"option":"spark.streaming.kafka.consumer.cache.enabled","correct":false}]},{"q":"<p>In Kafka, the <strong>num.network.threads</strong> thread represents the number of network threads that the server uses for handling network requests. What is the default value of this thread?</p>","a":[{"id":1166767,"option":"1","correct":false},{"id":1166768,"option":"5","correct":false},{"id":1166769,"option":"7","correct":false},{"id":1166770,"option":"3","correct":true}]},{"q":"<p>You are required to set the following options if you want both batch and streaming queries for a Kafka source. Which statements about this scenario are correct:</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>assign</li>\n\t<li>subscribePattern</li>\n\t<li>subscribe</li>\n\t<li>kafka.bootstrap.servers</li>\n</ol>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>assign</strong>:<strong> </strong>Represents the specific TopicPartitions to consume</li>\n\t<li><strong>subscribePattern</strong>: Represents the topic list to subscribe</li>\n\t<li><strong>subscribe</strong>:<strong> </strong>Represents the list of subscribers </li>\n\t<li><strong>kafka.bootstrap.servers</strong>:<strong> </strong>Represents the pattern used to subscribe to topics</li>\n\t<li>All these options—assign, subscribe, and subscribePattern—can be specified for a Kafka source</li>\n\t<li>Only one of these options—assign, subscribe, or subscribePattern—can be specified for a Kafka source</li>\n</ol>","a":[{"id":1167987,"option":"1, 2, 4","correct":false},{"id":1167988,"option":"1, 4, 6","correct":true},{"id":1167989,"option":"3, 5, 6","correct":false},{"id":1167990,"option":"1, 3, 4, 6","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, your application uses the consumer group ID <strong>terran </strong>to read from the <strong>zerg.hydra </strong>Kafka topic, which contains 10 partitions. You have configured your application such that it can consume the topic with only one thread. However, during runtime, you increase the number of threads from 1 to 14.</p>\n\n<p>Which of the following statements about the consumption of the threads in this scenario is correct:</p>\n\n<ol>\n\t<li>All the 14 threads are consumed from a single partition—each consumed in two rebalancing events.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads stay idle.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads are consumed in the next rebalancing event.</li>\n\t<li>Once the rebalancing is complete, 7 of the 14 threads are consumed—each from a single partition—and the remaining 7 threads stay idle.</li>\n</ol>","a":[{"id":1167798,"option":"1","correct":false},{"id":1167799,"option":"2","correct":true},{"id":1167800,"option":"3","correct":false},{"id":1167801,"option":"4","correct":false}]},{"q":"<p>Which of the following are the advantages of Kafka:</p>\n\n<ol>\n\t<li>Tracks web activities by storing or sending events to real-time processes</li>\n\t<li>Resilient to node failures and it enables automatic recovery</li>\n\t<li>Transforms data of different formats into a standard format to avoid ambiguity</li>\n</ol>","a":[{"id":1166755,"option":"1 and 2","correct":false},{"id":1166756,"option":"2 and 3","correct":false},{"id":1166757,"option":"1 and 3","correct":false},{"id":1166758,"option":"All of these","correct":true}]},{"q":"<p>In Kafka, which of the following represent the two essential configurations of the <strong>consumer API</strong>:</p>\n\n<ol>\n\t<li>group.id</li>\n\t<li>segment.ms</li>\n\t<li>zookeeper.connect</li>\n\t<li>retention.bytes</li>\n</ol>","a":[{"id":1166775,"option":"1 and 2","correct":false},{"id":1166776,"option":"2 and 3","correct":false},{"id":1166777,"option":"1 and 3","correct":true},{"id":1166778,"option":"2 and 4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following code are correct:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>def function(): StreamingContext = {\n  val hack_ssc = new StreamingContext(...)   \n  val h_lines = hack_ssc.socketTextStream(...) \n  ...\n  hack_ssc.checkpoint(checkpointDirectory)   \n  hack_ssc\n}\nval hack_context = StreamingContext.getOrCreate(checkpointDirectory, function _)\nhack_context. ...\nhack_context.start()\nhack_context.awaitTermination()</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>If the program starts for the first time, the <strong>StreamingContext.getOrCreate()</strong> function allows you to create a new StreamingContext class, set up all the streams, and then call <strong>stop()</strong>.</li>\n\t<li>If the program restarts after failure, then the <strong>StreamingContext.getOrCreate()</strong> function allows you to replace a StreamingContext from the checkpoint data in the checkpoint directory.</li>\n\t<li>If the checkpointDirectory exists, then the context is replaced from the checkpoint data.</li>\n\t<li>If the checkpoint directory does not exist, that is, running for the first time, then the <strong>functionToCreateContext </strong>function is called to create a new context and disable the DStreams.</li>\n</ol>","a":[{"id":1167886,"option":"1, 2, 3","correct":false},{"id":1167887,"option":"2, 3, 4","correct":false},{"id":1167888,"option":"1, 3, 4","correct":false},{"id":1167889,"option":"None of these","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, how many input <strong>DStreams</strong> are created in the following code:</p>\n\n<pre class=\"prettyprint\"><code>val numInputDStreams = 5.5\nval kafkaDStreams = (2 to numInputDStreams).map { _ =&gt; KafkaUtils.createStream(...) }</code></pre>\n\n<p> </p>","a":[{"id":1166779,"option":"1","correct":false},{"id":1166780,"option":"3","correct":false},{"id":1166781,"option":"5","correct":false},{"id":1166782,"option":"None of these","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, RDD actions inside the DStream output operations force the processing of the data that is received. If you used the <strong>dstream.foreachRDD()</strong> artifact without any RDD actions inside them, then which of the following statements about the given scenario is correct:</p>\n\n<ol>\n\t<li>The system receives the data and discards it, and therefore, there is no execution.</li>\n\t<li>The system does not receive any data.</li>\n\t<li>The system receives and processes the data and the execution runs successfully.</li>\n\t<li>The system receives no data, and therefore, there is no execution.</li>\n</ol>","a":[{"id":1167810,"option":"1","correct":true},{"id":1167811,"option":"2","correct":false},{"id":1167812,"option":"3","correct":false},{"id":1167813,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements describe the task performed by the following code:</p>\n\n<pre class=\"prettyprint\"><code>createStream(StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map&lt;String,Object&gt; topics, StorageLevel storageLevel)</code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>createStream</strong> allows you to provide the input stream details, which include the systems where the port is created and the topic name.</li>\n\t<li>ssc represents the StreamingFile object.</li>\n\t<li>groupId represents the customer ID of this particular customer.</li>\n\t<li>Checkpoint operations are performed on a stream of data after it is received.</li>\n</ol>","a":[{"id":1167974,"option":"1 and 2","correct":false},{"id":1167975,"option":"2 and 3","correct":false},{"id":1167976,"option":"1 and 3","correct":true},{"id":1167977,"option":"All of these","correct":false}]},{"q":"<p>In Kafka, the load factor of a hash table is used in log cleaning. Which of the following is the default value of the load factor?</p>","a":[{"id":1166771,"option":"0.3","correct":false},{"id":1166772,"option":"0.5","correct":false},{"id":1166773,"option":"0.9","correct":true},{"id":1166774,"option":"0.8","correct":false}]}]