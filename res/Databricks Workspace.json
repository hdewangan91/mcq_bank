[{"q":"<p>In Databricks, you are working on the Delta Lake from the Databricks Workspace. Delta Lake automatically validates that the schema of the DataFrame being written is compatible with the schema of the table. Now, if you are required to determine whether a write from a DataFrame to a table is compatible by using the Delta Lake, then which of the following rules in this scenario are correct:<br>\n<strong>Rules:</strong></p>\n\n<ol>\n\t<li>All DataFrame columns must exist in the target table. If there are columns in the DataFrame not present in the table, an exception is raised.</li>\n\t<li>DataFrame column data types must not match the column data types in the target table. If they match, an exception is raised.</li>\n\t<li>DataFrame column names cannot differ only by case.</li>\n</ol>","a":[{"id":1544741,"option":"1 and 2","correct":false},{"id":1544742,"option":"2 and 3","correct":false},{"id":1544743,"option":"1 and 3","correct":true},{"id":1544744,"option":"All of these","correct":false}]},{"q":"<p>In Databricks, you are working on the Delta Engine from the Databricks Workspace. If you are required to optimize the performance by using caching, then which of the following statements about <strong>Delta Caching</strong> are correct:<br>\n<strong>Statements</strong></p>\n\n<ol>\n\t<li>The Delta cache contains local copies of remote data. It can improve the performance of a wide range of queries, and also can be used to store results of arbitrary subqueries.</li>\n\t<li>The data stored in the Delta cache can be read and operated on faster than the data in the Spark cache.</li>\n\t<li>When the Delta cache is enabled, data that has to be fetched from a remote source is automatically added to the cache.</li>\n\t<li>The Delta cache is stored entirely on the local disk, so that memory is not taken away from other operations within Spark.</li>\n</ol>","a":[{"id":1544733,"option":"1, 2, and 3","correct":false},{"id":1544734,"option":"2, 3, and 4","correct":true},{"id":1544735,"option":"1, 2, and 4","correct":false},{"id":1544736,"option":"1, 3, and 4","correct":false}]},{"q":"<p>In Databricks, you are working on the table operations in the Delta Lake from the Databricks Workspace. You are given a Spark data frame that contains new data for books with <strong>bookID</strong> from a table named <strong>library.</strong> Some of these books may already present on the <strong>library</strong> table.<br>\nNow, you are required to merge the new data into the <strong>library</strong> table. For this, you are required to perform the following two operations:</p>\n\n<ol>\n\t<li>If the <strong>bookID </strong>is already present, then update the matching rows with the respect to the new table named updates.</li>\n\t<li>If the <strong>bookID</strong> is not available, then insert as new rows.</li>\n</ol>\n\n<p>Which of the following SQL queries must be executed to perform these actions in this scenario:<br>\n<br>\n<strong>SQL queries:</strong><br>\n1.</p>\n\n<pre class=\"prettyprint\"><code>Insert BOOKS into spark.read.format(\"delta\").load(\"/mnt/delta/books\")</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>MERGE INTO library\nUSING updates\nON bookID\nWHEN MATCHED THEN\n    UPDATE SET\n    library.data = updates.data, spark.read.format\nWHEN NOT MATCHED\n    THEN INSERT (date, bookID, data) VALUES (date, bookID, data)</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>MERGE INTO library\nUSING updates\nON library.bookID = updates.bookID\nWHEN MATCHED THEN\n    UPDATE SET\n    library.data = updates.data\nWHEN NOT MATCHED\n    THEN INSERT (date, bookID, data) VALUES (date, bookID, data)</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>Insert BOOKS into spark.read.format(\"delta\").load(\"/mnt/delta/books\") UPDATE SET;</code></pre>\n\n<p> </p>","a":[{"id":1544721,"option":"1","correct":false},{"id":1544722,"option":"2","correct":false},{"id":1544723,"option":"3","correct":true},{"id":1544724,"option":"4","correct":false}]},{"q":"<p>In Databricks, you are required to perform the table batch reads and writes operations in Delta Lake from your Databricks Workspace. You have created a table named <strong>myhack_table</strong> that contains various information about the customers of your organization. If you are required to retrieve the number of customers that are added to the organization's database over the last year, then which of the following SQL queries can be used to perform this action in this scenario:<br>\n<br>\n<strong>SQL queries</strong><br>\n1.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), ON source.emailId))</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), ON source.emailId, 365))</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), 7))</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), 365))</code></pre>\n\n<p> </p>","a":[{"id":1544717,"option":"1","correct":false},{"id":1544718,"option":"2","correct":false},{"id":1544719,"option":"3","correct":false},{"id":1544720,"option":"4","correct":true}]},{"q":"<p>In Databricks, you are working on the Delta Engine from the Databricks Workspace. You are required to optimize the performance with file management by tuning the file size of the Delta tables. Now, which of the following statements about the properties that are implemented in this tuning process are correct:</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>The property <strong>delta.targetFileSize</strong> contains the boolean values.</li>\n\t<li>The property <strong>delta.tuneFileSizesForRewrites</strong> contains the values that have the size in bytes or higher units.</li>\n\t<li>The property <strong>delta.targetFileSize</strong> contains the values that have the size in bytes or higher units.</li>\n\t<li>The property <strong>delta.tuneFileSizesForRewrites</strong> contains the boolean values.</li>\n</ol>","a":[{"id":1544661,"option":"1 and 2","correct":false},{"id":1544662,"option":"2 and 3","correct":false},{"id":1544663,"option":"3 and 4","correct":true},{"id":1544664,"option":"1 and 4","correct":false}]},{"q":"<p>In Databricks. you are working on the Databricks Workspace. Which of the following statements about the Delta Lake statements are correct:<br>\n<strong>Statements</strong></p>\n\n<ol>\n\t<li>The statement <strong>REPAIR TABLE</strong> removes the file entries from the transaction log of a Delta table that can no longer be found in the underlying file system.</li>\n\t<li>The statement <strong>CACHE SELECT</strong> caches the data accessed by the specified simple SELECT query in the Delta cache.</li>\n\t<li>The statement <strong>DELETE FROM</strong> deletes the rows that match a predicate. When no predicate is provided, deletes all rows.</li>\n\t<li>The statement <strong>GENERATE</strong> generates the given mode (specified as a string) in a Delta table.</li>\n</ol>","a":[{"id":1544645,"option":"1, 2, and 3","correct":false},{"id":1544646,"option":"2, 3, and 4","correct":true},{"id":1544647,"option":"1, 2, and 4","correct":false},{"id":1544648,"option":"1, 3, and 4","correct":false}]},{"q":"<p>In Databricks, you are working on the Databricks Workspace. Which of the following allows you to query an older snapshot of a Delta table?</p>","a":[{"id":1544713,"option":"Delta Lake Snapshot scheduler","correct":false},{"id":1544714,"option":"Delta Lake Time Travel","correct":true},{"id":1544715,"option":"Delta Lake Version controllers","correct":false},{"id":1544716,"option":"None of these","correct":false}]},{"q":"<p>In Databricks, you are working on Databricks Workspace for accessing all of your Databricks assets. Now, which of the following folders of this Workspace represents a web-based interface that contain runnable commands, visualizations, and narrative text?</p>","a":[{"id":1544649,"option":"Clusters","correct":false},{"id":1544650,"option":"Notebooks","correct":true},{"id":1544651,"option":"Libraries","correct":false},{"id":1544652,"option":"Repos","correct":false}]},{"q":"<p>In Databricks, you are working on the data object previleges of the Table access control from the Databricks Workspace. You are implementing the Data Governance model. Now, which of the following securable objects of this model is used to control the access to the underlying filesystem such that the user that are granted with this securable object can bypass the restrictions put on the catalog, databases, tables, and views by reading from the filesystem directly?</p>","a":[{"id":1544693,"option":"CATALOG","correct":false},{"id":1544694,"option":"DATABASE","correct":false},{"id":1544695,"option":"VIEW","correct":false},{"id":1544696,"option":"ANY FILE","correct":true}]},{"q":"<p>In Databricks. you are working on the Delta Lake from Databricks Workspace. If you are required to load data from a file location into a Delta table, then which of the following SQL commands is used to perform this action in this scenario?</p>","a":[{"id":1544709,"option":"CREATE AS LOAD","correct":false},{"id":1544710,"option":"CONVERT INTO","correct":false},{"id":1544711,"option":"COPY INTO","correct":true},{"id":1544712,"option":"CREATE USING AS ","correct":false}]}]