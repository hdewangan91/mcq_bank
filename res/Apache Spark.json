[{"q":"<p>Ben wants to work on Spark SQL CLI to run the Hive metastore service in local mode and execute queries from the command line. Assume that the configuration of Hive is done by placing his XML files <strong>hive-site.xml</strong>, <strong>core-site.xml</strong>, and <strong>hdfs-site.xml </strong> in the <strong><code>conf/</code>.  </strong>directory. Now, if he started the Spark SQL CLI to perform these actions in this scenario, then which of the following Distributed query engine directory commands must have been implemented to run the Spark SQL CLI in this scenario:</p>\n\n<p><strong>Directories</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>./bin/spark-sql/start-thriftserver.sh</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>./bin/spark-sql/beeline</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>./bin/spark-sql</code></pre>\n\n<p> </p>","a":[{"id":1575049,"option":"1","correct":false},{"id":1575050,"option":"2","correct":false},{"id":1575051,"option":"3","correct":true},{"id":1575052,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming, which of these statements about the following function are correct:</p>\n\n<p><strong>Function</strong></p>\n\n<pre class=\"prettyprint\"><code>groupByKeyAndWindow(windowDuration, slideDuration, [numTasks])</code></pre>\n\n<p><strong>Statement</strong></p>\n\n<ol>\n\t<li>When it is called on a DStream of (K, V) pairs, it returns a new DStream of (K, Seq[V]) pairs.</li>\n\t<li>The values for each key in the existing DStream pair are aggregated using a normal function.</li>\n\t<li>By default, it uses Spark's default number of parallel tasks to perform the grouping.</li>\n\t<li>You can pass only one numTasks argument to set a different number of tasks</li>\n</ol>\n\n<p> </p>","a":[{"id":1166913,"option":"1 and 2","correct":false},{"id":1166914,"option":"1 and 3","correct":true},{"id":1166915,"option":"1, 2, and 3","correct":false},{"id":1166916,"option":"2, 3, and 4","correct":false}]},{"q":"<p>In Kafka, if the <strong>auto.leader.rebalance.enable</strong> property is enabled, then which of the following statements is correct:</p>\n\n<ol>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by periodically returning the leadership to the preferred replica for each partition, if available.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to any of the replicas that are available.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to the preferred replica for each partition irrespective of its availability.</li>\n\t<li>The controller automatically tries to balance the leadership for the partitions among the brokers by randomly returning the leadership to any of the replicas for each partition irrespective of its availability.</li>\n</ol>\n\n<p> </p>","a":[{"id":1166921,"option":"1","correct":true},{"id":1166922,"option":"2","correct":false},{"id":1166923,"option":"3","correct":false},{"id":1166924,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, you want to write data to an external system. You create a connection object in the Spark driver as displayed in the following code. You encounter an error. Which of these statements is a valid reason for this error:</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>  dstream.foreachRDD(rdd =&gt; {\n      val connection = createNewConnection()  \n      rdd.foreach(record =&gt; {\n          connection.send(record) \n      })\n  })</code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>The connection object must be parallelized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the driver to the worker. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>The connection object must be serialized and sent from the worker to the driver. This is not done, and therefore, the transferability of the connection object across machines fails.</li>\n\t<li>There is no error. The transferability of the connection object across machines is successful.</li>\n</ol>","a":[{"id":1167806,"option":"1","correct":false},{"id":1167807,"option":"2","correct":true},{"id":1167808,"option":"3","correct":false},{"id":1167809,"option":"4","correct":false}]},{"q":"<p>Consider that you are using the <strong>Azure Databricks</strong> interactive notebook to run the following code. If you want to send data to Slack to generate feedback from users through Slack, then which of these statements represents the task performed by the following code:</p>\n\n<p><strong>Code </strong></p>\n\n<pre class=\"prettyprint\"><code>val superSecretSlackToken = \"xoxb-...\" superSecretSlackToken: String = xoxb-... </code></pre>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>Defines the Slack Bot API token</li>\n\t<li>Defines the functions used by Slack</li>\n\t<li>Prepares the Kafka producer to send messages to Slack</li>\n\t<li>Initializes the Slack channel</li>\n</ol>","a":[{"id":1166933,"option":"1","correct":true},{"id":1166934,"option":"2","correct":false},{"id":1166935,"option":"3","correct":false},{"id":1166936,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, if you increase the number of topic-specific partitions in <strong>KafkaUtils.createStream()</strong>, then<strong> </strong>which of the following occurs:</p>\n\n<ol>\n\t<li>The number of threads using which the topics are consumed within multiple receivers increases.</li>\n\t<li>The number of threads using which the topics are consumed within a single receiver increases.</li>\n\t<li>The parallelism property of Spark does not increase while processing data.</li>\n\t<li>The parallelism property of Spark decreases significantly while processing data.</li>\n</ol>","a":[{"id":1166929,"option":"1 and 2","correct":false},{"id":1166930,"option":"2 and 3","correct":true},{"id":1166931,"option":"3 and 4","correct":false},{"id":1166932,"option":"1 and 4","correct":false}]},{"q":"<p>Which of the following statements about Spark Streaming are correct:</p>\n\n<ol>\n\t<li>It is an extension of the core Spark API.</li>\n\t<li>It enables high-throughput and fault-tolerant stream processing of live data streams.</li>\n</ol>","a":[{"id":1166719,"option":"1","correct":false},{"id":1166720,"option":"2","correct":false},{"id":1166721,"option":"All of these","correct":true},{"id":1166722,"option":"None of these","correct":false}]},{"q":"<p>In Spark Streaming, you want to retrieve data from Kafka which is not present in the core API. Which of the following artifacts must be added to the dependencies to retrieve the required data in this scenario?</p>","a":[{"id":1166731,"option":"spark-streaming-flume_2.12","correct":false},{"id":1166732,"option":"spark-streaming-kinesis-asl_2.12","correct":false},{"id":1166733,"option":"spark-streaming-kafka-0-10_2.12","correct":true},{"id":1166734,"option":"spark-streaming-kafka-spark-streaming","correct":false}]},{"q":"<p>Which of the following statements represent the advantages of Spark Streaming:</p>\n\n<ol>\n\t<li>Quick recovery from failures</li>\n\t<li>Better load balancing</li>\n\t<li>Better resource usage</li>\n\t<li>Native integration with advanced processing libraries such as SQL, machine learning, and graph processing</li>\n</ol>","a":[{"id":1166727,"option":"1 and 2","correct":false},{"id":1166728,"option":"2, 3, and 4","correct":false},{"id":1166729,"option":"1, 2, and 3","correct":false},{"id":1166730,"option":"All of these","correct":true}]},{"q":"<p>You are given the following steps. Which of these sequences represent the internal working of Spark Streaming correctly:</p>\n\n<p><strong>Steps </strong></p>\n\n<ol>\n\t<li>Receives live input data streams</li>\n\t<li>Spark engine generates the final stream of results in batches</li>\n\t<li>Batches are then processed by the Spark engine</li>\n\t<li>Divides the data into batches</li>\n</ol>","a":[{"id":1166723,"option":"1=>3=>2=>4","correct":false},{"id":1166724,"option":"1=>3=>2=>4","correct":false},{"id":1166725,"option":"1=>4=>3=>2","correct":true},{"id":1166726,"option":"1=>2=>3=>4","correct":false}]},{"q":"<p>Which of these transformations will you use to join the following dataset and windowed stream:</p>\n\n<pre class=\"prettyprint\"><code>val myDataExample : RDD[String, String] = ...\nval myWindowedStream = stream.window(Seconds(100))...</code></pre>\n\n<p> </p>","a":[{"id":790063,"option":"myWindowedStream.transform { rdd => rdd.join(myDataExample) }","correct":true},{"id":790064,"option":"myWindowedStream.action{ rdd => rdd.join(myDataExample) }","correct":false},{"id":790065,"option":"rdd.join(myWindowedStream.transform(myDataExample) ","correct":false},{"id":790066,"option":"rdd.join(myWindowedStream.transform { rdd => (myDataExample) }","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, which of these statements about the following artifact is correct:</p>\n\n<p><strong>Artifact </strong></p>\n\n<pre class=\"prettyprint\"><code>ConsumerStrategies.Subscribe </code></pre>\n\n<p><strong>Statements </strong></p>\n\n<ol>\n\t<li>It allows you to subscribe to a fixed collection of topics.</li>\n\t<li>It allows you to use a regex to specify topics of interest.</li>\n\t<li>It allows you to distribute partitions evenly across available executors.</li>\n</ol>","a":[{"id":1167600,"option":"1","correct":true},{"id":1167601,"option":"2","correct":false},{"id":1167602,"option":"3","correct":false},{"id":1167603,"option":"None of these","correct":false}]},{"q":"<p>You are calling the <strong>reduceByKey(func, [numTasks])</strong> property on a DStream of <strong>(K, V)</strong> pairs that return a new DStream of <strong>(K, V)</strong> pairs.<br>\nIt uses the default number of parallel tasks that are available in the Spark framework. Which of the following correctly represents the value if you are calling the property in the cluster mode?</p>","a":[{"id":789984,"option":"0","correct":false},{"id":789985,"option":"1","correct":false},{"id":789986,"option":"2","correct":false},{"id":789987,"option":"Determined by the config property spark.default.parallelism","correct":true}]},{"q":"<p>In Spark shell, a <strong>SparkContext</strong> gate is already created in a variable <strong>sc</strong>. Which of the following arguments will you use to set the <strong>master </strong>instance that can connect you to the created variable?</p>","a":[{"id":787841,"option":"--connect","correct":false},{"id":787842,"option":"--master","correct":true},{"id":787843,"option":"--context","correct":false},{"id":787844,"option":"--connectionContext","correct":false}]},{"q":"<p>In Spark, you are using the built-in DataFrames aggregation functions. You cannot perform the assigned task by using these built-in functions. If you are creating a custom untyped aggregate function, then which of the following abstract classes must be extended to implement this custom function?</p>","a":[{"id":787901,"option":"AggregateFunction","correct":false},{"id":787902,"option":"UserDefinedAggregateFunction","correct":true},{"id":787903,"option":"Aggregator","correct":false},{"id":787904,"option":"DataFrameUserDefined","correct":false}]},{"q":"<p>In Spark, you are required to store the persisted RDD by using different storage levels while working with RDD. You are using the <strong>cache()</strong> method for serving the required objective. Which of the following storage levels would have been used in the given scenario?</p>","a":[{"id":789226,"option":"StorageLevel.MEMORY_ONLY","correct":true},{"id":789227,"option":"StorageLevel.MEMORY_AND_DISK","correct":false},{"id":789228,"option":"StorageLevel.MEMORY_ONLY_SER ","correct":false},{"id":789229,"option":"StorageLevel.DISK_ONLY","correct":false}]},{"q":"<p>Your task is to save DataFrames as persistent tables in the Hive metastore. Which of these Spark commands will you use for this purpose so that the contents of the DataFrame can be materialized and a pointer to the data that are available in the Hive metastore are created during the process?</p>","a":[{"id":789230,"option":"createOrReplaceTempView ","correct":false},{"id":789231,"option":"saveAsTable ","correct":true},{"id":789232,"option":"saveAsView","correct":false},{"id":789233,"option":"createOrReplaceTempTable","correct":false}]},{"q":"<p>You are required to use the shared variables that are provided by the Spark framework. The tasks across multiple stages require the same data and data caching must be done in the deserialized format. Which of the following shared variables will you explicitly create in this scenario?</p>","a":[{"id":789242,"option":"Global","correct":false},{"id":789243,"option":"Broadcast","correct":true},{"id":789244,"option":"Accumulator","correct":false},{"id":789245,"option":"Aggregator","correct":false}]},{"q":"<p>You created a numeric accumulator by calling the <strong>SparkContext.longAccumulator()</strong> method to accumulate the values of the <strong>Long</strong> data type. The tasks that are running on a cluster are also added to the accumulator by using the <strong>add</strong> method. If the value method is used to read the accumulator's value, then which of the following statements is correct?</p>","a":[{"id":789246,"option":"Tasks can read the accumulator's value","correct":false},{"id":789247,"option":"Driver program cannot read the accumulator’s value","correct":false},{"id":789248,"option":"Driver program can read the accumulator’s value","correct":true},{"id":789249,"option":"Both the tasks and the driver program can read the accumulator's value","correct":false}]},{"q":"<p>In Spark, you are applying certain operations that are causing a <strong>shuffle</strong> event. The data for this shuffle event must be organized and aggregated. Which of the following statements is correct regarding the organization and aggregation of the data?</p>","a":[{"id":789612,"option":"Spark generates a set of map tasks for organization and a set of reduce tasks for aggregation","correct":true},{"id":789613,"option":"Spark generates a set of reduce tasks for organization and a set of map tasks for aggregation","correct":false},{"id":789614,"option":"Spark generates a set of map tasks for organization as well as for aggregation","correct":false},{"id":789615,"option":"Spark generates a set of reduce tasks for organization as well as for aggregation","correct":false}]},{"q":"<p>You are running a Spark Streaming program on a cluster. Before running the application, you came to know that the number of cores that are allocated to the application is less than the number of receivers. Which of the following conditions can occur if the application is running in the <strong>cluster</strong> mode?</p>","a":[{"id":789656,"option":"System receives data but it cannot process it","correct":true},{"id":789657,"option":"System receives data and process it","correct":false},{"id":789658,"option":"System cannot receive or process data","correct":false},{"id":789659,"option":"An exception is thrown","correct":false}]},{"q":"<p>You are applying an operation that is causing a shuffle event. Therefore, a lot of heap memory is getting consumed. While implementing this operation, you find that the data size is more than the provided memory. The Spark framework is spilling these tables to a disk. </p>\n\n<p>Which of the following conditions can occur in this scenario?</p>","a":[{"id":789933,"option":"Garbage collection decreases","correct":false},{"id":789934,"option":"Cause an overhead of network I/O","correct":false},{"id":789935,"option":"Cause an overhead of disk I/O","correct":true},{"id":789936,"option":"None of these","correct":false}]},{"q":"<p>To maintain an arbitrary state in a Spark streaming application, you are using the <strong>updateStateByKey</strong> operation. The state update function is applied to all existing keys in a batch by the Spark framework to complete this operation. Which of the following values is returned if the update function that returns the key-value pair gets eliminated?</p>","a":[{"id":790001,"option":"0","correct":false},{"id":790002,"option":"1","correct":false},{"id":790003,"option":"Null","correct":false},{"id":790004,"option":"None of these","correct":true}]},{"q":"<p>In Spark, you are using the table partitioning technique as an optimization approach. You noticed that the data types of the columns are automatically retrieved during partitioning. You configured the <strong>spark.sql.sources.partitionColumnTypeInference.enabled</strong> property and set it to false in the given scenario. Which of the following data types will you use to partition the columns once you disable the configuration?</p>","a":[{"id":790038,"option":"Numeric","correct":false},{"id":790039,"option":"String","correct":true},{"id":790040,"option":"Date","correct":false},{"id":790041,"option":"Timestamp","correct":false}]},{"q":"<p>In Spark, you are using the following properties to read data from a database by using the JDBC API:</p>\n\n<ul>\n\t<li>partitionColumn</li>\n\t<li>lowerBound</li>\n\t<li>upperBound</li>\n</ul>\n\n<p>Along with these properties, which of the following properties must be used to specify the technique through which the table is partitioned while it is being read from multiple workers?</p>","a":[{"id":790100,"option":"fetchPartitions ","correct":false},{"id":790101,"option":"numPartitions ","correct":true},{"id":790102,"option":"createPartitions ","correct":false},{"id":790103,"option":"cascadePartitions","correct":false}]},{"q":"<p>You are using the <strong>dataFrame.cache()</strong> method that is provided by the Spark SQL module to cache tables by using an in-memory columnar format. To minimize the memory usage and GC pressure, the Spark SQL module scans only the required columns and automatically tunes the compression process. Which of the following will you use to remove the table from the memory?</p>","a":[{"id":790112,"option":"spark.catalog.uncacheTable(\"tableName\")","correct":true},{"id":790113,"option":"spark.catalog.cacheTable(\"tableName\")","correct":false},{"id":790114,"option":"spark.sql.inMemoryColumnarStorage.compressed(\"tableName\")","correct":false},{"id":790115,"option":"spark.sql.inMemoryColumnarStorage.catchTable(\"tableName\")","correct":false}]},{"q":"<p>You are required to call the <strong>groupByKey([numPartitions])</strong> transformation on a dataset of <strong>(K, V)</strong> pairs to retrieve a dataset of <strong>(K, Iterable&lt;V&gt;)</strong> pairs. You are grouping them in order to perform an aggregation. The final result contains a level of parallelism in the Spark framework. Which of the following factors is the main component for these levels of parallelism?</p>","a":[{"id":790156,"option":"Number of partitions of the child RDDs","correct":false},{"id":790157,"option":"Number of partitions of the parent RDDs","correct":true},{"id":790158,"option":"Both of these","correct":false},{"id":790159,"option":"None of these","correct":false}]},{"q":"<p>You are required to configure the maximum size of a table. The size must be configured to broadcast the table to all worker nodes while performing a join operation. You are using the <strong>spark.sql.autoBroadcastJoinThreshold</strong> property of the Spark framework. To disable the broadcast of the table, which of the following values will you set the property to?</p>","a":[{"id":790176,"option":"0","correct":false},{"id":790177,"option":"1","correct":false},{"id":790178,"option":"-1","correct":true},{"id":790179,"option":"Null","correct":false}]},{"q":"<p>In order to maintain an arbitrary state in a Spark streaming application, you are using the <strong>updateStateByKey</strong> operation. To perform this operation, you must apply the <strong>update</strong> function to all existing keys in a batch by using the Spark framework.<br>\nWhich of the following conditions, if satisfied, allows you to apply this function to all the keys?</p>","a":[{"id":790192,"option":"Only if new data is available in a batch","correct":false},{"id":790193,"option":"Only if new data is not available in a batch","correct":false},{"id":790194,"option":"New data may or may not be available in a batch","correct":true},{"id":790195,"option":"If the function has yet not been exposed in the DStream API","correct":false}]},{"q":"<p>In Spark, you are unable to convert an RDD that contains case classes to a <strong>DataFrame</strong> collection. However, you decided to create the <strong>DataFrame</strong> programmatically. Which of the following methods will you use to apply the schema that you have created to the RDD of rows while performing this task?</p>","a":[{"id":787897,"option":"createDataFrame ","correct":true},{"id":787898,"option":"applyDataFrame","correct":false},{"id":787899,"option":"createRowS","correct":false},{"id":787900,"option":"collectDataFrame","correct":false}]},{"q":"<p>You have created a <strong>SparkSession</strong> class to access the functionalities of the Spark framework. Which of the following applications cannot create a data frame using the <strong>SparkSession</strong> class?</p>\n\n<p> </p>\n\n<p> </p>","a":[{"id":787885,"option":"Existing RDD","correct":false},{"id":787886,"option":"Hive table","correct":false},{"id":787887,"option":"Spark data source","correct":false},{"id":787888,"option":"XML file","correct":true}]},{"q":"<p>In Spark, which of the following methods is used to retrieve the first n elements of the dataset as an array?</p>","a":[{"id":787881,"option":"collect()","correct":false},{"id":787882,"option":"first()","correct":false},{"id":787883,"option":"take()","correct":true},{"id":787884,"option":"join()","correct":false}]},{"q":"<p>You are given a part of a transformation process whose some part is encoded as follows:</p>\n\n<pre class=\"prettyprint\"><code>XXXX(func)</code></pre>\n\n<p>Your task is to determine <strong>XXXX</strong> so that a new dataset is returned in such a way that it is created by selecting the elements of the source on which <strong>func</strong> returns true.</p>","a":[{"id":787877,"option":"map","correct":false},{"id":787878,"option":"filter","correct":true},{"id":787879,"option":"flatMap","correct":false},{"id":787880,"option":"sample","correct":false}]},{"q":"<p>You are performing an operation by using a distributed dataset in the Spark framework. Which of the following techniques will you use to aggregate the elements of the <strong>Resilient Distributed Datasets</strong> (RDD) data structure and then return the final result to the <strong>driver</strong> program?</p>","a":[{"id":787853,"option":"Map","correct":false},{"id":787854,"option":"Reduce","correct":true},{"id":787855,"option":"Persist","correct":false},{"id":787856,"option":"Replicate","correct":false}]},{"q":"<p>Your task is to perform some operations in the Spark framework by using the <strong>Resilient Distributed Datasets</strong> data structure. If you are required to retrieve the value after computation is performed on the dataset, then which of the following operations will you use to complete this task?</p>\n\n<p> </p>","a":[{"id":787849,"option":"Transformations","correct":false},{"id":787850,"option":"Actions","correct":true},{"id":787851,"option":"Computations","correct":false},{"id":787852,"option":"Mapping","correct":false}]},{"q":"<p>You are writing a Spark Streaming program. Which of the following objects will you require to create that serves as the entry point of all Spark Streaming functionality to initialize this program?</p>","a":[{"id":787837,"option":"Stream object","correct":false},{"id":787838,"option":"StreamingContext ","correct":true},{"id":787839,"option":"SparkContext","correct":false},{"id":787840,"option":"SparkConf","correct":false}]},{"q":"<p>You are converting a Hive metastore Parquet table to a Spark SQL Parquet table. For this you need to reconcile Hive metastore schema with Parquet schema.<br>\n<br>\nOnce the reconciliation is done, you observe that the reconciled field have the data type of the Parquet side.<br>\nWhich of the following statements will hold true in the given scenario?</p>","a":[{"id":790054,"option":"Field appearing in the Parquet schema are added as nullable field in the reconciled schema","correct":false},{"id":790055,"option":"Field appearing in the Hive metastore  schema are dropped in the reconciled schema","correct":false},{"id":790056,"option":"Reconciled schema contains exactly those fields defined in Parquet schema","correct":false},{"id":790057,"option":"Field appearing in the Hive metastore schema are added as nullable field in the reconciled schema","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, what does the following code allow you to do:</p>\n\n<pre class=\"prettyprint\"><code>Ranges = []\n\n def storeRanges(rdd):\n     global Ranges\n     Ranges = rdd.Ranges()\n     return rdd\n\n def printRanges(rdd):\n     for o in Ranges:\n         print \"%s %s %s %s\" % (o.topic, o.partition, o.fromOffset, o.untilOffset)\n\n directKafkaStream \\\n     .transform(storeRanges) \\\n     .foreachRDD(printRanges)</code></pre>\n\n<p><strong>Tasks</strong></p>\n\n<ol>\n\t<li>Access the Kafka offsets consumed in each batch.</li>\n\t<li>If you want Zookeeper-based Kafka monitoring tools to show the progress of the streaming application then it updates Zookeeper.</li>\n\t<li>Create multiple-input Kafka streams and merge them.</li>\n\t<li>Increase the number of topic-specific partitions.</li>\n</ol>","a":[{"id":1167960,"option":"1 and 2","correct":true},{"id":1167961,"option":"2 and 3","correct":false},{"id":1167962,"option":"3 and 4","correct":false},{"id":1167963,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark streaming integrated with Kafka, which of the following artifacts is used to change this setting to handle more than (64 * number of executors) Kafka partitions?</p>","a":[{"id":1166925,"option":"spark.streaming.kafka.consumer.cache.maxCapacity","correct":true},{"id":1166926,"option":"spark-streaming-kafka-0-10_2.11","correct":false},{"id":1166927,"option":"LocationStrategies.PreferConsistent","correct":false},{"id":1166928,"option":"spark.streaming.kafka.consumer.cache.enabled","correct":false}]},{"q":"<p>In Spark, you are using the <strong>collect()</strong> method to transfer an RDD to the driver node while working in the cluster mode. Your task is to print all the elements of that specific RDD. You observed that this task can cause the driver to run out of memory.</p>\n\n<p>Which of the following will you use to print only a few elements of the RDD?</p>","a":[{"id":787893,"option":"rdd.foreach(println) ","correct":false},{"id":787894,"option":"rdd.map(println)","correct":false},{"id":787895,"option":"rdd.take(100).foreach(println)","correct":true},{"id":787896,"option":"rdd.collect().foreach(println)","correct":false}]},{"q":"<p>In Spark, you are working on a file-based data source. The dataset APIs are used here. Which of the following techniques will you use to make the source capable of applying both the <strong>save</strong> and <strong>saveAsTable</strong> operation in this scenario?</p>","a":[{"id":789234,"option":"Bucketing ","correct":false},{"id":789235,"option":"Sorting","correct":false},{"id":789236,"option":"Partitioning","correct":true},{"id":789237,"option":"None of these","correct":false}]},{"q":"<p>You are working on a Spark Streaming application. You want to receive multiple streams of data in your application to satisfy a certain requirement. What will you do to receive such data in the application?</p>","a":[{"id":789539,"option":"Create multiple input DStreams ","correct":true},{"id":789540,"option":"Create multiple Receivers","correct":false},{"id":789541,"option":"Allocate the same number of cores as the number of streams that are available in the application","correct":false},{"id":789542,"option":"Not possible to receive multiple streams of data in parallel","correct":false}]},{"q":"<p>In Spark, you are required to store the persisted RDD by using different storage levels. You used the <strong>persist()</strong> method to implement this task. Which of the following conditions can occur if you use the <strong>MEMORY_ONLY</strong> storage level in this scenario?</p>","a":[{"id":789628,"option":"RDD is stored as deserialized objects in the memory","correct":true},{"id":789629,"option":"RDD is stored as serialized objects in the memory","correct":false},{"id":789630,"option":"RDD is stored on the off-heap memory","correct":false},{"id":789631,"option":"RDD partitions are stored on a disk","correct":false}]},{"q":"<p>You are running a Spark Streaming program locally. The tasks are implemented locally by using a single thread that might not allow the received data to get processed. Which of the master URLs will you use to run the program locally to resolve this issue?</p>","a":[{"id":789644,"option":"local","correct":false},{"id":789645,"option":"local[0]","correct":false},{"id":789646,"option":"local[1]","correct":false},{"id":789647,"option":"local[n]","correct":true}]},{"q":"<p>In Spark, the transform operation is used to apply an arbitrary RDD-to-RDD function on a DStream. Which of the following conditions must be satisfied in order to use it to apply an RDD operation?</p>","a":[{"id":790017,"option":"Operation must not have been exposed in the DStream API","correct":true},{"id":790018,"option":"Operation must have been exposed in the DStream API","correct":false},{"id":790019,"option":"Operation may or may not have been exposed in the DStream API","correct":false},{"id":790020,"option":"Can be used regardless of any condition","correct":false}]},{"q":"<p>In Spark, you are using the <strong>spark.sql.parquet.compression.codec</strong> property to set the compression codec that can be used while writing Parquet files. Assume that, you have specified the <strong>parquet.compression</strong> property in the table-specific options or properties before starting the <strong>write</strong> operation. Which of the following is the correct order of precedence in the provided scenario?</p>","a":[{"id":790092,"option":"Compression -> parquet.compression -> spark.sql.parquet.compression.codec","correct":true},{"id":790093,"option":"Parquet.compression -> compression -> spark.sql.parquet.compression.code","correct":false},{"id":790094,"option":"Compression -> spark.sql.parquet.compression.codec -> parquet.compression","correct":false},{"id":790095,"option":"Spark.sql.parquet.compression.codec -> parquet.compression -> compression","correct":false}]},{"q":"<p>Your task is to print all the elements of an RDD on the driver. Which of the following methods will you use to retrieve the RDD to the driver node so that the elements can be printed?</p>","a":[{"id":787873,"option":"bring()","correct":false},{"id":787874,"option":"take()","correct":false},{"id":787875,"option":"collect()","correct":true},{"id":787876,"option":"gather()","correct":false}]},{"q":"<p>You are required to set the following options if you want both batch and streaming queries for a Kafka source. Which statements about this scenario are correct:</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>assign</li>\n\t<li>subscribePattern</li>\n\t<li>subscribe</li>\n\t<li>kafka.bootstrap.servers</li>\n</ol>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li><strong>assign</strong>:<strong> </strong>Represents the specific TopicPartitions to consume</li>\n\t<li><strong>subscribePattern</strong>: Represents the topic list to subscribe</li>\n\t<li><strong>subscribe</strong>:<strong> </strong>Represents the list of subscribers </li>\n\t<li><strong>kafka.bootstrap.servers</strong>:<strong> </strong>Represents the pattern used to subscribe to topics</li>\n\t<li>All these options—assign, subscribe, and subscribePattern—can be specified for a Kafka source</li>\n\t<li>Only one of these options—assign, subscribe, or subscribePattern—can be specified for a Kafka source</li>\n</ol>","a":[{"id":1167987,"option":"1, 2, 4","correct":false},{"id":1167988,"option":"1, 4, 6","correct":true},{"id":1167989,"option":"3, 5, 6","correct":false},{"id":1167990,"option":"1, 3, 4, 6","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, your application uses the consumer group ID <strong>terran </strong>to read from the <strong>zerg.hydra </strong>Kafka topic, which contains 10 partitions. You have configured your application such that it can consume the topic with only one thread. However, during runtime, you increase the number of threads from 1 to 14.</p>\n\n<p>Which of the following statements about the consumption of the threads in this scenario is correct:</p>\n\n<ol>\n\t<li>All the 14 threads are consumed from a single partition—each consumed in two rebalancing events.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads stay idle.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads are consumed in the next rebalancing event.</li>\n\t<li>Once the rebalancing is complete, 7 of the 14 threads are consumed—each from a single partition—and the remaining 7 threads stay idle.</li>\n</ol>","a":[{"id":1167798,"option":"1","correct":false},{"id":1167799,"option":"2","correct":true},{"id":1167800,"option":"3","correct":false},{"id":1167801,"option":"4","correct":false}]},{"q":"<p>In Spark, you have been assigned the task of selecting the storage level that must be used for storing the persisted RDD. The process to select the storage level must be fast and fault recoverable in such a way that you do not have to wait for recomputing a lost partition to run tasks on RDD.<br>\n<br>\nWhich of the following storage levels will you select in the given scenario?</p>","a":[{"id":789543,"option":"MEMORY_ONLY","correct":false},{"id":789544,"option":"MEMORY_ONLY_SER","correct":false},{"id":789545,"option":"Replicated storage level","correct":true},{"id":789546,"option":"DISK_ONLY","correct":false}]},{"q":"<p>You are using the output operations on the <strong>DStream</strong> abstraction procedure. These output operations execute the DStreams slowly. If you are working on a Spark Streaming application, then which of the following conditions does not allow you to execute your code:</p>\n\n<ol>\n\t<li>Application does not contain output operations</li>\n\t<li>Application contains output operations without RDD actions in these operations</li>\n\t<li>Application contains output operations with RDD actions that are available in these operations</li>\n</ol>","a":[{"id":790075,"option":"1 and 2","correct":true},{"id":790076,"option":"2 and 3","correct":false},{"id":790077,"option":"1 and 3","correct":false},{"id":790078,"option":"1, 2, and 3","correct":false}]},{"q":"<p>The <strong>Parquet </strong>metadata is cached by the Spark SQL module to ensure better performance of the table. The Hive metastore Parquet table conversion method is enabled. Since the metadata of these converted tables is also cached, which of the following techniques will you use to ensure consistent metadata if Hive or any other external tool updates these tables?</p>","a":[{"id":790079,"option":"Reconcile Hive metastore schema with the Parquet schema","correct":false},{"id":790080,"option":"Set spark.sql.hive.convertMetastoreParquet to true","correct":false},{"id":790081,"option":"Refresh the tables manually","correct":true},{"id":790082,"option":"Tables are automatically updated","correct":false}]},{"q":"<p>In the Spark framework, the <strong>SparkSession</strong> class is the basic requirement for all the functionalities that are available in the framework. Which of the following methods will you use to create this class?</p>","a":[{"id":787889,"option":"SparkSession.builder()","correct":true},{"id":787890,"option":"SparkSession.create()","correct":false},{"id":787891,"option":"SparkSession.generator()","correct":false},{"id":787892,"option":"SparkSession.start()","correct":false}]},{"q":"<p>You are required to convert an existing RDD into a dataset. You must use a programming interface for performing this conversion. Which of the following conditions is suitable for the implementation of this method?</p>","a":[{"id":787865,"option":"When you already know the schema while writing your Spark application","correct":false},{"id":787866,"option":"When the columns and their types are not known until runtime","correct":true},{"id":787867,"option":"When you have to infer the schema of an RDD","correct":false},{"id":787868,"option":"None of these","correct":false}]},{"q":"<p>In Spark, you are performing operations that are capable of triggering the shuffle event. Which of the following techniques must be used such that it does not trigger the shuffling event?</p>","a":[{"id":787905,"option":"groupByKey","correct":false},{"id":787906,"option":"reduceByKey","correct":false},{"id":787907,"option":"cogroup","correct":false},{"id":787908,"option":"countByKey","correct":true}]},{"q":"<p>You are required to create a view that can be easily shared among all the sessions and can be kept alive until the termination of a Spark application. Which of the following views will you create in this scenario?</p>","a":[{"id":787861,"option":"Local","correct":false},{"id":787862,"option":"Temporary","correct":false},{"id":787863,"option":"Global","correct":false},{"id":787864,"option":"Global temporary","correct":true}]},{"q":"<p>You are using the Spark framework for fault-tolerant stream processing of live data streams. Which of the following types of abstractions is used to represent a continuous stream of data?</p>","a":[{"id":787833,"option":"KStream","correct":false},{"id":787834,"option":"CStream","correct":false},{"id":787835,"option":"DStream","correct":true},{"id":787836,"option":"ZStream","correct":false}]},{"q":"<p>Which of the following classes is required to initialize a Spark Streaming program?</p>","a":[{"id":1166735,"option":"StreamingContext  ","correct":true},{"id":1166736,"option":"socketTextStream","correct":false},{"id":1166737,"option":" DStream","correct":false},{"id":1166738,"option":"None of these","correct":false}]},{"q":"<p>You are working on a Spark Streaming application that you need to test with test data. To perform this task, you must create a DStream by using a queue of RDDs. Which of the following methods will you use to create the DStream for the required purpose?</p>","a":[{"id":789682,"option":"streamingContext.queueStream(queueOfRDDs)","correct":true},{"id":789683,"option":"streamingContext.createQueueStream(queueOfRDDs)","correct":false},{"id":789684,"option":"streamingContext.createStream(queueOfRDDs)","correct":false},{"id":789685,"option":"streamingContext.queueFileSystem(queueOfRDDs)","correct":false}]},{"q":"<p>In Spark, you are required to create a custom aggregate function because built-in functions cannot be used to fulfill your requirement. Which of the following types of aggregate functions you must create if you are extending the <strong>Aggregator</strong> class to implement the custom aggregate function?</p>","a":[{"id":789956,"option":"Type-Safe User-Defined Aggregate Functions","correct":true},{"id":789957,"option":"Untyped User-Defined Aggregate Functions","correct":false},{"id":789958,"option":"Both of these","correct":false},{"id":789959,"option":"None of these","correct":false}]},{"q":"<p>What is the output of the following Spark statement:</p>\n\n<pre class=\"prettyprint\"><code>SELECT sortedList(array('q', 'z', null, 'e', 'j'), true);</code></pre>\n\n<p> </p>","a":[{"id":787869,"option":"[null,\"e\",\"j\",\"q\",\"z\"]","correct":true},{"id":787870,"option":"[\"e\",\"j\",\"q\",\"z\",null]","correct":false},{"id":787871,"option":"[\"z\",\"q\",\"j\",\"e\",null]","correct":false},{"id":787872,"option":"[null,\"z\",\"q\",\"j\",\"e\"]","correct":false}]},{"q":"<p>You are required to perform RDD operations. These operations are distributed into multiple tasks by the Spark framework. Each of these tasks can be executed by the executor process. Which of the following is computed by the framework for each task before their execution?</p>","a":[{"id":787857,"option":"counter","correct":false},{"id":787858,"option":"controller","correct":false},{"id":787859,"option":"closure","correct":true},{"id":787860,"option":"Time taken for execution","correct":false}]},{"q":"<p>In the following Spark statement, which of these represents XXXX that creates a parallelized collection:</p>\n\n<pre class=\"prettyprint\"><code>val value = Array(10, 20, 30, 40, 50)\nval dataSet = XXXX(value)</code></pre>\n\n<p> </p>","a":[{"id":787845,"option":"sc.parallelize","correct":true},{"id":787846,"option":"SparkContext.parallelize","correct":false},{"id":787847,"option":"builder.parallelize","correct":false},{"id":787848,"option":"sc.builder.parallelize","correct":false}]},{"q":"<p>In Kafka, which of the following statements about the <strong>zookeeper.connect</strong> configuration are correct:</p>\n\n<ol>\n\t<li>It allows you to add a chroot path that makes all the Kafka data for this cluster appear under all the paths.</li>\n\t<li>It allows you to set up multiple Kafka clusters or other applications on the different ZooKeeper cluster at an instance of time. </li>\n\t<li>It specifies the ZooKeeper connection string in the port:hostname where hostname and port are the host and port for a node in the ZooKeeper clusterIn Kafka.</li>\n</ol>","a":[{"id":1166917,"option":"1","correct":false},{"id":1166918,"option":"2","correct":false},{"id":1166919,"option":"3","correct":false},{"id":1166920,"option":"None of these","correct":true}]},{"q":"<p>In Spark, you have applied an operation that can trigger a shuffle event. Therefore, a lot of heap memory is consumed because of the implementation of in-memory data structures. Which of the following can be used so that these data structures are generated on the reduce side?</p>","a":[{"id":787909,"option":"aggregateByKey ","correct":false},{"id":787910,"option":"reduceByKey ","correct":false},{"id":787911,"option":"collect","correct":false},{"id":787912,"option":"sortByKey","correct":true}]},{"q":"<p>In Spark, you are using some operations while working with RDDs that are only available on RDDs of key-value pairs. Assume that you are using custom objects as a key in these key-value pair operations. Which of the following two methods must be used together in this scenario?</p>","a":[{"id":790132,"option":"Equals() and hashcode()","correct":true},{"id":790133,"option":"Collcet() and take()","correct":false},{"id":790134,"option":"Map() and filter()","correct":false},{"id":790135,"option":"Distinct() and reduceByKey()","correct":false}]}]