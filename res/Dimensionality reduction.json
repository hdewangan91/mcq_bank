[{"q":"<p>In Machine Learning, you are working on a project and want to use dimensionality reduction. Your data does not lie on a linear subspace. In this scenario, which of the following learning methods will you select for dimensionality reduction?</p>","a":[{"id":1719897,"option":"Principal Component Analysis","correct":false},{"id":1719898,"option":"Multi-dimensional scaling","correct":true},{"id":1719899,"option":"Linear Discriminant Analysis","correct":false},{"id":1719900,"option":"None of these","correct":false}]},{"q":"<p>In Machine Learning, you are working on&nbsp;dimensionality reduction of your data and want to use feature selection. You came around the variance threshold as it drops all features where the variance along the column does not exceed a threshold value. What is the output of the following code?</p>\n\n<p><strong>Code</strong></p>\n\n<pre class=\"prettyprint\"><code>from sklearn.feature_selection import VarianceThreshold\n\nX = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n\nselector = VarianceThreshold()\nselector.fit_transform(X)</code></pre>\n\n<p>&nbsp;</p>","a":[{"id":1719861,"option":"[[2, 0], [1, 4], [1, 1]]","correct":true},{"id":1719862,"option":"[[0,2], [0,1], [0,1]]","correct":false},{"id":1719863,"option":"[[0,3], [4,3], [1,3]]","correct":false},{"id":1719864,"option":"None of these","correct":false}]},{"q":"<p>In Machine Learning, you are working on a project with a large-scale dimension of 5000 rows and 800 columns. 50% of the rows and columns contain missing values. In this scenario, which of the following option will you not select?</p>","a":[{"id":1719857,"option":"Insert missing data using nearest neighbours","correct":false},{"id":1719858,"option":"Insert missing data using median values","correct":false},{"id":1719859,"option":"Clean up the data by removing rows with missing values","correct":false},{"id":1719860,"option":"Add extra features as indicators for the missing values","correct":true}]},{"q":"<p>In Machine Learning, you are working on a project. If you have a large-scale dataset and you want to reduce the dimension of the data set, then which of the following options will you select?</p>","a":[{"id":1720117,"option":"Remove columns which have high variance in data ","correct":false},{"id":1720118,"option":"Remove columns which have 60% missing values","correct":true},{"id":1720119,"option":"Remove columns with dissimilar data trends","correct":false},{"id":1720120,"option":"None of these","correct":false}]},{"q":"<p>In Machine Learning, you are working on a large-scale data set with hundreds of features. Which of the following algorithm will you select to use dimensionality reduction for your dense&nbsp;data set, a data set with few&nbsp;zero values?</p>","a":[{"id":1719849,"option":"Principal Component Analysis","correct":true},{"id":1719850,"option":"Singular Value Decomposition","correct":false},{"id":1719851,"option":"Density-Based Spatial Clustering of Applications with Noise","correct":false},{"id":1719852,"option":"None of these","correct":false}]},{"q":"<p>In Machine Learning, you are working on a project with a large-scale dataset with multiple features. If you want to use a dimensionality reduction algorithm known as principal component analysis (PCA), then which of the following statements about PCA are correct:</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>It performs mean normalization.</li>\n\t<li>It creates an embedding of the dataset and attempts to preserve the relationships in the dataset.</li>\n\t<li>It uses covariance matrix and eigenvectors for its implementation.</li>\n\t<li>Data standardization is not required before implementation.</li>\n\t<li>Every principal component is always orthogonal.</li>\n</ol>","a":[{"id":1719841,"option":"1, 2, and 4","correct":false},{"id":1719842,"option":"1, 3, and 5","correct":true},{"id":1719843,"option":"2, 3, and 4","correct":false},{"id":1719844,"option":"All of these","correct":false}]},{"q":"<p>In Machine Learning, you are working on a large-scale data set with hundreds of features. Which of the following algorithms will you use to access dimensionality reduction for your sparse data set, a data set with many zero values?</p>\n\n<p>&nbsp;</p>","a":[{"id":1719845,"option":"Principal Component Analysis","correct":false},{"id":1719846,"option":"Singular Value Decomposition","correct":true},{"id":1719847,"option":"Density-Based Spatial Clustering of Applications with Noise","correct":false},{"id":1719848,"option":"None of these","correct":false}]},{"q":"<p>In Machine Learning, you are working on a project and want to use dimensionality reduction. While implementing you came around the term, <em>Curse of dimensionality</em>. In this scenario, which of the following statements about the curse of dimensionality are correct:</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>It is a&nbsp;problem caused by the exponential increase in the number of features.</li>\n\t<li>It occurs when the complexity increases rapidly, caused by the increasing number of possible combinations of inputs.</li>\n\t<li>It has a higher possibility of noise in the data.</li>\n\t<li>It has a lower possibility of data&nbsp;redundancy in the data.</li>\n</ol>","a":[{"id":1719853,"option":"1, 2, and 3","correct":true},{"id":1719854,"option":"2, 3, and 4","correct":false},{"id":1719855,"option":"1, 3, and 4","correct":false},{"id":1719856,"option":"All of these","correct":false}]}]