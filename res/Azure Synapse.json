[{"q":"<p>How can you build no-ETL HTAP solutions using Azure Synapse Link when working with column-oriented Azure Cosmos DB analytical store?</p>","a":[{"id":1688877,"option":"By directly linking to Azure Cosmos DB analytical store from Azure Synapse Analytics","correct":true},{"id":1688878,"option":"Create aggregations and sequential scans of selected fields","correct":false},{"id":1688879,"option":"Both 1 and 2","correct":false},{"id":1688880,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Suppose you want to copy a subset of blobs under a folder when working with Azure Synapse Analytics. Which of the following mode of action do you need to follow in order to do so?</p>","a":[{"id":1688801,"option":"Specify folderPath for the folder part only","correct":false},{"id":1688802,"option":"Specify folderPath for the folder part and fileName for the file name","correct":false},{"id":1688803,"option":"Specify folderPath for the folder part and fileName with a wildcard filter","correct":true},{"id":1688804,"option":"None of these","correct":false}]},{"q":"<p>You do not require every pipeline execution of your data flow activities in Azure Synapse Analytics to fully log all verbose telemetry logs. Which of the following logging levels should you set in order to accomplish the above requirement?</p>","a":[{"id":1688797,"option":"Basic","correct":false},{"id":1688798,"option":"Log","correct":false},{"id":1688799,"option":"None","correct":false},{"id":1688800,"option":"Either 1 or 3","correct":true}]},{"q":"<p>You run the query given alongside in Azure Synapse to check if a query was executed with a result cache hit or miss.</p>\n\n<p><strong>Query</strong></p>\n\n<pre class=\"prettyprint\"><code>SELECT request_id, command, result_cache_hit FROM sys.dm_pdw_exec_requests\nWHERE request_id = &lt;'Your_Query_Request_ID'&gt;</code></pre>\n\n<p>What value will be returned by result_cache_hit column if result set caching was not used?</p>","a":[{"id":1688793,"option":"<0","correct":true},{"id":1688794,"option":"0","correct":false},{"id":1688795,"option":"1","correct":false},{"id":1688796,"option":">1","correct":false}]},{"q":"<p>You are given that result set caching is turned ON for a database in an Azure Synapse dedicated pool. In the given scenario, for which of these queries are the results not cached until the cache is full:</p>\n\n<p><strong>Queries</strong></p>\n\n<ol>\n\t<li>Queries returning large result sets</li>\n\t<li>Queries with built-in functions</li>\n\t<li>Queries using user-defined functions</li>\n\t<li>Queries using tables with either row-level security or column level security enabled</li>\n</ol>","a":[{"id":1688789,"option":"Only 1, 2 and 3","correct":false},{"id":1688790,"option":"Only 2, 3 and 4","correct":false},{"id":1688791,"option":"Only 1, 3 and 4","correct":false},{"id":1688792,"option":"All 1, 2, 3 and 4","correct":true}]},{"q":"<p>You have created an ordered CCI (Clustered columnstore index) on a large table in an Azure Synapse dedicated pool. However, you want to reduce segment overlapping during the creation. Which of the following options will be suitable in the given scenario?</p>","a":[{"id":1688785,"option":"By using xlargerc resource class on a lower DWU","correct":false},{"id":1688786,"option":"By creating ordered CCI with MAXDOP set to 0","correct":false},{"id":1688787,"option":"By creating ordered CCI with MAXDOP set to 1","correct":true},{"id":1688788,"option":"By creating ordered CCI with MAXDOP set to -1","correct":false}]},{"q":"<p>You want to load data directly to a dedicated SQL pool in Azure Synapse Analytics without going through Azure Blob storage. Which of the following loading options can be used to achieve this?</p>","a":[{"id":1688781,"option":"bcp","correct":true},{"id":1688782,"option":"SqlBulkCopy API","correct":false},{"id":1688783,"option":"COPY statement ","correct":false},{"id":1688784,"option":"PolyBase with SSIS ","correct":false}]},{"q":"<p>You are working with Data Warehouse Units in Synapse SQL. You are given a task to scan a large number of rows using a standard data warehousing query and later perform a complex aggregation. What will be the nature of the performance of the respective operation?</p>","a":[{"id":1688777,"option":"I/O intensive","correct":false},{"id":1688778,"option":"CPU intensive","correct":false},{"id":1688779,"option":"Network intensive","correct":false},{"id":1688780,"option":"Both 1 and 2","correct":true}]},{"q":"<p>Which of the following join types can used for non-equi joins in Azure Synapse Analytics?</p>","a":[{"id":1688773,"option":"Left Outer","correct":false},{"id":1688774,"option":"Right Outer","correct":false},{"id":1688775,"option":"Inner Join","correct":false},{"id":1688776,"option":"Custom cross join","correct":true}]},{"q":"<p>You get the error message: “Error 403” when working with a copy activity in Azure Synapse Analytics. What could be a valid cause for such an error to occur?</p>","a":[{"id":1688769,"option":"Invalid Python file URI","correct":false},{"id":1688770,"option":"Databricks cluster does not exist ","correct":false},{"id":1688771,"option":"Databricks cluster has been deleted","correct":false},{"id":1688772,"option":"The Databricks access token has expired  ","correct":true}]},{"q":"<p>Which of the following operators in Azure Synapse Analytics can be used to access a specific element when dealing with columns or functions that return array types?</p>","a":[{"id":1688765,"option":"( )","correct":false},{"id":1688766,"option":"{ }","correct":false},{"id":1688767,"option":"[ ]","correct":true},{"id":1688768,"option":"| |","correct":false}]},{"q":"<p>Which of the following URLs should you choose to use serverless SQL pool in Azure Synapse:</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;-sql.transact.azuresynapse.net</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;-transact.sql.azuresynapse.net</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;.sql.azuresynapse.net</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;-ondemand.sql.azuresynapse.net</code></pre>\n\n<p>&nbsp;</p>","a":[{"id":1688757,"option":"1","correct":false},{"id":1688758,"option":"2","correct":false},{"id":1688759,"option":"3","correct":false},{"id":1688760,"option":"4","correct":true}]},{"q":"<p>Which of the following is true about Synapse workspace role assignments?</p>\n\n<ol>\n\t<li>If a principal is assigned the same role at different scopes, you'll see multiple assignments for the principal.</li>\n\t<li>If a role is assigned to a security group, you'll see the roles explicitly assigned to the group but not roles inherited from parent groups.</li>\n</ol>","a":[{"id":1687117,"option":"Only 1","correct":false},{"id":1687118,"option":"Only 2","correct":false},{"id":1687119,"option":"Both 1 and 2","correct":true},{"id":1687120,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>A guest user from a different Azure directory tenant is assigned the Synapse Administrator role.<br>\nWhich of these deductions can be made in the given context?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Guest users cannot see or manage role assignments</li>\n\t<li>Guest users can see or manage role assignments</li>\n\t<li>Guest users can list Synapse RBAC role assignments for all scopes</li>\n\t<li>Guest users can include assignments for objects they don't have access to</li>\n</ol>","a":[{"id":1687113,"option":"1","correct":true},{"id":1687114,"option":"2","correct":false},{"id":1687115,"option":"3","correct":false},{"id":1687116,"option":"4","correct":false}]},{"q":"<p>You are trying to connect to Azure Synapse workspace resources from a restricted network. To achieve this, you are creating private endpoints for your workspace resource.<br>\nWhat should you do in the given scenario to access the resources inside your Azure Synapse Analytics Studio workspace resource?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Create at least one Private link endpoint with a SQL type </li>\n\t<li>Create at least one Private link endpoint with a Workspace type </li>\n\t<li>Create at least one Private link endpoint with a SqlOnDemand type </li>\n\t<li>Create atleast one Private link endpoint with a Target sub-resource type</li>\n</ol>","a":[{"id":1687109,"option":"1","correct":false},{"id":1687110,"option":"2","correct":false},{"id":1687111,"option":"3","correct":false},{"id":1687112,"option":"4","correct":true}]},{"q":"<p>When trying to expand the storage structure in Azure Synapse Studio by selecting the arrow to Data -&gt; Linked you get the following error message in the left panel of the linked storage node.</p>\n\n<p>“REQUEST_SEND_ERROR: Failed to send the request to storage server”<br>\nWhat might be the possible reason behind this?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>The container resource you are accessing either has been deleted or does not exist</li>\n\t<li>The workspace tenant that you used to log in is not the same with the tenant of the storage account</li>\n\t<li>The storage resource is not behind a virtual network but the Blob service endpoint is not accessible due to firewall configured</li>\n\t<li>The storage resource being accessed is Azure Data Lake Storage Gen2 and is behind a firewall and a virtual network at the same time</li>\n</ol>","a":[{"id":1687105,"option":"1","correct":false},{"id":1687106,"option":"2","correct":false},{"id":1687107,"option":"3","correct":true},{"id":1687108,"option":"4","correct":false}]},{"q":"<p>You want to monitor pipelines, triggers, and integration runtimes in your Synapse workspace using the Monitor Hub. Under which of the following tabs in the Monitor Hub can this be achieved?</p>","a":[{"id":1687057,"option":"Under Integration","correct":true},{"id":1687058,"option":"Under Activities","correct":false},{"id":1687059,"option":"Under Runtime","correct":false},{"id":1687060,"option":"Under Requests","correct":false}]},{"q":"<p>You are creating a pipeline containing a copy activity that ingests data from Azure SQL Database into a dedicated Azure Synapse SQL pools. When doing so, you decide to use a previously created Azure Data Lake Storage Gen2 linked service to stage data before it loads into Azure Synapse Analytics by using PolyBase.<br>\nWhat happens to the interim data in Azure Data Lake Storage Gen2 after the copy is completed?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>It is automatically cleaned up.</li>\n\t<li>It is automatically validated</li>\n\t<li>It is stored in a new container</li>\n\t<li>It is stored in a new table</li>\n</ol>","a":[{"id":1687053,"option":"1","correct":true},{"id":1687054,"option":"2","correct":false},{"id":1687055,"option":"3","correct":false},{"id":1687056,"option":"4","correct":false}]},{"q":"<p>You are trying to interact with Azure Cosmos DB using Apache Spark in Azure Synapse Link. Which of the following are valid ways to query the Azure Cosmos DB analytical store from Spark when doing so?</p>\n\n<ol>\n\t<li>Load to Spark DataFrame</li>\n\t<li>Create Spark table</li>\n\t<li>Create a full-fidelity schema</li>\n</ol>","a":[{"id":1687049,"option":"1","correct":false},{"id":1687050,"option":"2","correct":false},{"id":1687051,"option":"3","correct":false},{"id":1687052,"option":"Both 1 and 2","correct":true}]},{"q":"<p>You want to build a serverless SQL pool database and views over Synapse Link for Azure Cosmos DB. Which of these are valid prerequisites to achieve this?</p>\n\n<ol>\n\t<li>Create a Synapse workspace named SynapseLinkBI.</li>\n\t<li>Enable Azure Synapse Link for your Azure Cosmos account</li>\n\t<li>Connect the Azure Cosmos database to the Synapse workspace.</li>\n</ol>","a":[{"id":1687045,"option":"Only 1 and 2","correct":false},{"id":1687046,"option":"Only 2 and 3","correct":false},{"id":1687047,"option":"Only 1 and 3","correct":false},{"id":1687048,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>You are trying to access an Azure Cosmos DB database from Azure Synapse Analytics studio. When doing so, you want to run large-scale analytics into Azure Cosmos DB without impacting your operational performance.<br>\nWhat should you do to achieve your objective?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Use a linked service to connect to an Azure Cosmos DB database</li>\n\t<li>Enable Synapse Link for Azure Cosmos DB</li>\n\t<li>Enable HTAP capability</li>\n</ol>","a":[{"id":1687041,"option":"1","correct":false},{"id":1687042,"option":"2","correct":true},{"id":1687043,"option":"3","correct":false},{"id":1687044,"option":"Either 1 or 2","correct":false}]},{"q":"<p>Assume that you have created serverless Apache Spark pools in your Synapse workspace to user Spark analytics. In the given scenario, which of these are valid ways to use Spark within Azure Synapse?</p>\n\n<ol>\n\t<li>Using Spark Notebooks</li>\n\t<li>Using Spark job definitions</li>\n</ol>","a":[{"id":1687037,"option":"Only 1","correct":false},{"id":1687038,"option":"Only 2","correct":false},{"id":1687039,"option":"Both 1 and 2","correct":true},{"id":1687040,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Which of the following can be achieved by using Azure Synapse Link for Azure Cosmos DB?</p>\n\n<ol>\n\t<li>You can run near real-time analytics over operational data in Azure Cosmos DB</li>\n\t<li>You can create fully isolated column store.</li>\n\t<li>You can run near real-time business intelligence pipelines</li>\n</ol>","a":[{"id":1687033,"option":"1","correct":true},{"id":1687034,"option":"2","correct":false},{"id":1687035,"option":"3","correct":false},{"id":1687036,"option":"None of these","correct":false}]},{"q":"<p>You are loading data using dedicated SQL pools in Azure Synapse Analytics. In the given scenario, what should you do to ensure that the loading speed is the fastest?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Running a single load job at a time</li>\n\t<li>Running multiple load jobs in parallel</li>\n\t<li>For larger jobs, scaling down the SQL pool before the load operation</li>\n\t<li>For smaller jobs, scaling up the SQL pool before the load operation</li>\n</ol>","a":[{"id":1687029,"option":"1","correct":true},{"id":1687030,"option":"2","correct":false},{"id":1687031,"option":"3","correct":false},{"id":1687032,"option":"4","correct":false}]},{"q":"<p>Which of the following wait types for dedicated SQL pools in Azure Synapse Analytics has the maximum value as 1?</p>\n\n<ol>\n\t<li>UserConcurrencyResourceType</li>\n\t<li>DmsConcurrencyResourceType</li>\n\t<li>BackupConcurrencyResourceType</li>\n\t<li>LocalQueriesConcurrencyResourceType</li>\n</ol>","a":[{"id":1687025,"option":"1","correct":false},{"id":1687026,"option":"2","correct":false},{"id":1687027,"option":"3","correct":true},{"id":1687028,"option":"4","correct":false}]},{"q":"<p>Which of the following SQL pool tools is the best option for loading and exporting large amounts of data in a faster manner in Synapse Analytics?</p>","a":[{"id":1687021,"option":"BCP","correct":false},{"id":1687022,"option":"PolyBase","correct":true},{"id":1687023,"option":"Azure Data Factory","correct":false},{"id":1687024,"option":"Azure Data Lake storage","correct":false}]},{"q":"<p>What is the correct sequence in which the operations given alongside takes place during a scale-up operation in Azure Synapse SQL?</p>","a":[{"id":1687017,"option":"(I) -> (III) -> (II)","correct":true},{"id":1687018,"option":"(I) -> (III) -> (IV)","correct":false},{"id":1687019,"option":"(I) -> (IV)","correct":false},{"id":1687020,"option":"(III) -> (I) -> (IV)","correct":false}]},{"q":"<p>Which of the following is a valid method of executing pipeline runs in Data Factory when working with Azure Synapse Analytics?</p>","a":[{"id":1687013,"option":"Dataset execution","correct":false},{"id":1687014,"option":"Manual execution","correct":false},{"id":1687015,"option":"Trigger execution","correct":false},{"id":1687016,"option":"Both 2 and 3","correct":true}]},{"q":"<p>You want to combine different Azure Open Datasets using serverless SQL pool. You then want to visualize the results in Synapse Studio for Azure Synapse Analytics.<br>\nIn the given scenario, which of these functions can you use to access files in Azure Storage?</p>","a":[{"id":1687009,"option":" OPENROWSET function ","correct":true},{"id":1687010,"option":"CAST function","correct":false},{"id":1687011,"option":"OPENFILESET function","correct":false},{"id":1687012,"option":"All 1, 2 or 3","correct":false}]},{"q":"<p>Which of the following can be used to create small tables with less than 60 million rows in Azure Synapse Analytics?</p>","a":[{"id":1687001,"option":"Heap tables","correct":true},{"id":1687002,"option":"Temporary tables","correct":false},{"id":1687003,"option":"Clustered indexes","correct":false},{"id":1687004,"option":"Non-clustered indexes","correct":false}]},{"q":"<p>In which of the given scenarios can you consider designing a hash-distributed table using a dedicated SQL pool in Azure Synapse Analytics?</p>\n\n<ol>\n\t<li>There is no obvious joining key</li>\n\t<li>The table is a temporary staging table</li>\n\t<li>The table has a frequent insert, update, and delete operations</li>\n\t<li>The table does not share a common join key with other tables</li>\n</ol>","a":[{"id":1686997,"option":"1","correct":false},{"id":1686998,"option":"2","correct":false},{"id":1686999,"option":"3","correct":true},{"id":1687000,"option":"4","correct":false}]},{"q":"<p>What is the maximum value to which MAXDOP (max_degree_of_parallelism) can be set while creating columnstore indexes in Azure Synapse Analytics?</p>","a":[{"id":1686993,"option":"1","correct":false},{"id":1686994,"option":"4","correct":false},{"id":1686995,"option":"16","correct":false},{"id":1686996,"option":"64","correct":true}]},{"q":"<p>You want to reduce the memory requirements for compressing rowgroups into columnstore indexes in Synapse SQL. Which of the following techniques can be used to do so?</p>","a":[{"id":1686989,"option":"Use fewer columns","correct":false},{"id":1686990,"option":"Use fewer string columns","correct":false},{"id":1686991,"option":"Avoid over-partitioning","correct":false},{"id":1686992,"option":"All of these","correct":true}]},{"q":"<p>You are trying to connect to Synapse workspace resources from a restricted network. When doing so, you want to access the Azure Synapse Analytics Studio. What can you do to achieve this?</p>","a":[{"id":1678337,"option":"Create a private endpoint from the Azure portal.","correct":true},{"id":1678338,"option":"Create a Region value from the Azure portal","correct":false},{"id":1678339,"option":"Both 1 and 2","correct":false},{"id":1678340,"option":"None of these","correct":false}]},{"q":"<p>You have set the Analytical Time-to-Live (TTL) on a container using the <em>AnalyticalStoreTimeToLiveInSeconds</em> property when working with Azure Synapse Analytics.<br>\n<br>\nWhat among the following will happen if you set the value of <em>TTL as \"-1\"</em>?<br>\n1. The analytical store will have infinite retention of your operational data<br>\n2. The analytical store will retain all historical data, irrespective of the retention of the data in the transactional store<br>\n3. The items will expire from the analytical store \"n\" seconds after their last modified time in the transactional store</p>","a":[{"id":1678333,"option":"1 and 3","correct":false},{"id":1678334,"option":"2 and 3","correct":false},{"id":1678335,"option":"1 and 2","correct":true},{"id":1678336,"option":"1, 2 and 3","correct":false}]},{"q":"<p>You are using the <em>FIELDTERMINATOR ='field_terminator'</em><strong> </strong>option to adjust parsing rules to custom CSV format when querying CSV source data in Synapse SQL. Which of the following will be considered as the default field terminator in the given scenario?</p>","a":[{"id":1678329,"option":"Colon (“:”)","correct":false},{"id":1678330,"option":"Comma (\",\")","correct":true},{"id":1678331,"option":"Semicolon (“;”)","correct":false},{"id":1678332,"option":"Newline character (\\r\\n)","correct":false}]},{"q":"<p>You are getting the following error message when working with Azure Synapse Analytics.<br>\n“This job was rejected because it requires 24 AUs. This account's administrator-defined policy prevents a job from using more than 5 AUs”.</p>\n\n<p>Which of the following could be a potential reason behind this problem?</p>","a":[{"id":1678309,"option":"Throttling on Data Lake Analytics","correct":true},{"id":1678310,"option":"Incorrect path to the U-SQL file","correct":false},{"id":1678311,"option":"Incorrect Azure Active Directory tenant","correct":false},{"id":1678312,"option":"Incorrect Data Lake Analytics account in the linked service ","correct":false}]},{"q":"<p>You get the following error message when working with Azure Synapse Analytics: <em>“Cannot find the 'Azure Data Lake Store' file or folder”.</em><br>\nWhich of the following is a valid way to resolve this?</p>","a":[{"id":1678305,"option":"Increase the limits on Data Lake Analytics","correct":false},{"id":1678306,"option":"Reduce the number of submitted jobs to Data Lake Analytics","correct":false},{"id":1678307,"option":"Verify the path and credentials provided in the linked service","correct":true},{"id":1678308,"option":"Change Data Factory triggers and concurrency settings on activities","correct":false}]},{"q":"<p>You notice that the definition for an Azure function activity in Synapse Analytics is not complete. Which of the error messages given alongside will be thrown in the given scenario?</p>\n\n<p>1. Azure function activity missing function key.<br>\n2. Azure function activity missing function name.<br>\n3. Azure function activity missing Method in JSON.<br>\n4. Azure function activity missing LinkedService definition in JSON.</p>","a":[{"id":1678301,"option":"Either 3 or 4","correct":false},{"id":1678302,"option":"Either 1, 2 or 3","correct":false},{"id":1678303,"option":"Either 1 or 2","correct":false},{"id":1678304,"option":"Either 1, 2,3 or 4","correct":true}]},{"q":"<p>In which of the given scenarios does a Azure Synapse workspace use a default storage container?<br>\n1. Storing the backing data files for Spark tables<br>\n2. Execution logs for Spark jobs<br>\n3. Managing libraries that you choose to install</p>","a":[{"id":1678297,"option":"1, 2","correct":false},{"id":1678298,"option":"2, 3","correct":false},{"id":1678299,"option":"1, 3","correct":false},{"id":1678300,"option":"1, 2, 3","correct":true}]},{"q":"<p>You observe that copying data into Azure SQL Database from an Azure Synapse Analytics datastore is very slow. When analyzing this problem, the root cause turns out to be triggered by the bottleneck of the Azure SQL Database. What might be the possible reason behind this bottleneck?</p>","a":[{"id":1678293,"option":"Indexes are not set properly","correct":true},{"id":1678294,"option":"Azure SQL Database tier is too high","correct":false},{"id":1678295,"option":"Azure SQL Database DTU usage is less than 100%","correct":false},{"id":1678296,"option":"Instead of stored procedure, bulk insert is being used","correct":false}]},{"q":"<p>Which of the security groups should be assigned to the following users in your Azure Synapse Analytics workspace?</p>\n\n<p><strong>Users</strong><br>\n1. Users who need complete control over the workspace.<br>\n2. Users who need to manage and monitor Apache Spark pools and Integration runtimes.</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdmins\n2. workspace1_SynapseAdministrators</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdmins\n2. workspace1_SynapseCredentialUsers</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdministrators\n2. workspace1_SynapseComputeOperators</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdministrators\n2. workspace1_SynapseContributors</code></pre>\n\n<p> </p>","a":[{"id":1678289,"option":"1","correct":false},{"id":1678290,"option":"2","correct":false},{"id":1678291,"option":"3","correct":true},{"id":1678292,"option":"4","correct":false}]},{"q":"<p>You want to represent a high-level representation of usage across a SQL pool in your Azure Synapse Analytics workspace.<br>\nWhich of these Dedicated SQL pool metrics can be used to do so?<br>\n1. DWULimit<br>\n2. DWUUsedPercent<br>\n3. DWUUsed</p>","a":[{"id":1678285,"option":"Only 2","correct":false},{"id":1678286,"option":"Either 1 or 3","correct":false},{"id":1678287,"option":"Either 1 or 2","correct":false},{"id":1678288,"option":"Either 2 or 3","correct":true}]},{"q":"<p>Which one of the following is <em>not</em> a potential cause for poor segment quality in Azure Synapse Analytics columnstore indices?</p>","a":[{"id":1678281,"option":"Too many partitions","correct":false},{"id":1678282,"option":"Too many string columns","correct":true},{"id":1678283,"option":"Memory pressure when index was built","correct":false},{"id":1678284,"option":"High volume of DML operations","correct":false}]},{"q":"<p>What is the maximum memory required to compress one row group in Azure Synapse SQL?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>72 MB + #rows * #columns * 8 bytes + #rows * #short-string-columns * 32 bytes + #long-string-columns * 16 MB for compression dictionary</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>32 MB+#rows * #columns * 8 bytes + #rows * #short-string-columns * 32 bytes + #long-string-columns * 16 MB for compression dictionary</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>72 MB + #rows * #columns * 8 bytes + #rows * #short-string-columns * 16 bytes + #long-string-columns * 16 MB for compression dictionary</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>64 MB+#rows * #columns * 8 bytes + #rows * #short-string-columns * 32 bytes </code></pre>\n\n<p> </p>","a":[{"id":1678277,"option":"1","correct":true},{"id":1678278,"option":"2","correct":false},{"id":1678279,"option":"3","correct":false},{"id":1678280,"option":"4","correct":false}]},{"q":"<p>What is the maximum retention period after which Azure Synapse Dedicated pool automatically deletes a restoration point?</p>","a":[{"id":1678273,"option":"24 hours","correct":false},{"id":1678274,"option":"1 week","correct":true},{"id":1678275,"option":"14 days","correct":false},{"id":1678276,"option":"1 month","correct":false}]},{"q":"<p>Which of the following statements regarding sharding patterns in Azure Synapse Analytics is <em>not</em> correct?</p>","a":[{"id":1678265,"option":"A round-robin distributed table distributes data evenly across a table but without any further optimization","correct":false},{"id":1678266,"option":"A replicated table provides the fastest query performance for small tables","correct":false},{"id":1678267,"option":"A round-robin table delivers slow performance when used as a staging table for loads","correct":true},{"id":1678268,"option":"A hash distributed table can deliver the highest query performance for joins and aggregations on large tables","correct":false}]},{"q":"<p>Assume that you have created a network connection between Azure Synapse Analytics Studio and a workstation within a restricted network. In the given scenario, which of these network outbound security rules is optional?</p>","a":[{"id":1678261,"option":"AzureMonitor","correct":true},{"id":1678262,"option":"AzureActiveDirectory","correct":false},{"id":1678263,"option":"AzureResourceManager","correct":false},{"id":1678264,"option":"AzureFrontDoor.Frontend","correct":false}]},{"q":"<p>Assume that you have created an Azure Synapse workspace using the Azure portal. In the given scenario, which of these methods can you use to open Synapse studio after the workspace is created?</p>","a":[{"id":1678257,"option":"Go to the https://web.azuresynapse.net and sign in to your workspace.","correct":false},{"id":1678258,"option":"Open your Synapse workspace in the Azure portal. Select Launch Synapse Studio from the top of the Overview section","correct":false},{"id":1678259,"option":"Either 1 or 2","correct":true},{"id":1678260,"option":"None of these","correct":false}]},{"q":"<p>You want to configure and use Azure Active Directory authentication when working with Azure Synapse Analytics. What is the correct sequence in which the steps given below need to be followed to achieve this?</p>\n\n<p>1. Create an Azure Active Directory identity<br>\n2. Create and populate Azure Active Directory<br>\n3. Connect to Synapse Studio by using Azure Active Directory identities<br>\n4.  Assign role to created Azure Active Directory identity in Synapse workspace</p>","a":[{"id":1678253,"option":"2, 1, 4, 3","correct":true},{"id":1678254,"option":"1, 2, 3, 4","correct":false},{"id":1678255,"option":"4, 3, 2, 1","correct":false},{"id":1678256,"option":"3, 4, 1, 2","correct":false}]},{"q":"<p>Which of the following statements are valid with respect to creating an Azure Synapse workspace using the Azure portal?<br>\n1. You will not be able to move the workspace to another Azure Active Directory tenant.<br>\n2. You cannot create a Synapse Analytics workspace in a Cloud Solution Provider (CSP) subscription.</p>","a":[{"id":1678249,"option":"Only 1","correct":false},{"id":1678250,"option":"Only 2","correct":false},{"id":1678251,"option":"All of these","correct":true},{"id":1678252,"option":"None of these","correct":false}]},{"q":"<p>Which of the following built-in database roles is most commonly used to grant full permission to users in Azure Synapse Analytics?</p>","a":[{"id":1678245,"option":"db_owner","correct":true},{"id":1678246,"option":"db_ddladmin","correct":false},{"id":1678247,"option":"db_datawriter","correct":false},{"id":1678248,"option":"db_datareader","correct":false}]},{"q":"<p>You have moved an Azure Synapse workspace to another Azure Active Directory tenant through subscription migration. What will happen as a result of this?</p>","a":[{"id":1678065,"option":"You will not be able to perform any actions within the workspace","correct":false},{"id":1678066,"option":"You may lose access to the entire workspace","correct":false},{"id":1678067,"option":"You may lose access to the artifacts within the workspace","correct":true},{"id":1678068,"option":"Both 1 and 3","correct":false}]},{"q":"<p>You are analyzing some data in the blob storage using Spark when working with Synapse.<br>\nWhich of the following code snippet can you use to see the schema of the dataframe run a cell when doing so?</p>\n\n<p><strong>Code Snippet</strong></p>\n\n<p>1. </p>\n\n<pre class=\"prettyprint\"><code>data_df.print()</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>data_df.print(Schema)</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>data_df.printSchema()</code></pre>\n\n<p> </p>","a":[{"id":1678061,"option":"1","correct":false},{"id":1678062,"option":"2","correct":false},{"id":1678063,"option":"3","correct":true},{"id":1678064,"option":"Either 1 or 2","correct":false}]},{"q":"<p>You are loading data into a dedicated SQL pool in <em>Azure</em> Synapse Analytics. Which of the following tools can be used to move data to Azure Data Lake Store Gen2 when doing so?</p>","a":[{"id":1678401,"option":"AZCopy utility ","correct":false},{"id":1678402,"option":"Azure ExpressRoute","correct":false},{"id":1678403,"option":"Azure Data Factory ","correct":false},{"id":1678404,"option":"All of these","correct":true}]},{"q":"<p>You want to maintain statistics when developing data warehouse solutions for <em>Azure</em> Synapse SQL. On which of the following columns should you maintain the statistics in order to gain the most benefit?</p>\n\n<ol>\n\t<li>On columns involved in joins</li>\n\t<li>On columns used in the WHERE clause</li>\n\t<li>On columns found in GROUP BY</li>\n</ol>","a":[{"id":1677969,"option":"Only 1","correct":false},{"id":1677970,"option":"Only 2","correct":false},{"id":1677971,"option":"Only 3","correct":false},{"id":1677972,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>Consider the given scenario:<br>\nThe member name parameter specified during the classification process for an <em>Azure</em> Synapse SQL pool is a database user instead of a database role. Which of the following can be inferred from the given scenario?</p>","a":[{"id":1677965,"option":"The weighting for the user is higher","correct":true},{"id":1677966,"option":"The weighting for the user is lower","correct":false},{"id":1677967,"option":"The weighting for the user cannot be determined","correct":false},{"id":1677968,"option":"None of these","correct":false}]},{"q":"<p>You are trying to connect to <em>Azure</em> Synapse workspace resources from a restricted network. When doing so, you want to find the Private DNS zone in the Azure portal. Which of these private DNS zone dedicated names should you use for the private endpoint of accessing the Azure Synapse Analytics Studio gateway?</p>","a":[{"id":1677961,"option":"privatelink.azuresynapse.net","correct":true},{"id":1677962,"option":"privatelink.sql.azuresynapse.net","correct":false},{"id":1677963,"option":"privatelink.dfs.core.windows.net","correct":false},{"id":1677964,"option":"privatelink.dev.azuresynapse.net","correct":false}]},{"q":"<p>You are in process of creating a private DNS zone when trying to connect to <em>Azure</em> Synapse workspace resources from a restricted network. You have added the virtual network link and you now need to add the DNS record set in the private DNS zone. In the given scenario, which of these dedicated name strings should you use for the private endpoint of SQL query execution in the built-in pool?</p>","a":[{"id":1677957,"option":"Web","correct":false},{"id":1677958,"option":"SQL","correct":false},{"id":1677959,"option":"YourWorkSpaceName","correct":false},{"id":1677960,"option":"YourWorkSpaceName-ondemand","correct":true}]},{"q":"<p>A query in a serverless SQL pool in <em>Azure</em> Synapse Analytics fails with the following error message:</p>\n\n<p><em>“Failed to execute query. Error: CREATE EXTERNAL TABLE/DATA SOURCE/DATABASE SCOPED CREDENTIAL/FILE FORMAT is not supported in master database”.</em><br>\nWhat might have caused such an error to occur in the given scenario?</p>","a":[{"id":1677953,"option":"The master database in serverless SQL pool does not support the creation of external tables","correct":false},{"id":1677954,"option":"The master database in serverless SQL pool does not support the creation of external file formats","correct":false},{"id":1677955,"option":"The master database in serverless SQL pool does not support the creation of external data sources","correct":false},{"id":1677956,"option":"All of these","correct":true}]},{"q":"<p>A query in <em>Azure</em> Synapse Analytics fails with the following error message:</p>\n\n<p><em>“File cannot be opened because it does not exist or it is used by another process”.</em><br>\nWhat might be the cause of this?</p>","a":[{"id":1677949,"option":"You do not have permissions to log into serverless SQL pool","correct":false},{"id":1677950,"option":"Your network prevents communication to Azure Synapse backend","correct":false},{"id":1677951,"option":"Your Azure Active Directory identity does not have rights to access the file","correct":true},{"id":1677952,"option":"The serverless SQL pool is not able to execute it at this moment due to resource constraints","correct":false}]},{"q":"<p>Which of the following scope levels is located at the top of the hierarchy in <em>Azure</em> Synapse role-based access control?</p>","a":[{"id":1677945,"option":"Workspace","correct":true},{"id":1677946,"option":"Spark pool","correct":false},{"id":1677947,"option":"Integration runtime","correct":false},{"id":1677948,"option":"Linked service","correct":false}]},{"q":"<p>You are loading data using dedicated SQL pools in <em>Azure</em> Synapse Analytics. What can you do to minimize latency when doing so?</p>","a":[{"id":1677889,"option":"Colocate your storage layer and your dedicated SQL pool.","correct":true},{"id":1677890,"option":"Create loading users designated for running loads","correct":false},{"id":1677891,"option":"Either 1 or 2","correct":false},{"id":1677892,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You need to transform data in various formats and also import this data into a relational platform when working with <em>Azure</em> Synapse Analytics. Which of the following can be used to achieve this?</p>","a":[{"id":1677885,"option":"Azure Data Lake Storage","correct":false},{"id":1677886,"option":"Azure Synapse","correct":false},{"id":1677887,"option":"Azure PolyBase","correct":true},{"id":1677888,"option":"Azure Data Factory","correct":false}]},{"q":"<p>You are working on a large set of data that has been stored in <em>Azure</em> Data Lake Storage in the parquet format. You have been asked to query this data from Azure Synapse Analytics using T-SQL. Which of the following will you use to query the data in the given scenario?</p>","a":[{"id":1677881,"option":"PolyBase","correct":true},{"id":1677882,"option":"Azure Databricks","correct":false},{"id":1677883,"option":"Azure HDInsight","correct":false},{"id":1677884,"option":"Azure Data Factory","correct":false}]},{"q":"<p>You receive the following error when trying to copy data from SQL Server into <em>Azure</em> Synapse Analytics using staged copy and PolyBase:</p>\n\n<p><em>\" ErrorCode=FailedDbOperation,Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException, Message=Error happened when loading data into Azure Synapse Analytics., \"</em><br>\n<br>\nWhich of the following is the best possible way to resolve this error?</p>","a":[{"id":1677877,"option":"Reduce column width to less than 1 MB","correct":false},{"id":1677878,"option":"Use a bulk insert approach by disabling PolyBase","correct":false},{"id":1677879,"option":"Set the use type default option to false in the copy activity sink, under PolyBase settings","correct":true},{"id":1677880,"option":"Set the use type default option to true in the copy activity sink, under PolyBase settings","correct":false}]},{"q":"<p>You receive the following error when trying to use SQL query to pull data from <em>Azure</em> Synapse Analytics:</p>\n\n<p><em>\"...StorageException: The condition specified using HTTP conditional header(s) is not met...\"</em><br>\n<br>\nWhat might be the potential cause behind this type of issue?</p>","a":[{"id":1677873,"option":"The schema is too large ","correct":false},{"id":1677874,"option":"Azure Synapse Analytics PolyBase cannot convert an empty string to a GUID","correct":false},{"id":1677875,"option":"Azure Synapse Analytics PolyBase cannot insert an empty string (null value) into a decimal column","correct":false},{"id":1677876,"option":"Azure Synapse Analytics encountered an issue while querying the external table in Azure Storage","correct":true}]},{"q":"<p>You receive the following error when trying to copy data into <em>Azure</em> Synapse Analytics by using PolyBase:</p>\n\n<p><em>\"Message=110802;An internal DMS error occurred that caused this operation to fail. Details: Exception: Microsoft.SqlServer.DataWarehouse.DataMovement.Common.ExternalAccess.HdfsAccessException,\"</em></p>\n\n<p>When troubleshooting the issue you found out that the potential cause might be that the schema is too large. As a result, you are required to check the schema of the target Azure Synapse Analytics table by adding the size of all columns. In the given scenario, what should the size of the column “Nvarchar(n)” be added as?</p>","a":[{"id":1677869,"option":"n bytes","correct":false},{"id":1677870,"option":"n*2 bytes","correct":true},{"id":1677871,"option":"n*4 bytes","correct":false},{"id":1677872,"option":"n*8 bytes","correct":false}]},{"q":"<p>Suppose when trying to run the Copy activity in <em>Azure</em> Data Lake Storage Gen1 you receive the following error message:<br>\n<br>\n<em>The remote server returned an error: (403) Forbidden. Response details: {\"RemoteException\":{\"exception\":\"AccessControlException\"\"message\":\"CREATE failed with error 0x83090aa2 (Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation.)....</em><br>\nWhich of the following is the best possible way to correct this problem when working with Azure Synapse Analytics?</p>","a":[{"id":1677865,"option":"By rerunning the copy activity after several minutes","correct":false},{"id":1677866,"option":"By providing the Service Token Server which is owned by Azure Active Directory","correct":false},{"id":1677867,"option":"By granting appropriate permissions to all the folders and subfolders you need to copy","correct":true},{"id":1677868,"option":"By using the staged copy to skip the Transport Layer Security validation for Azure Data Lake Storage Gen1","correct":false}]},{"q":"<p>A copy activity in <em>Azure</em> Synapse SQL fails because the Service Token Server (STS) owned by Azure Active Directory is unavailable. Which of the following HTTP errors will be thrown in this scenario?</p>","a":[{"id":1677861,"option":"HTTP error 500","correct":false},{"id":1677862,"option":"HTTP error 502","correct":false},{"id":1677863,"option":"HTTP error 503","correct":true},{"id":1677864,"option":"HTTP error 507","correct":false}]},{"q":"<p>When working with <em>Azure</em> Synapse Analytics, you encountered an issue where the network connection to the Databricks service was interrupted. Which of the following error messages will be visible on your computer screen in the respective scenario?</p>","a":[{"id":1677857,"option":"Job execution failed.","correct":false},{"id":1677858,"option":"An error occurred while sending the request.","correct":true},{"id":1677859,"option":"The cluster is in Terminated state, not available to receive jobs. Please fix the cluster or retry later.","correct":false},{"id":1677860,"option":"User: SimpleUserContext {userId=..., name=user@company.com, orgId=...} is not authorized to access cluster.","correct":false}]},{"q":"<p>Suppose you are getting the following error message:</p>\n\n<p><em>“Invalid Python file URI... Please visit Databricks user guide for supported URI schemes”.</em><br>\nWhich of the following is the best possible way to resolve such type of issue?</p>","a":[{"id":1677853,"option":"Verify the linked service definition","correct":false},{"id":1677854,"option":"Verify that the Databricks cluster exists","correct":false},{"id":1677855,"option":"Create a new token and update the linked service","correct":false},{"id":1677856,"option":"Specify the absolute path for workspace-addressing schemes","correct":true}]},{"q":"<p>Which of these options is the correct order in which the ELT (Extract, Load, and Transform) process should be used to load data in <em>Azure</em> Synapse Analytics?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Transform the data.</li>\n\t<li>Prepare the data for loading.</li>\n\t<li>Extract the source data into text files.</li>\n\t<li>Land the data into Azure Blob storage.</li>\n\t<li>Insert the data into production tables.</li>\n\t<li>Load the data into staging tables with the COPY command.</li>\n</ol>","a":[{"id":1677849,"option":"1 -> 2 -> 3 -> 4 -> 5 -> 6","correct":false},{"id":1677850,"option":"3 -> 4 -> 2 -> 6 -> 1 -> 5","correct":true},{"id":1677851,"option":"4 -> 3 -> 6 -> 2 -> 5 -> 1","correct":false},{"id":1677852,"option":"5 -> 1 -> 6 -> 2 -> 4 -> 3","correct":false}]},{"q":"<p>You are using <em>Azure</em> Active Directory Authentication for authentication with Synapse SQL. In the given scenario, which of the following authentication methods is not supported for Azure Active Directory server principals?</p>","a":[{"id":1677845,"option":"Azure Active Directory Password","correct":false},{"id":1677846,"option":"Azure Active Directory Integrated","correct":false},{"id":1677847,"option":"Azure Active Directory Universal with MFA","correct":false},{"id":1677848,"option":"Using Application token authentication","correct":true}]},{"q":"<p>Which of the following permissions should an <em>Azure</em> Synapse Analytics workspace managed identity have in order to either encrypt or decrypt static data?</p>","a":[{"id":1677841,"option":"WrapKey ","correct":false},{"id":1677842,"option":"UnwrapKey ","correct":false},{"id":1677843,"option":"Get","correct":false},{"id":1677844,"option":"All of these","correct":true}]},{"q":"<p>You want to enhance query performance when working with columnstore indexes in dedicated <em>Azure</em> Synapse SQL pools. What should you do to achieve this?</p>","a":[{"id":1677837,"option":"Maximize the number of rows in each rowgroup ","correct":true},{"id":1677838,"option":"Compress all the rows designated for each rowgroup","correct":false},{"id":1677839,"option":"Bulk load the columnstore into a cluster","correct":false},{"id":1677840,"option":"None of these","correct":false}]},{"q":"<p>You are running a query that uses a serverless SQL pool in <em>Azure</em> Synapse Analytics for the first time. How much time will it take for the serverless SQL pool to gather the SQL resources required for running the queries?</p>","a":[{"id":1677833,"option":"1 second","correct":false},{"id":1677834,"option":"5 seconds","correct":false},{"id":1677835,"option":"10 seconds","correct":true},{"id":1677836,"option":"30 seconds","correct":false}]},{"q":"<p>Which of the following are valid considerations that you need to be followed when naming a dedicated SQL pool in <em>Azure</em> Synapse SQL?</p>","a":[{"id":1677829,"option":"The name should contain reserved words","correct":false},{"id":1677830,"option":"The name should be unique in the workspace","correct":true},{"id":1677831,"option":"The name should contain special characters","correct":false},{"id":1677832,"option":"The name must be minimum 20 characters","correct":false}]},{"q":"<p>Which of these <em>Azure</em> Synapse SQL clients will allow you to run SQL queries on your On-demand database?</p>","a":[{"id":1677825,"option":"Azure Data Studio ","correct":false},{"id":1677826,"option":"SQL Server Management Studio","correct":true},{"id":1677827,"option":"Azure Synapse Studio ","correct":false},{"id":1677828,"option":"SQL Server Data Tools","correct":false}]},{"q":"<p>Alice is working on Compute Nodes in Azure Synapse. What is the correct range of compute nodes that should be available to the <em>Azure</em> Synapse dedicated SQL pool?</p>","a":[{"id":1677821,"option":"1-20","correct":false},{"id":1677822,"option":"1-40","correct":false},{"id":1677823,"option":"1-60","correct":true},{"id":1677824,"option":"1-80","correct":false}]},{"q":"<p>Which of these can be used in<em> Azure</em> Synapse to move data between services and orchestrate activities?</p>","a":[{"id":1677817,"option":"Pipelines","correct":true},{"id":1677818,"option":"Synapse SQL","correct":false},{"id":1677819,"option":"Data Flows","correct":false},{"id":1677820,"option":"Activities","correct":false}]},{"q":"<p>Which of the following roles can you assign to yourself when you prepare an existing storage account for use with <em>Azure</em> Synapse Analytics?</p>","a":[{"id":1677653,"option":"Owner role only ","correct":false},{"id":1677654,"option":"Storage Blob Data Owner role only ","correct":false},{"id":1677655,"option":"Both Owner role and Storage Blob Data Owner role","correct":true},{"id":1677656,"option":"None of these","correct":false}]},{"q":"<p>You submitted SQL requests via the <em>Azure</em> Synapse Studio in a workspace enabled dedicated SQL pool. Where can these requests be viewed in your Synapse workspace?</p>","a":[{"id":1677649,"option":"In the Activities tab","correct":false},{"id":1677650,"option":"In the SQL DW","correct":false},{"id":1677651,"option":"In the Monitor hub","correct":true},{"id":1677652,"option":"In the SQL pool","correct":false}]},{"q":"<p>Which of the following tells <em>Azure</em> Synapse how many Spark resources to use when creating a serverless Apache Spark pool?</p>","a":[{"id":1677645,"option":"The Spark pool","correct":true},{"id":1677646,"option":"The SQL pool","correct":false},{"id":1677647,"option":"The logical SQL server","correct":false},{"id":1677648,"option":"The dedicated SQL pool","correct":false}]},{"q":"<p>Which of the following resource models does <em>Azure</em> Synapse SQL provide?</p>\n\n<p><strong>Options</strong><br>\n1. Serverless resource model<br>\n2. Dedicated resource model</p>","a":[{"id":1677641,"option":"Only 1","correct":false},{"id":1677642,"option":"Only 2","correct":false},{"id":1677643,"option":"Both 1 and 2","correct":true},{"id":1677644,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>What is the basic requirement for using string interpolation syntax in SQL source queries when working with Azure Synapse Analytics?</p>","a":[{"id":1677517,"option":"The query string must be on one single line, with '/n'","correct":false},{"id":1677518,"option":"The query string must be on one single line, without '/n'","correct":true},{"id":1677519,"option":"The query string must be on multiple lines, with '/n'","correct":false},{"id":1677520,"option":"The query string must be on multiple lines, without '/n'","correct":false}]},{"q":"<p>You are trying to preserve metadata and ACLs using copy activity in Azure Synapse Analytics. Which of the following will allow different sets of characters in the keys of customer specified metadata when doing so?</p>","a":[{"id":1677513,"option":"Amazon S3","correct":false},{"id":1677514,"option":"Azure Blob storage","correct":false},{"id":1677515,"option":"Both 1 and 2","correct":true},{"id":1677516,"option":"None of these","correct":false}]},{"q":"<p>Which of the following creates a restore point you can leverage to recover your data warehouse to a previous state in Azure Synapse Dedicated SQL pool?</p>","a":[{"id":1677509,"option":"A data warehouse snapshot","correct":true},{"id":1677510,"option":"Restore points","correct":false},{"id":1677511,"option":"User defined restore points","correct":false},{"id":1677512,"option":"Retention snapshots","correct":false}]},{"q":"<p>Which of these are necessary steps that need to be taken when creating an Azure Synapse workspace with Azure PowerShell?</p>\n\n<p><strong>Steps</strong></p>\n\n<ol>\n\t<li> Define necessary environment variables to create resources for Azure Synapse workspace</li>\n\t<li> Create a resource group as a container for your Azure Synapse workspace</li>\n</ol>","a":[{"id":1677505,"option":"Only 1","correct":false},{"id":1677506,"option":"Only 2","correct":false},{"id":1677507,"option":"Both 1 and 2","correct":true},{"id":1677508,"option":"None of these","correct":false}]},{"q":"<p>Which of the following table action can be used to remove all rows from the target table in a SQL database when working with Azure Synapse Analytics?</p>","a":[{"id":1677497,"option":"Recreate","correct":false},{"id":1677498,"option":"Truncate","correct":true},{"id":1677499,"option":"Remove","correct":false},{"id":1677500,"option":"Retrieve","correct":false}]},{"q":"<p>Which of the following type of Pipeline-to-trigger relationship is supported by Tumbling window triggers during pipeline execution in Azure Synapse Analytics?</p>","a":[{"id":1677485,"option":"One-to-one relationship","correct":true},{"id":1677486,"option":"One-to-many relationship","correct":false},{"id":1677487,"option":"Many-to-many relationship","correct":false},{"id":1677488,"option":"Many-to-one relationship","correct":false}]},{"q":"<p>Which of these object types is not supported by serverless SQL pools in Synapse SQL?</p>","a":[{"id":1677481,"option":"Tables","correct":true},{"id":1677482,"option":"Views","correct":false},{"id":1677483,"option":"Schemas","correct":false},{"id":1677484,"option":"Procedures","correct":false}]},{"q":"<p>How many consumption models are offered by Azure Synapse SQL?</p>","a":[{"id":1688761,"option":"One","correct":false},{"id":1688762,"option":"Two","correct":true},{"id":1688763,"option":"Three","correct":false},{"id":1688764,"option":"Four","correct":false}]},{"q":"<p>You are using the BuiltinSqlPoolDataRequestsEnded metric to monitor the number of SQL requests that ended for the built-in serverless SQL pool in a Synapse workspace.<br>\nWhich of these dimensions of the metrics should be used to filter the results by the final state?</p>","a":[{"id":1687061,"option":"Result dimension","correct":true},{"id":1687062,"option":"Filter dimension","correct":false},{"id":1687063,"option":"State dimension","correct":false},{"id":1687064,"option":"Count dimension","correct":false}]},{"q":"<p>Which of the following statements are valid with respect to configuring and using Azure Synapse Link for Azure Cosmos DB?</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li> Turning on Synapse Link does not turn on the analytical store automatically.</li>\n\t<li> Analytical store should be enabled on containers to start replicating your operation data to analytical store.</li>\n</ol>","a":[{"id":1677493,"option":"Only 1","correct":false},{"id":1677494,"option":"Only 2","correct":false},{"id":1677495,"option":"Both 1 and 2","correct":true},{"id":1677496,"option":"None of these","correct":false}]},{"q":"Which of these actions can you take to load data into a Spark DataFrame when developing in Synapse using Azure Purview?","a":[{"id":1687005,"option":" New SQL Script","correct":false},{"id":1687006,"option":"New notebook","correct":true},{"id":1687007,"option":" New data flow","correct":false},{"id":1687008,"option":"New table","correct":false}]},{"q":"What will be the consequence of increasing the number of data warehouse units (DWUs) in a Azure Synapse Dedicated SQL pool?","a":[{"id":1678269,"option":"The number of readers and writers for PolyBase load operations decreases","correct":false},{"id":1678270,"option":"The maximum number of concurrent queries and concurrency slots decreases","correct":false},{"id":1678271,"option":"The performance of the system changes linearly for scans, aggregations, and CTAS statements","correct":true},{"id":1678272,"option":"The performance of the system changes in a non-linear fashion for scans, aggregations, and CTAS statements","correct":false}]},{"q":"What is the effective Data Integration Units (DIU) for staged storage-to-Synapse when using PolyBase with Azure Integration Runtime?","a":[{"id":1677501,"option":"0","correct":false},{"id":1677502,"option":"1","correct":false},{"id":1677503,"option":"2","correct":true},{"id":1677504,"option":"4","correct":false}]},{"q":"Which of these wildcard patterns represents a recursive directory nesting in Azure Synapse Analytics?","a":[{"id":1677489,"option":"**","correct":true},{"id":1677490,"option":"?","correct":false},{"id":1677491,"option":"( )","correct":false},{"id":1677492,"option":"{ }","correct":false}]}]