[{"q":"<p>In Azure, you have a pipeline in Data Factory with the following three consecutive activities where each activity runs only if the previous activity succeeds.<br>\nActivity A -&gt; Activity B -&gt; Activity C<br>\nIn the given scenario, what will be the status of B and C if A fails to run?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Skipped\nCompleted</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Failed \nFailed </code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Failed\nSkipped</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>Skipped\nSkipped</code></pre>","a":[{"id":1678113,"option":"1","correct":false},{"id":1678114,"option":"2","correct":false},{"id":1678115,"option":"3","correct":false},{"id":1678116,"option":"4","correct":true}]},{"q":"<p>Which of the following is the fastest naming option for writing files in Azure Data Factory?</p>","a":[{"id":1688641,"option":"Default","correct":true},{"id":1688642,"option":"Pattern","correct":false},{"id":1688643,"option":"Pre partition","correct":false},{"id":1688644,"option":"As data in column","correct":false}]},{"q":"<p>Which of the following conditional paths is passed as default when handling errors during pipeline execution in Data Factory?</p>","a":[{"id":1688637,"option":"Upon Completion","correct":false},{"id":1688638,"option":"Upon Failure","correct":false},{"id":1688639,"option":"Upon Skip","correct":false},{"id":1688640,"option":"Upon Success ","correct":true}]},{"q":"<p>You encounter validation errors at the source with multiline CSV files when mapping data flows in Azure Data Factory. In the given scenario, which of these error messages will not be visible on the Azure Data Factory portal?</p>","a":[{"id":1688629,"option":"Schema validation at source fails","correct":false},{"id":1688630,"option":"The last column is null or missing","correct":false},{"id":1688631,"option":"During Data Flow activity execution: Hit unexpected exception and execution failed","correct":true},{"id":1688632,"option":"Schema import fails to show correctly in the UX and the last column has a new line character in the name","correct":false}]},{"q":"<p>You receive the error given alongside in your Data Factory portal when joining SQL Server Integration Services (SSIS) Integration Runtime (IR) to an Azure virtual network.</p>\n\n<p><strong>Error</strong></p>\n\n<pre class=\"prettyprint\"><code>\"SubnetId is not enabled for current account. Microsoft.Batch resource provider is not registered under the same subscription of VNet.\"</code></pre>\n\n<p>What does such an error indicate?</p>","a":[{"id":1688625,"option":"It indicates that the virtual network does not exist","correct":false},{"id":1688626,"option":"It indicates that the specified subnet does not exist","correct":false},{"id":1688627,"option":"It indicates that the Azure Batch service cannot access the virtual network","correct":true},{"id":1688628,"option":"It indicates that the  Azure Batch service cannot access the specified subnet","correct":false}]},{"q":"<p>You are given an error code:</p>\n\n<p>\"InternalServerError is displayed in the Data Factory portal when provisioning SQL Server Integration Services (SSIS) Integration Runtime(IR).\"<br>\nIn Azure, which of these management issues could be a reason for such an error to occur?</p>","a":[{"id":1688621,"option":"Transient issues","correct":true},{"id":1688622,"option":"Custom setup issues","correct":false},{"id":1688623,"option":"Virtual network configuration problems","correct":false},{"id":1688624,"option":"Azure SQL Database or Azure SQL Managed Instance issues","correct":false}]},{"q":"<p>In Azure, you encounter the error message given alongside in your Data Factory portal when executing SQL Server Integration Services (SSIS) packages in the SSIS integration runtime.</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>\"Connection Timeout Expired\" or \"The service has encountered an error processing your request. Please try again.\"</code></pre>\n\n<p>Which of the following is a potential cause for this error?</p>","a":[{"id":1688617,"option":"The data source or destination is overloaded","correct":false},{"id":1688618,"option":"The network between the SSIS integration runtime and the data source or destination is unstable","correct":false},{"id":1688619,"option":"The local disk is used up in the SSIS integration runtime node","correct":false},{"id":1688620,"option":"Either 1 or 2","correct":true}]},{"q":"<p>You are using the Activity dispatch operation to route activity to a target compute service when working with Data Factory.<br>\nWhat is the benefit of doing so?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. There will be no need to scale up the compute size</p>\n\n<p>2. The elastically gets scaled up</p>","a":[{"id":1686845,"option":"1","correct":true},{"id":1686846,"option":"2","correct":false},{"id":1686847,"option":"Both 1 and 2","correct":false},{"id":1686848,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Which of the following can be used to manually run your pipeline in Azure Data Factory?<br>\n1. .NET SDK<br>\n2. Python SDK<br>\n3. Azure Synapse</p>","a":[{"id":1686841,"option":"Only 1 and 2","correct":true},{"id":1686842,"option":"Only 2 and 3","correct":false},{"id":1686843,"option":"Only 1 and 3","correct":false},{"id":1686844,"option":"All 1, 2 and 3","correct":false}]},{"q":"<p>Which of these Azure Data Factory control activities allows a pipeline to invoke another pipeline?</p>","a":[{"id":1686837,"option":"Append Pipeline","correct":false},{"id":1686838,"option":"Until Activity","correct":false},{"id":1686839,"option":"Set Pipeline","correct":false},{"id":1686840,"option":"Execute Pipeline","correct":true}]},{"q":"<p>You are running and monitoring a Power Query data wrangling activity in Data Factory.<br>\nIn the given scenario, what does \"Trigger now\" do once your pipeline is published?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. It executes an on-demand run of the last published pipeline</p>\n\n<p>2. It schedules all existing Azure Data Factory triggers</p>\n\n<p>3. It pipelines the canvas</p>\n\n<p>4. It visualizes the output of a triggered Power Query activity run</p>","a":[{"id":1686833,"option":"1","correct":true},{"id":1686834,"option":"2","correct":false},{"id":1686835,"option":"3","correct":false},{"id":1686836,"option":"4","correct":false}]},{"q":"<p>Which of the following can be used as a source dataset for a Power Query mash-up in Azure Data Factory?</p>\n\n<ol>\n\t<li>An existing dataset</li>\n\t<li>Create a new dataset</li>\n\t<li>Select a wrangling dataset</li>\n</ol>","a":[{"id":1686829,"option":"1","correct":false},{"id":1686830,"option":"2","correct":false},{"id":1686831,"option":"3","correct":false},{"id":1686832,"option":"Either 1 or 2","correct":true}]},{"q":"<p>You are creating a data factory when transforming data with mapping data flows. Which of the following steps should you take when doing so?<br>\n1. Select a globally unique name for the Azure data factory.<br>\n2. Select an existing resource group or create a new one.<br>\n3. Select a location for the data factory</p>","a":[{"id":1686825,"option":"Only 1 and 2","correct":false},{"id":1686826,"option":"Only 1 and 3","correct":false},{"id":1686827,"option":"Only 2 and 3","correct":false},{"id":1686828,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>You are asked to execute a pipeline run in Data Factory using a trigger that operates on a periodic interval while also retaining the state. Which of the following triggers can you use to achieve the mentioned scenario?</p>","a":[{"id":1678121,"option":"Schedule trigger","correct":false},{"id":1678122,"option":"Event-based trigger","correct":false},{"id":1678123,"option":"Equal interval trigger","correct":false},{"id":1678124,"option":"Tumbling window trigger","correct":true}]},{"q":"<p>Consider a pipeline in a Data Factory with two activities in the sequence Activity X -&gt; Activity Y, where Y runs only if X has a final status of either succeeded or failed.<br>\nWhat will be the dependency condition of Activity Y on Activity X in the given scenario?</p>","a":[{"id":1678117,"option":"Failed","correct":false},{"id":1678118,"option":"Skipped","correct":false},{"id":1678119,"option":"Completed","correct":true},{"id":1678120,"option":"Succeeded","correct":false}]},{"q":"<p>You want to create a pipeline in a data factory that copies data from one folder to another folder in Azure blob storage. Which of the following tools can be used to achieve this?</p>","a":[{"id":1678109,"option":"Azure Data Studio","correct":false},{"id":1678110,"option":"Azure CLI","correct":false},{"id":1678111,"option":"Azure CloudShell","correct":false},{"id":1678112,"option":"Azure PowerShell","correct":true}]},{"q":"<p>You are copying data between two data stores that are publicly accessible through the internet from any IP.<br>\nWhich of these integration runtimes can you use to run the given copy activity in Data Factory?</p>","a":[{"id":1678101,"option":"Self-hosted IR","correct":false},{"id":1678102,"option":"System-operated IR","correct":false},{"id":1678103,"option":"Azure IR","correct":true},{"id":1678104,"option":"Azure-SSIS IR","correct":false}]},{"q":"<p>Which of the following provides a bridge between an activity and a linked service in Azure Data Factory?</p>","a":[{"id":1678097,"option":"Dataset","correct":false},{"id":1678098,"option":"Integration runtime ","correct":true},{"id":1678099,"option":"Pipeline","correct":false},{"id":1678100,"option":"Trigger","correct":false}]},{"q":"<p>Which of the following parameters can be considered as a strongly-typed parameter for a pipeline in Data Factory?</p>\n\n<ol>\n\t<li>Activities</li>\n\t<li>Datasets</li>\n\t<li>Linked Services</li>\n</ol>","a":[{"id":1678093,"option":"1 and 2","correct":false},{"id":1678094,"option":"1 and 3","correct":false},{"id":1678095,"option":"2 and 3","correct":true},{"id":1678096,"option":"All of these","correct":false}]},{"q":"<p>Which of these capabilities does <em>Azure</em> Integration runtime provide for a private network in Data Factory:</p>\n\n<ol>\n\t<li>Data Flow</li>\n\t<li>Data movement</li>\n\t<li>Activity dispatch</li>\n</ol>","a":[{"id":1677933,"option":"Only 1 and 2","correct":false},{"id":1677934,"option":"Only 2 and 3","correct":false},{"id":1677935,"option":"Only 1 and 3","correct":false},{"id":1677936,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>Assume that you have created a Data Factory using Azure Data Factory UI. In the given context, which of these roles should you belong to in order to create and manage child resources in the Azure portal?</p>","a":[{"id":1677545,"option":"Data Factory Contributor role at the resource group level only","correct":true},{"id":1677546,"option":"Data Factory Contributor role at the resource group level or above","correct":false},{"id":1677547,"option":"The contributor role at the resource level or above","correct":false},{"id":1677548,"option":"The contributor role at the resource level only","correct":false}]},{"q":"<p>Assume that you have created a Data Factory using Azure Data Factory UI. Now, you want to create a Data Factory instance using an Azure user account. In the given scenario, the account must be a member of which of these roles in order to achieve this?</p>","a":[{"id":1677541,"option":"contributor role","correct":false},{"id":1677542,"option":"owner role","correct":false},{"id":1677543,"option":"Both 1 and 2","correct":true},{"id":1677544,"option":"None of these","correct":false}]},{"q":"<p>You want to allow a user to create, edit, or delete any data factory in a resource group from the Azure portal. Which of these custom roles should you assign to the user in order to achieve this?</p>","a":[{"id":1677453,"option":"Assign the built-in reader role on the data factory resource","correct":false},{"id":1677454,"option":"Assign the built-in contributor role on the data factory resource","correct":false},{"id":1677455,"option":"Assign the built-in Data Factory contributor role at the data factory level","correct":false},{"id":1677456,"option":"Assign the built-in Data Factory contributor role at the resource group level","correct":true}]},{"q":"<p>You are using the TRY-CATCH block given alongside for handling errors in Data Factory. In the given context, what will the overall pipeline outcome be if the previous activity fails?</p>","a":[{"id":1677449,"option":"It succeeds if Upon Failure path succeeds","correct":true},{"id":1677450,"option":"It Fails if Upon Failure path succeeds fails","correct":false},{"id":1677451,"option":"It is skipped","correct":false},{"id":1677452,"option":"None of these","correct":false}]},{"q":"<p>You notice that multiple error messages are returned when connectivity issues occur in your sink datastore. How can you resolve this issue if you're using Azure IR in Data Factory?</p>","a":[{"id":1677361,"option":"Disable the firewall setting of the datastore","correct":true},{"id":1677362,"option":"Check your proxy and firewall settings","correct":false},{"id":1677363,"option":"Check for failures in a multiple-node self-hosted IR","correct":false},{"id":1677364,"option":"None of these","correct":false}]},{"q":"<p>Which of the following error messages will be returned in Data factory by Azure Function Activity in case of incorrect function URLs?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>Invalid HttpMethod: '%method;'.</p>\n\n<p><strong>2.</strong></p>\n\n<p>Response Content is not a valid JObject.</p>\n\n<p><strong>3.</strong></p>\n\n<p>There was an error while calling endpoint.</p>\n\n<p><strong>4.</strong></p>\n\n<p>Call to provided Azure function '%FunctionName;' failed with status-'%statusCode;' and message - '%message;'.</p>","a":[{"id":1677357,"option":"1","correct":false},{"id":1677358,"option":"2","correct":false},{"id":1677359,"option":"3","correct":true},{"id":1677360,"option":"4","correct":false}]},{"q":"<p>You have a Data Factory and an Azure function app running on a private endpoint. You are trying to run a pipeline that interacts with the function app using three different methods out of which</p>\n\n<ol>\n\t<li>One method returns the error \"Bad Request\"</li>\n\t<li>The other two methods return \"103 Error Forbidden\"</li>\n</ol>\n\n<p>Which of these resolutions can you adopt to fix this problem?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>Apply the correct monitoring filters</p>\n\n<p><strong>2.</strong></p>\n\n<p>Run your pipelines at different trigger times</p>\n\n<p><strong>3.</strong></p>\n\n<p>Create a PrivateLinkService endpoint and provide your function app's DNS</p>\n\n<p><strong>4.</strong></p>\n\n<p>Create a new integration runtime, and split your pipelines across multiple integration runtimes</p>","a":[{"id":1677353,"option":"1","correct":false},{"id":1677354,"option":"2","correct":false},{"id":1677355,"option":"3","correct":true},{"id":1677356,"option":"4","correct":false}]},{"q":"<p>Which of the following cluster sizes is not available for compute optimized Spark clusters in Data factory?</p>","a":[{"id":1677345,"option":"4 driver nodes and 4 worker nodes","correct":true},{"id":1677346,"option":"16 driver nodes and 32 worker nodes","correct":false},{"id":1677347,"option":"16 driver nodes and 16 worker nodes","correct":false},{"id":1677348,"option":"16 driver nodes and 64 worker nodes","correct":false}]},{"q":"<p>Consider the graphical node monitoring view of a Data Factory transformation given alongside. What do the green circle icons in the respective transformation represent?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>They show the number of success and failed rows</p>\n\n<p><strong>2.</strong></p>\n\n<p>They show partition stats, column counts, skewness, and kurtosis</p>\n\n<p><strong>3.</strong></p>\n\n<p>They represent the codes that were executed together on the cluster</p>\n\n<p><strong>4.</strong></p>\n\n<p>They represent the count of the number of sinks that data is flowing into</p>","a":[{"id":1677333,"option":"1","correct":false},{"id":1677334,"option":"2","correct":false},{"id":1677335,"option":"3","correct":false},{"id":1677336,"option":"4","correct":true}]},{"q":"<p>You want to generate a quick Select transformation on a column when debugging in Data factory. Which of these options should you select from the data preview toolbar to do so?</p>","a":[{"id":1677329,"option":"Typecast","correct":false},{"id":1677330,"option":"Modify","correct":false},{"id":1677331,"option":"Statistics","correct":false},{"id":1677332,"option":"Remove","correct":true}]},{"q":"<p>Which of the given keys is Azure Data Factory <em>NOT</em> composed of?</p>\n\n<p><strong>Keys</strong></p>\n\n<ol>\n\t<li>Data Flows</li>\n\t<li>Integration Runtimes</li>\n\t<li>API</li>\n\t<li>Pipelines</li>\n</ol>","a":[{"id":1677261,"option":"Only 1","correct":false},{"id":1677262,"option":"Only 2","correct":false},{"id":1677263,"option":"Only 3","correct":true},{"id":1677264,"option":"Only 4","correct":false}]},{"q":"<p>What can be used to transform data collected in centralized data stored in the cloud?</p>","a":[{"id":1677253,"option":"ADF mapping clusters","correct":false},{"id":1677254,"option":"ADF mapping data flows","correct":true},{"id":1677255,"option":"ADF clusters","correct":false},{"id":1677256,"option":"ADF spark clusters","correct":false}]},{"q":"<p>Which of the following is <em>NOT</em><strong> </strong>true about Azure Data Factory?</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>Data Factory contains a series of interconnected systems that provide a complete end-to-end platform for data engineers</li>\n\t<li>Data Factory is a cloud-based ETL and data integration service that allows you to create data-driven workflows</li>\n</ol>","a":[{"id":1677245,"option":"Only 1","correct":false},{"id":1677246,"option":"Only 2","correct":false},{"id":1677247,"option":"Both 1 and 2","correct":false},{"id":1677248,"option":"None of these","correct":true}]},{"q":"<p>What might be a possible cause for the error code: GenericIssues to pop up in your Data Factory portal during the execution of a SQL Server Integration Services (SSIS) package?</p>","a":[{"id":1688633,"option":"Transient network issue","correct":false},{"id":1688634,"option":"Test connectivity timed out","correct":false},{"id":1688635,"option":"There is a problem with your custom DNS","correct":false},{"id":1688636,"option":"The test connection encountered a general temporary problem","correct":true}]},{"q":"<p>You have three pipelines in Data Factory that execute at 10:00 AM, 12:00 PM, and 2:00 PM. What will the exact number of pipeline runs and pipeline run IDs be for all of the given three pipelines?</p>","a":[{"id":1677445,"option":"One pipeline run and One unique pipeline run ID","correct":false},{"id":1677446,"option":"Three pipeline runs and Three unique pipeline run IDs","correct":true},{"id":1677447,"option":"One pipeline run and Three unique pipeline run IDs","correct":false},{"id":1677448,"option":"Three pipeline runs and One unique pipeline run ID","correct":false}]},{"q":"<p>Assume that the trigger is not a tumbling window trigger. What type of relationship is shared between a pipeline and a trigger in Azure Data Factory? </p>","a":[{"id":1678125,"option":"One-to-one relationship","correct":false},{"id":1678126,"option":"One-to-many relationship","correct":false},{"id":1678127,"option":"Many-to-many relationship","correct":true},{"id":1678128,"option":"Self-joining relationship","correct":false}]},{"q":"<p>Which of the following activities is supported by Azure Data Factory?<br>\n1. Data movement activity<br>\n2. Data transformation activity<br>\n3. Control activity</p>","a":[{"id":1678105,"option":"1 and 2","correct":false},{"id":1678106,"option":"2 and 3","correct":false},{"id":1678107,"option":"1 and 3","correct":false},{"id":1678108,"option":"All of these","correct":true}]},{"q":"<p>In which of the given scenarios can linked services be used in Azure Data Factory?</p>\n\n<p><strong>Scenarios</strong></p>\n\n<ol>\n\t<li>To represent a compute resource that can host the execution of an activity.</li>\n\t<li>To represent the unit of processing that determines when a pipeline execution needs to be kicked off.</li>\n</ol>","a":[{"id":1677277,"option":"Only 1","correct":true},{"id":1677278,"option":"Only 2","correct":false},{"id":1677279,"option":"Both 1 and 2","correct":false},{"id":1677280,"option":"None of these","correct":false}]},{"q":"<p>You have built-up a reusable library of data transformation routines and want to execute those processes in a scaled-out manner. What can you do to achieve this?</p>","a":[{"id":1677269,"option":"Use ADF clusters","correct":false},{"id":1677270,"option":"Use ADF Pipelines","correct":true},{"id":1677271,"option":"Use ADF routines","correct":false},{"id":1677272,"option":"Use ADF Copy activity ","correct":false}]},{"q":"<p>Which of these are built-in supports for monitoring pipelines in Azure Data Factory?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Azure Monitor</li>\n\t<li>Azure Monitor logs</li>\n\t<li>PowerShell</li>\n</ol>","a":[{"id":1677257,"option":"Only 1 and 2","correct":false},{"id":1677258,"option":"Only 2 and 3","correct":false},{"id":1677259,"option":"Only 1 and 3","correct":false},{"id":1677260,"option":"All 1, 2 and 3","correct":true}]},{"q":"Which of the following is the fastest partitioning option for writing files for evenly distributed data in Data factory?","a":[{"id":1677349,"option":"Set Partitioning","correct":false},{"id":1677350,"option":"Use current partitioning","correct":true},{"id":1677351,"option":"Single partition","correct":false},{"id":1677352,"option":"Multiple partition","correct":false}]},{"q":"Which of these Spark clusters is not recommended by Azure Data Factory for most production workloads?","a":[{"id":1677341,"option":"Specific purpose clusters","correct":false},{"id":1677342,"option":"General purpose clusters","correct":false},{"id":1677343,"option":"Memory optimized cluster","correct":false},{"id":1677344,"option":"Compute optimized cluster","correct":true}]},{"q":"Which of the following partitioning options can be used for distributing data equally across several partitions in a Data factory transformation?","a":[{"id":1677337,"option":"Round robin","correct":true},{"id":1677338,"option":"Hash","correct":false},{"id":1677339,"option":"Dynamic range","correct":false},{"id":1677340,"option":"Fixed range","correct":false}]},{"q":"Which of the following Azure Data Factory artifact names should be unique within a data factory?","a":[{"id":1677325,"option":"Data factory","correct":false},{"id":1677326,"option":"Resource Group","correct":false},{"id":1677327,"option":"Integration Runtime","correct":true},{"id":1677328,"option":"Data flow transformations","correct":false}]},{"q":"Which of the following represents a processing step in a Azure Data Factory Pipeline?","a":[{"id":1677273,"option":"Activities","correct":true},{"id":1677274,"option":"Datasets","correct":false},{"id":1677275,"option":"Linked services","correct":false},{"id":1677276,"option":"Dataflows","correct":false}]},{"q":"How can the activities in a Azure Data Factory pipeline operate?","a":[{"id":1677265,"option":"Sequentially","correct":false},{"id":1677266,"option":"Independently","correct":false},{"id":1677267,"option":"Both 1 and 2","correct":true},{"id":1677268,"option":"None of these","correct":false}]},{"q":"Which of the following can be used to move data from cloud source data stores to a centralised data store in the cloud using Data Factory?","a":[{"id":1677249,"option":"Copy Activity","correct":true},{"id":1677250,"option":"Azure Blob","correct":false},{"id":1677251,"option":"Data pipeline","correct":false},{"id":1677252,"option":"Azure Data Lake ","correct":false}]}]