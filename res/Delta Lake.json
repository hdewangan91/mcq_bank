[{"q":"<p>In Databricks, you are working on the Delta Lake from the Databricks Workspace. Delta Lake automatically validates that the schema of the DataFrame being written is compatible with the schema of the table. Now, if you are required to determine whether a write from a DataFrame to a table is compatible by using the Delta Lake, then which of the following rules in this scenario are correct:<br>\n<strong>Rules:</strong></p>\n\n<ol>\n\t<li>All DataFrame columns must exist in the target table. If there are columns in the DataFrame not present in the table, an exception is raised.</li>\n\t<li>DataFrame column data types must not match the column data types in the target table. If they match, an exception is raised.</li>\n\t<li>DataFrame column names cannot differ only by case.</li>\n</ol>","a":[{"id":1544741,"option":"1 and 2","correct":false},{"id":1544742,"option":"2 and 3","correct":false},{"id":1544743,"option":"1 and 3","correct":true},{"id":1544744,"option":"All of these","correct":false}]},{"q":"<p>In Databricks, you are working on the table operations in the Delta Lake from the Databricks Workspace. You are given a Spark data frame that contains new data for books with <strong>bookID</strong> from a table named <strong>library.</strong> Some of these books may already present on the <strong>library</strong> table.<br>\nNow, you are required to merge the new data into the <strong>library</strong> table. For this, you are required to perform the following two operations:</p>\n\n<ol>\n\t<li>If the <strong>bookID </strong>is already present, then update the matching rows with the respect to the new table named updates.</li>\n\t<li>If the <strong>bookID</strong> is not available, then insert as new rows.</li>\n</ol>\n\n<p>Which of the following SQL queries must be executed to perform these actions in this scenario:<br>\n<br>\n<strong>SQL queries:</strong><br>\n1.</p>\n\n<pre class=\"prettyprint\"><code>Insert BOOKS into spark.read.format(\"delta\").load(\"/mnt/delta/books\")</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>MERGE INTO library\nUSING updates\nON bookID\nWHEN MATCHED THEN\n    UPDATE SET\n    library.data = updates.data, spark.read.format\nWHEN NOT MATCHED\n    THEN INSERT (date, bookID, data) VALUES (date, bookID, data)</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>MERGE INTO library\nUSING updates\nON library.bookID = updates.bookID\nWHEN MATCHED THEN\n    UPDATE SET\n    library.data = updates.data\nWHEN NOT MATCHED\n    THEN INSERT (date, bookID, data) VALUES (date, bookID, data)</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>Insert BOOKS into spark.read.format(\"delta\").load(\"/mnt/delta/books\") UPDATE SET;</code></pre>\n\n<p> </p>","a":[{"id":1544721,"option":"1","correct":false},{"id":1544722,"option":"2","correct":false},{"id":1544723,"option":"3","correct":true},{"id":1544724,"option":"4","correct":false}]},{"q":"<p>In Databricks, you are required to perform the table batch reads and writes operations in Delta Lake from your Databricks Workspace. You have created a table named <strong>myhack_table</strong> that contains various information about the customers of your organization. If you are required to retrieve the number of customers that are added to the organization's database over the last year, then which of the following SQL queries can be used to perform this action in this scenario:<br>\n<br>\n<strong>SQL queries</strong><br>\n1.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), ON source.emailId))</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), ON source.emailId, 365))</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), 7))</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>SELECT count(distinct emailId) - ( SELECT count(distinct emailId) FROM myhack_table TIMESTAMP AS OF date_sub(current_date(), 365))</code></pre>\n\n<p> </p>","a":[{"id":1544717,"option":"1","correct":false},{"id":1544718,"option":"2","correct":false},{"id":1544719,"option":"3","correct":false},{"id":1544720,"option":"4","correct":true}]},{"q":"<p>In Databricks, you are working on the Delta Lake and Delta Engine. If you are required to create a table named organization that contains four columns named <strong>id (long)</strong>, <strong>date (string)</strong>, <strong>location (string)</strong>, and <strong>company (string)</strong>, then which of the SQL queries must be executed such that only the companies with the dates post <strong>01/01/2001</strong> should be inserted into the table:<br>\n<strong>Queries</strong><br>\n1.</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE organization(\n    id LONG NOT NULL,\n    date STRING,\n    location STRING,\n    description STRING, ADD CONSTRAINT dateWithinRange CHECK (date &gt; '2001-01-01');\n) USING DELTA;</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>DELTA TABLE organization(\n    id LONG NOT NULL,\n    date STRING,\n    location STRING,\n    description STRING, DELTA ADD CONSTRAINT dateWithinRange CHECK (date &gt; '2001-01-01');\n);</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>CREATE TABLE organization(\n    id LONG NOT NULL,\n    date STRING,\n    location STRING,\n    description STRING) USING DELTA; ALTER table organization ADD CONSTRAINT dateWithinRange CHECK (date &gt; '2001-01-01');</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>DELTA TABLE organization(\n    id LONG NOT NULL,\n    date STRING,\n    location STRING,\n    description STRING, CONSTRAINT dateWithinRange CHECK (date &gt; '2001-01-01');\n);</code></pre>\n\n<p> </p>","a":[{"id":1544669,"option":"1","correct":false},{"id":1544670,"option":"2","correct":false},{"id":1544671,"option":"3","correct":true},{"id":1544672,"option":"4","correct":false}]},{"q":"<p>In Databricks, you are working on the Databricks Workspace. Which of the following allows you to query an older snapshot of a Delta table?</p>","a":[{"id":1544713,"option":"Delta Lake Snapshot scheduler","correct":false},{"id":1544714,"option":"Delta Lake Time Travel","correct":true},{"id":1544715,"option":"Delta Lake Version controllers","correct":false},{"id":1544716,"option":"None of these","correct":false}]},{"q":"<p>In Databricks. you are working on the Delta Lake from Databricks Workspace. If you are required to load data from a file location into a Delta table, then which of the following SQL commands is used to perform this action in this scenario?</p>","a":[{"id":1544709,"option":"CREATE AS LOAD","correct":false},{"id":1544710,"option":"CONVERT INTO","correct":false},{"id":1544711,"option":"COPY INTO","correct":true},{"id":1544712,"option":"CREATE USING AS ","correct":false}]}]