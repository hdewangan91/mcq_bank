[{"q":"<p>In Azure, for which of the following items would you have to select values when creating a Data Lake Analytics account?</p>\n\n<p><strong>Items</strong></p>\n\n<ol>\n\t<li>Subscription</li>\n\t<li>Resource Group</li>\n\t<li>Server Address</li>\n\t<li>Location</li>\n</ol>","a":[{"id":1688645,"option":"1 and 3","correct":false},{"id":1688646,"option":"1, 3, and 4","correct":false},{"id":1688647,"option":"2 and 3","correct":false},{"id":1688648,"option":"1, 2, and 4","correct":true}]},{"q":"<p>Which of the following benefits would you get if you use the Azure Data Lake Analytics to transform your data:</p>\n\n<p><strong>Benefits</strong></p>\n\n<ol>\n\t<li>Dynamic Scaling</li>\n\t<li>Faster development</li>\n\t<li>Use of U-SQL</li>\n\t<li>Integration with existing IT investments</li>\n</ol>","a":[{"id":1688609,"option":"3 and 4","correct":false},{"id":1688610,"option":"2, 3, and 4","correct":false},{"id":1688611,"option":"1 and 3","correct":false},{"id":1688612,"option":"All of these","correct":true}]},{"q":"<p>When creating an Azure Data Lake Storage Gen2 account, you want to configure this account to be able to process analytical data workloads for best performance. What can you do to achieve this?</p>","a":[{"id":1688605,"option":"On the Basic tab, set the Performance option to Standard","correct":false},{"id":1688606,"option":"On the Basic tab, set the Performance option to ON","correct":false},{"id":1688607,"option":"On the Advanced tab, set the Hierarchical Namspace to Enable","correct":true},{"id":1688608,"option":"On the Advance tab, set the Hierarchical Namspace to Disable","correct":false}]},{"q":"<p>In Azure, you are setting up the instance details in the basic tab when creating a Data Lake Storage Gen2 store. In the given context, what would you select from the Account kind drop-down menu if your default deployment model is Resource Manager?</p>","a":[{"id":1688601,"option":"StorageV2 (general-purpose v1)","correct":false},{"id":1688602,"option":"StorageV1 (general-purpose v2)","correct":false},{"id":1688603,"option":"StorageV2 (general-purpose v2)","correct":true},{"id":1688604,"option":"StorageV1 (general-purpose v1)","correct":false}]},{"q":"<p>In Azure, you have chosen to use the default deployment model i.e. Resource Manager, when creating the Data Lake Storage Gen2. In the given context, which of the following options would you select?</p>","a":[{"id":1688597,"option":"set Access Tier as 'Hot'","correct":true},{"id":1688598,"option":"set Performance as 'Premium'","correct":false},{"id":1688599,"option":"set Access Tier as 'Cool'","correct":false},{"id":1688600,"option":"set Performance as 'Basic'","correct":false}]},{"q":"<p>Which of the following can be set using the basic tab when creating an Azure Data Lake Storage Gen2 account:</p>\n\n<ol>\n\t<li>Performance</li>\n\t<li>Account Kind</li>\n\t<li>Replication</li>\n\t<li>Access Tier</li>\n</ol>","a":[{"id":1688593,"option":"1, 2, and 4","correct":false},{"id":1688594,"option":"All of these","correct":true},{"id":1688595,"option":"1 and 3","correct":false},{"id":1688596,"option":"2, 3, and 4","correct":false}]},{"q":"<p>You want to enable the \"secure transfer required\" setting when creating an Azure Data Lake Storage Gen2 account using the portal. Which of these tabs can be used to achieve this?</p>","a":[{"id":1688589,"option":"In tags tab","correct":false},{"id":1688590,"option":"In Review + create tab","correct":false},{"id":1688591,"option":"In Advance tab","correct":true},{"id":1688592,"option":"In Basic tab","correct":false}]},{"q":"<p>You are creating a Data Lake Storage Gen2 data store that requires Azure Storage account with the Hierarchical namespace enabled.<br>\nIn which of these tabs would you find the option to enable this?</p>","a":[{"id":1688585,"option":"In Basic tab","correct":false},{"id":1688586,"option":"In Advance tab","correct":true},{"id":1688587,"option":"In tags tab","correct":false},{"id":1688588,"option":"In Review + create tab","correct":false}]},{"q":"<p>How can permissions for an item be inherited in a Data Lake Storage Gen2 that uses the POSIX style model?</p>","a":[{"id":1688581,"option":"No need to take permission","correct":false},{"id":1688582,"option":"From the Permission repository","correct":false},{"id":1688583,"option":"From the Parent item","correct":false},{"id":1688584,"option":"From the item itself","correct":true}]},{"q":"<p>In a scenario from Azure Data Lake, instead of repeatedly using EXTRACT to read from the same source file, you want to take a different approach.<br>\nIf you have a single query expression, which of the following will be most suited for encapsulating that expression:</p>\n\n<ol>\n\t<li>Table-valued function</li>\n\t<li>U-SQL View</li>\n</ol>","a":[{"id":1688573,"option":"Only 1","correct":false},{"id":1688574,"option":"Only 2","correct":true},{"id":1688575,"option":"Both 1 and 2 are equally suited","correct":false},{"id":1688576,"option":"Using either 1 or 2 in such a scenario will transform the data","correct":false}]},{"q":"<p>In Azure Data Lake, you observe that the outputter makes use of the System.IO.StreamWriter to write the output data to a file.<br>\nIn the given context, the stream parameter should be set to which of the following when doing so?</p>","a":[{"id":1688569,"option":"output.BaseStream","correct":true},{"id":1688570,"option":"output.IUnstructuredWriter ","correct":false},{"id":1688571,"option":"output.StreamWriter","correct":false},{"id":1688572,"option":"output.IOutputter","correct":false}]},{"q":"<p>In Azure Data Lake, you are using a user-defined outputter to write data in a custom-defined format. You are also using the output object to set output data to the target file. Which of these can be called to determine the individual column names when doing so?</p>","a":[{"id":1688565,"option":"row.Get<>()","correct":false},{"id":1688566,"option":"row.schema","correct":true},{"id":1688567,"option":"row.Col","correct":false},{"id":1688568,"option":"col.Get<>(\"row Name\")","correct":false}]},{"q":"<p>In Azure Data Lake, you are using the Partition_Label to specify an explicit partition into which the data needs to be inserted. In which of the following cases will an error be raised if the target table is not partitioned:</p>\n\n<p><strong>Cases</strong></p>\n\n<ol>\n\t<li>Integrity_Clause is specified</li>\n\t<li>Partition_Label is specified</li>\n</ol>","a":[{"id":1688561,"option":"Only 1","correct":false},{"id":1688562,"option":"Only 2","correct":false},{"id":1688563,"option":"Both 1 and 2","correct":true},{"id":1688564,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>In Azure Data Lake, you are using the Job Heat Map Display to see the time and throughput heat map of a job.<br>\nIn the given context, how can you see the duration of time it would take to execute all work in the stage with only 1 vertex?</p>","a":[{"id":1688557,"option":"Using Compute time","correct":true},{"id":1688558,"option":"Using Average execution time per node","correct":false},{"id":1688559,"option":"Using Total execution time","correct":false},{"id":1688560,"option":"Using Input/Output time","correct":false}]},{"q":"<p>In Azure Data Lake, you observe that the data you are using does not require quoting when using the outputter. What will you do to achieve faster data processing in this scenario?</p>","a":[{"id":1688553,"option":"Set the quoting parameter to true","correct":false},{"id":1688554,"option":"Set the quoting parameter to false","correct":true},{"id":1688555,"option":"Set the quoting parameter to null","correct":false},{"id":1688556,"option":"Set the quoting parameter to 0","correct":false}]},{"q":"<p>In Azure Data Lake, which of the following usages of the User-Defined Types(UDT) will raise an error if used without serialization:</p>\n\n<ol>\n\t<li>Used in EXTRACTOR</li>\n\t<li>Used in OUTPUTTER</li>\n</ol>","a":[{"id":1688549,"option":"Only 1","correct":false},{"id":1688550,"option":"Only 2","correct":false},{"id":1688551,"option":"Both 1 and 2","correct":true},{"id":1688552,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You are processing data using Azure Data lake storage. In which of these phases of this process would you identify the technology and processes that are used to acquire the source data?</p>","a":[{"id":1686953,"option":"Store","correct":false},{"id":1686954,"option":"Prep and train","correct":false},{"id":1686955,"option":"Model and Serve","correct":false},{"id":1686956,"option":"Ingestion","correct":true}]},{"q":"<p>You are using blob APIs and Data Lake Storage Gen2 APIs to operate on the same data. Which of these delimiters can you use while performing the List Blobs operation in the given context?</p>","a":[{"id":1686949,"option":"Backward slash (\\)","correct":false},{"id":1686950,"option":"Forward slash (/)","correct":true},{"id":1686951,"option":"Semicolon (;)","correct":false},{"id":1686952,"option":"Double Quote (\")","correct":false}]},{"q":"<p>You want to load data into Azure Synapse Analytics using the COPY statement. If your staging Data Lake Storage Gen2 is configured with an Azure Virtual Network endpoint then which of these types of authentication would you use to do so?</p>","a":[{"id":1686945,"option":"Managed identity authentication","correct":true},{"id":1686946,"option":"Account key authentication","correct":false},{"id":1686947,"option":"Service principal authentication","correct":false},{"id":1686948,"option":"Lookup activity property","correct":false}]},{"q":"<p>A set of data is transferred from an Azure Data Lake Storage account using Azure Data Factory. This data is then loaded into a data warehouse in Azure Synapse using Azure Polybase. Also, the data in the storage account is accessed via a virtual network service endpoint.<br>\nWhich of the following authentication method will you use to access the data in storage account?</p>","a":[{"id":1678341,"option":"Managed identity authentication","correct":true},{"id":1678342,"option":"Account key authentication","correct":false},{"id":1678343,"option":"Service principal authentication","correct":false},{"id":1678344,"option":"Shared access key authentication","correct":false}]},{"q":"<p>For which of the following items would you need to set values for, while creating a Data Lake Analytics account?</p>\n\n<p>a) Subscription<br>\nb) Resource Group<br>\nc) Server Address<br>\nd) Location</p>","a":[{"id":1678173,"option":"(a) and (c)","correct":false},{"id":1678174,"option":"(a), (c) and (d)","correct":false},{"id":1678175,"option":"(b) and (c)","correct":false},{"id":1678176,"option":"(a), (b) and (d)","correct":true}]},{"q":"<p>Assume that you have attached the<strong> </strong><em>Azure Data Lake Gen 2 storage</em> to a workspace for configuring the Dataflow Storage. Now, you want to detach Azure Data Lake Gen 2 storage from the workspace. How would you be able to do this?</p>","a":[{"id":1678169,"option":"First ensure the firewall is disconnected then select Disconnect in workspace settings","correct":false},{"id":1678170,"option":"First ensure no dataflows in the workspace are deleted then Disconnect","correct":false},{"id":1678171,"option":"First ensure all dataflows in the workspace are deleted then Disconnect","correct":true},{"id":1678172,"option":"select Disconnect in the workspace settings","correct":false}]},{"q":"<p>You are working with big data in Data Lake Storage Gen2. When doing so, you have decided to use <em>Azure</em> Active Directory security groups instead of assigning individual users to directories and files. What is the benefit of doing so?</p>","a":[{"id":1677941,"option":"Adding or removing users from the group doesn’t require any updates to Data Lake Storage Gen2","correct":false},{"id":1677942,"option":"It ensures that you don't exceed the maximum number of access control entries per access control list","correct":false},{"id":1677943,"option":"It enables you to access to data in a storage account with hierarchical namespace enabled,","correct":true},{"id":1677944,"option":"Both 1 and 2","correct":false}]},{"q":"<p>Which of these are valid ways to use POSIX access controls for <em>Azure</em> Active Directory users offered by Azure Data Lake Storage Gen2?</p>","a":[{"id":1677937,"option":"Setting the access controls to existing files and directories","correct":false},{"id":1677938,"option":"Create default permissions that can be automatically applied to new files","correct":false},{"id":1677939,"option":"Both 1 and 2","correct":true},{"id":1677940,"option":"None of these","correct":false}]},{"q":"<p>Which of the following tools would you use for monitoring and auditing when working with <em>Azure</em><strong> </strong>Data Lake Analytics:</p>","a":[{"id":1677657,"option":"Azure COSMOS DB","correct":false},{"id":1677658,"option":"On the Advance tab, set the Hierarchical Namespace to Disable","correct":false},{"id":1677659,"option":"Azure Kubernetes Service (AKS)","correct":false},{"id":1677660,"option":"No tools needed. It comes with Built-in monitoring and auditing","correct":true}]},{"q":"<p>You have a requirement wherein you have to use a user-defined extractor that is capable of reading CSV-like data into SQL.MAP&lt;string,string&gt; columns and SQL.ARRAY&lt;int&gt; columns. Which of the following User-Defined Extractor can you use to define an extractor in this scenario?</p>","a":[{"id":1677633,"option":"SampleExtractor","correct":false},{"id":1677634,"option":"DriverExtractor","correct":true},{"id":1677635,"option":"FlexExtractor","correct":false},{"id":1677636,"option":"Both DriverExtractor and FlexExtractor","correct":false}]},{"q":"<p>You are using the user-defined outputters to write data in a custom-defined format in <em>Azure</em> Data Lake. During the process, you flushed the data buffer to the file after each row iteration. Which of the following must you do along with flushing the data?<br>\n<br>\n1. Use StreamWriter object with the Disposable attribute enabled<br>\n2. Use StreamWriter object with the using keyword</p>","a":[{"id":1677629,"option":"Only1 ","correct":false},{"id":1677630,"option":"Only 2","correct":false},{"id":1677631,"option":"Both 1 and 2","correct":true},{"id":1677632,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You have set the SqlUserDefinedOutputter attribute to true when using user-defined outputters to write data in a custom-defined format in <em>Azure</em> Data Lake. What will happen as a result of this?</p>","a":[{"id":1677625,"option":"The Outputter will only deal with atomic output files","correct":true},{"id":1677626,"option":"The Outputter will only deal with split files","correct":false},{"id":1677627,"option":"The Outputter will only deal with distributed files","correct":false},{"id":1677628,"option":"The Outputter will deal with all atomic / split / distributed files","correct":false}]},{"q":"<p>You are using the EXTRACT expression to extract data from a compressed file of size more than 4GB. What will happen once you apply the EXTRACT expression to the file during the extraction in <em>Azure</em> Data Lake?</p>","a":[{"id":1677621,"option":"The expression will compile successfully without any errors","correct":false},{"id":1677622,"option":"The EXTRACT will apply the decompression utility on the compressed file","correct":false},{"id":1677623,"option":"An error will be raised during the compilation of the job","correct":true},{"id":1677624,"option":"A limited amount of data will be fetched from the file","correct":false}]},{"q":"<p>You are using the catalog views to get the information used by U-SQL. Which of the following will the catalog views contain during the processing of the obtained information?<br>\n<strong>Options</strong><br>\n 1. Objects created as part of the same script<br>\n 2. Objects that the user submitting the query has the right to see</p>","a":[{"id":1677617,"option":"Only 1","correct":false},{"id":1677618,"option":"Only 2","correct":true},{"id":1677619,"option":"Both 1 and 2","correct":false},{"id":1677620,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You want to provide access to some resources for a limited period of time in your Azure Data lake storage Gen 2 account. Which of the following will you use to grant this permission?</p>","a":[{"id":1677549,"option":"Access keys for storage account","correct":false},{"id":1677550,"option":"Azure AD users","correct":false},{"id":1677551,"option":"A shared access signature","correct":true},{"id":1677552,"option":"Role based access control","correct":false}]},{"q":"<p>You want to write to a folder in the sink transformation when using the Azure Data Lake Storage Gen2 as Sink type. In the given scenario, which of these tabs lets you manage how the files get written?</p>","a":[{"id":1677321,"option":"Inspect","correct":false},{"id":1677322,"option":"Optimize","correct":false},{"id":1677323,"option":"Mapping","correct":false},{"id":1677324,"option":"Settings","correct":true}]},{"q":"<p>Which of the following options can be used if you want to copy data from Azure Data Lake Storage Gen2?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Copy from the given path specified in the dataset</li>\n\t<li>Wildcard filter against folder path or file name</li>\n\t<li>Copy the files defined in a given text file as file set</li>\n\t<li>Copy the given text specified in the files to another file</li>\n</ol>","a":[{"id":1677313,"option":"2 and 4","correct":false},{"id":1677314,"option":"3 and 4","correct":false},{"id":1677315,"option":"1, 2 and 3 ","correct":true},{"id":1677316,"option":"1, 2 and 4","correct":false}]},{"q":"<p>Assume that you have provided security to the data in Azure Data Lake Storage Gen2. Which of the technologies/utilities given alongside can you use for configuring this security?</p>\n\n<p><strong>Technologies</strong></p>\n\n<ol>\n\t<li> Hive</li>\n\t<li> Spark</li>\n\t<li> Azure Storage Explorer</li>\n</ol>","a":[{"id":1677305,"option":"1 and 3","correct":false},{"id":1677306,"option":"1","correct":false},{"id":1677307,"option":"3","correct":false},{"id":1677308,"option":"All of these","correct":true}]},{"q":"<p>At which of these levels should you set the permission for the data stored within a data lake?<br>\n<strong>Levels</strong></p>\n\n<ol>\n\t<li>Directory level</li>\n\t<li>Node level</li>\n\t<li>File level</li>\n\t<li>Storage level</li>\n</ol>","a":[{"id":1677301,"option":"3","correct":false},{"id":1677302,"option":"4","correct":false},{"id":1677303,"option":"2","correct":false},{"id":1677304,"option":"1","correct":true}]},{"q":"<p>Which of the following permissions can you get for data stored in a data lake by using Azure Data Lake Storage Gen2?</p>\n\n<p><strong>Permissions</strong></p>\n\n<ol>\n\t<li>Access control lists (ACLs)</li>\n\t<li>Portable Operating System Interface (POSIX)</li>\n\t<li>AdlCopy</li>\n\t<li>DistCp</li>\n</ol>","a":[{"id":1677297,"option":"1, 2 and 4","correct":false},{"id":1677298,"option":"1, 2 and 3","correct":false},{"id":1677299,"option":"1 and 2","correct":true},{"id":1677300,"option":"1","correct":false}]},{"q":"<p>Assume that the managed table in which you are going to insert data is specified by an identifier. In which of the following scenarios will the data be inserted into the table in the given context?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1</strong></p>\n\n<p>If the Identifier is a simple identifier</p>\n\n<p><strong>2</strong></p>\n\n<p>If the Identifier is a two-part identifier</p>\n\n<p><strong>3</strong></p>\n\n<p>If the Identifier is a three-part identifier</p>\n\n<p><strong>4</strong></p>\n\n<p>If the Identifier is a four-part identifier</p>","a":[{"id":1677281,"option":"1","correct":false},{"id":1677282,"option":"2","correct":false},{"id":1677283,"option":"3","correct":true},{"id":1677284,"option":"4","correct":false}]},{"q":"<p>What should you integrate Azure Data Lake Analytics with to use it for user management and permissions?</p>","a":[{"id":1688613,"option":"Active Directory","correct":true},{"id":1688614,"option":"Access Control","correct":false},{"id":1688615,"option":"YARN","correct":false},{"id":1688616,"option":"Spark","correct":false}]},{"q":"<p>How can you set up the storage account as an Azure Data Lake Storage Gen2 account if you are performing analytics on the data?</p>","a":[{"id":1688577,"option":"by setting SQL Pools","correct":false},{"id":1688578,"option":"by setting the Hierarchical Namespace option to Disabled","correct":false},{"id":1688579,"option":"by setting the Hierarchical Namespace option to Enabled","correct":true},{"id":1688580,"option":"by using Azure RBAC","correct":false}]},{"q":"<p>You are using Hadoop Compatible Access to store the data in one place and access it without moving the data between environments. Which of the compute technologies given alongside can you use to do so?</p>\n\n<p><strong>Technologies</strong></p>\n\n<ol>\n\t<li>Azure Databricks</li>\n\t<li>Azure HDInsight</li>\n\t<li>Azure Synapse Analytics</li>\n\t<li>Azure Data Analytics</li>\n</ol>","a":[{"id":1677293,"option":"1","correct":false},{"id":1677294,"option":"2, 3 and 4","correct":false},{"id":1677295,"option":"1, 2 and 4","correct":false},{"id":1677296,"option":"1, 2 and 3","correct":true}]},{"q":"<p>You observe that Data Lake Storage Gen2 takes advantage of the Azure Blob replication models that provide data redundancy in a single data center to a secondary region. Which of the following do you think is used to achieve this?</p>","a":[{"id":1677317,"option":"Geo-redundant storage (GRS)","correct":true},{"id":1677318,"option":"Remote-redundant storage (RRS)","correct":false},{"id":1677319,"option":"hierarchy of directories","correct":false},{"id":1677320,"option":"locally redundant storage (LRS)","correct":false}]},{"q":"<p>Which of these are valid benefits provided by Azure Data Lake storage while processing stored data?<br>\n<strong>Benefits</strong></p>\n\n<ol>\n\t<li>Less computational resources</li>\n\t<li>Reduced time</li>\n\t<li>Reduced cost</li>\n\t<li>Easier navigation</li>\n</ol>","a":[{"id":1677309,"option":"1, 2 and 4","correct":false},{"id":1677310,"option":"All the benefits","correct":true},{"id":1677311,"option":"1 and 3","correct":false},{"id":1677312,"option":"2, 3 and 4","correct":false}]},{"q":"<p>You want the data to work in the same way as it is when stored in a Hadoop Distributed File System when working on Azure Data Lake.<br>\nWhich Data lake would you use in order to achieve this?</p>","a":[{"id":1677289,"option":"Data Lake Storage Gen2","correct":true},{"id":1677290,"option":"Data Lake Storage Gen1","correct":false},{"id":1677291,"option":"dedicated SQL pools","correct":false},{"id":1677292,"option":"U-SQL jobs","correct":false}]},{"q":"How does Azure Data Lake help you with quickly identifying insights into your data?","a":[{"id":1677285,"option":"using U-SQL jobs","correct":false},{"id":1677286,"option":"combines a file system with a storage platform","correct":true},{"id":1677287,"option":"integrates with Visual Studio","correct":false},{"id":1677288,"option":"Using appropriate sorting algorithms","correct":false}]},{"q":"<p>In Azure Data Lake Analytics, you are working with Azure PowerShell to manage this Data Lake Analytics. You have logged into Azure with your subscription ID by implementing the following PowerShell cmdlet. Now, you have observed that this cmdlet always prompts for the user credentials. You wanted to avoid these prompts. If you are required to load the login session information from the path <strong>D:\\profile.json</strong>, then which of these cmdlets can be implemented to perform this action in this scenario:</p>\n\n<p><strong>PowerShell cmdlet used to login into Azure</strong></p>\n\n<pre class=\"prettyprint\"><code>Connect-AzAccount -SubscriptionId $subId</code></pre>\n\n<p><strong>cmdlets</strong></p>\n\n<ol>\n\t<li>Connect-AzAccount -path D:\\profile.json</li>\n\t<li>Save-AzAccounts -path D:\\profile.json</li>\n\t<li>Select-AzAccounts -path D:\\profile.json</li>\n</ol>\n\n<p> </p>","a":[{"id":1444026,"option":"1","correct":false},{"id":1444027,"option":"2","correct":false},{"id":1444028,"option":"3","correct":true},{"id":1444029,"option":"None of these","correct":false}]}]