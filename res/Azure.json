[{"q":"<p>How can you build no-ETL HTAP solutions using Azure Synapse Link when working with column-oriented Azure Cosmos DB analytical store?</p>","a":[{"id":1688877,"option":"By directly linking to Azure Cosmos DB analytical store from Azure Synapse Analytics","correct":true},{"id":1688878,"option":"Create aggregations and sequential scans of selected fields","correct":false},{"id":1688879,"option":"Both 1 and 2","correct":false},{"id":1688880,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You want to dynamically scale your database when using an Azure SQL single database. You also want to define the maximum amount of resources that will be assigned to each database for scalability. Which of the following will you use to define the maximum amount of resources in the given scenario?</p>","a":[{"id":1688873,"option":"DTU","correct":false},{"id":1688874,"option":"vCore","correct":false},{"id":1688875,"option":"Elastic Pools","correct":false},{"id":1688876,"option":"Either DTU or vCore","correct":true}]},{"q":"<p>You are able to login to the Azure database but you are failing to access data from any of the tables when working with Azure SQL Database. You also are unable to see any other DB objects like SP or views. The problem in the given scenario could be related to which of the following?</p>","a":[{"id":1688869,"option":"Authentication","correct":false},{"id":1688870,"option":"Authorization","correct":true},{"id":1688871,"option":"Row level security","correct":false},{"id":1688872,"option":"Dynamic data masking","correct":false}]},{"q":"<p>You have been assigned a task to make an Azure SQL database instance private enabling only applications or resources within a specific subnet to access the database resources. Which of the following security setting will you configure in the given scenario?</p>","a":[{"id":1688865,"option":"Network Security","correct":true},{"id":1688866,"option":"Authorization","correct":false},{"id":1688867,"option":"Authentication","correct":false},{"id":1688868,"option":"Transparent data encryption","correct":false}]},{"q":"<p>A company has a on-premise SQL Server instance with 16 databases out of which 4 of them require common Language Runtime (CLR) features. You want to be able to manage each of these databases separately as each of them have their own resource needs. In order to achieve this, you decide to migrate these databases to Azure using SQL commands for backup and restore. Which of the following is the most suitable deployment option to use in the given scenario?</p>","a":[{"id":1688861,"option":"Azure SQL Database managed instance","correct":true},{"id":1688862,"option":"Azure Cosmos DB with SQL API","correct":false},{"id":1688863,"option":"Azure SQL Database with an elastic pool","correct":false},{"id":1688864,"option":"Azure Cosmos DB with the Table API","correct":false}]},{"q":"<p>Your company wants to migrate an on-premise Microsoft SQL Server database to Azure. &nbsp;You are given the requirement that should be considered when migrating the database.<br>\nSERVER-1: This server hosts 2 databases. The company wants to have complete control over the underlying database server<br>\nWhich of the following will you recommend as the right technology in Azure for SERVER-1 ensuring costs are kept in check?</p>","a":[{"id":1688857,"option":"Azure SQL Database - Hyperscale","correct":false},{"id":1688858,"option":"Azure SQL Database - Elastic pools","correct":false},{"id":1688859,"option":"Azure SQL Database - Managed Instance","correct":false},{"id":1688860,"option":"SQL Database hosted on a virtual machine","correct":true}]},{"q":"<p>Your company wants to migrate an on-premises Microsoft SQL Server database to Azure SQL. You are given the requirement that should be considered when migrating the database.<br>\n<br>\nSERVER-1: This database server is hosting a single database. The company wants to ensure a simple Lift-and-shift migration for this database<br>\n<br>\nWhich of the following will you recommend as the right technology in Azure for SERVER-1 ensuring costs are kept in check?</p>","a":[{"id":1688853,"option":"Azure SQL Database - Hyperscale","correct":false},{"id":1688854,"option":"Azure SQL Database - Elastic pools","correct":false},{"id":1688855,"option":"Azure SQL Database - Managed Instance","correct":true},{"id":1688856,"option":"SQL Database hosted on a virtual machine","correct":false}]},{"q":"<p>Which of the following can be used to create more powerful application user accounts or less powerful management accounts when securing a dedicated SQL pool?</p>","a":[{"id":1688849,"option":"Use RBAC roles","correct":false},{"id":1688850,"option":"Create an Azure service principal ","correct":false},{"id":1688851,"option":"Create a database firewall rule","correct":false},{"id":1688852,"option":"Use database roles","correct":true}]},{"q":"<p>A company has set up an Azure SQL data warehouse where data would be loaded weekly from an Azure SQL database instance. You have to ensure that data engineers can only connect from their on-premise workstations to meet the security requirements for the data warehouse.<br>\nWhich of the following port must be enabled on the firewall for the data engineer's workstation in the given scenario?</p>","a":[{"id":1688845,"option":"80","correct":false},{"id":1688846,"option":"1433","correct":true},{"id":1688847,"option":"3306","correct":false},{"id":1688848,"option":"8080","correct":false}]},{"q":"<p>Your company wants to set up an Azure SQL database that will store sensitive Personally Identifiable Information (PII) data. Their objective behind this setup is to have the ability to track and store all the queries that are executed against the PII data. What should you do as a database administrator to achieve the objective behind the setup?</p>","a":[{"id":1688841,"option":"Adding classifications to the columns that contain sensitive data","correct":false},{"id":1688842,"option":"Add classifications to the columns that contain sensitive data and turn the Auditing on for the database","correct":true},{"id":1688843,"option":"Turn the Auditing on for the database","correct":false},{"id":1688844,"option":"Add classifications to all the rows of the table and turn the Auditing on for the database","correct":false}]},{"q":"<p>You have a collection of single databases with a shared set of resources such as CPU or memory. Which of the following Azure databases are you likely to be working with?</p>","a":[{"id":1688837,"option":"Single database","correct":false},{"id":1688838,"option":"Elastic Pool","correct":true},{"id":1688839,"option":"Managed Instance","correct":false},{"id":1688840,"option":"Database pool","correct":false}]},{"q":"<p>Your company is using an Azure SQL database containing tables and columns containing sensitive data. You have been asked to implement a solution to ensure that the sensitive data accessed from the columns is encrypted in transit.<br>\nWhich of the following would you use to encrypt the data in the given scenario?</p>","a":[{"id":1688833,"option":"Dynamic data masking","correct":false},{"id":1688834,"option":"Always encrypted","correct":true},{"id":1688835,"option":"Row-level security","correct":false},{"id":1688836,"option":"Transparent data encryption ","correct":false}]},{"q":"<p>A company wants to use an Azure SQL database service for some business applications that are accessing the database. In order to do so, it decides to create an Azure SQL database-managed instance. Which of the following will help the company if the database has to recover automatically in case of full or partial loss of the Azure SQL database service in the primary region?</p>","a":[{"id":1688829,"option":"Azure SQL data sync","correct":false},{"id":1688830,"option":"SQL Replication","correct":false},{"id":1688831,"option":"Failover-groups","correct":true},{"id":1688832,"option":"Active geo-replication","correct":false}]},{"q":"<p>Your company is using an on-premise Microsoft SQL Server and wants to migrate the database to Azure SQL databases. Which of the following must be used as the underlying storage type for the exported data in the given scenario?</p>","a":[{"id":1688825,"option":"Disk","correct":false},{"id":1688826,"option":"Blob","correct":true},{"id":1688827,"option":"File","correct":false},{"id":1688828,"option":"Queue","correct":false}]},{"q":"<p>Which of the following are valid monitoring and troubleshooting features provided by the Azure SQL database:</p>\n\n<ol>\n\t<li>Archive vast amounts of telemetry for a small price</li>\n\t<li>Integration of SQL Database telemetry with your custom monitoring solution or hot pipelines</li>\n\t<li>Identification of the potential performance issues</li>\n\t<li>A built-in monitoring solution with reporting, alerting, and mitigating capabilities</li>\n</ol>","a":[{"id":1688821,"option":"Only 2","correct":false},{"id":1688822,"option":"2, 3, and 4","correct":false},{"id":1688823,"option":"2 and 4","correct":false},{"id":1688824,"option":"Only 4","correct":true}]},{"q":"<p>Your company is using an on-premise Microsoft SQL Server and wants to migrate the database to Azure SQL databases. Which of the following must be used as a file type for exporting the on-premise database in the given scenario?</p>","a":[{"id":1688817,"option":"VHD","correct":false},{"id":1688818,"option":"DAC","correct":false},{"id":1688819,"option":"BACPAC","correct":true},{"id":1688820,"option":"VHDX","correct":false}]},{"q":"<p>You are working on an application where you would be using Azure SQL database and Azure storage accounts. The functions of the application includes</p>\n\n<ol>\n\t<li>Extracting data.</li>\n\t<li>Converting it to text documents.</li>\n\t<li>Storing them in storage accounts where the text documents would be accessible from an SMB network share.</li>\n</ol>\n\n<p>Which of the following will you use as the underlying service type for the Azure storage account in the given scenario?</p>","a":[{"id":1688813,"option":"Blob","correct":false},{"id":1688814,"option":"Queue","correct":false},{"id":1688815,"option":"Files","correct":true},{"id":1688816,"option":"Table","correct":false}]},{"q":"<p>You have used the proxy connection type to establish a TCP session using a gateway for an Azure managed instance. Which of the following is likely to occur once this session is established?</p>","a":[{"id":1688809,"option":"All subsequent packets flow through gateway","correct":true},{"id":1688810,"option":"Client session will obtain the destination virtual IP of the virtual cluster node from the load balancer","correct":false},{"id":1688811,"option":"Subsequent packets flow directly to the virtual cluster node","correct":false},{"id":1688812,"option":"None of these","correct":false}]},{"q":"<p>You want to connect directly to the node hosting the database while working with an Azure SQL Managed instance. Which of the following connections would you use to establish the connection in the given scenario?</p>","a":[{"id":1688805,"option":"Redirect","correct":true},{"id":1688806,"option":"Proxy","correct":false},{"id":1688807,"option":"Direct","correct":false},{"id":1688808,"option":"EIther Redirect or Proxy","correct":false}]},{"q":"<p>Suppose you want to copy a subset of blobs under a folder when working with Azure Synapse Analytics. Which of the following mode of action do you need to follow in order to do so?</p>","a":[{"id":1688801,"option":"Specify folderPath for the folder part only","correct":false},{"id":1688802,"option":"Specify folderPath for the folder part and fileName for the file name","correct":false},{"id":1688803,"option":"Specify folderPath for the folder part and fileName with a wildcard filter","correct":true},{"id":1688804,"option":"None of these","correct":false}]},{"q":"<p>You do not require every pipeline execution of your data flow activities in Azure Synapse Analytics to fully log all verbose telemetry logs. Which of the following logging levels should you set in order to accomplish the above requirement?</p>","a":[{"id":1688797,"option":"Basic","correct":false},{"id":1688798,"option":"Log","correct":false},{"id":1688799,"option":"None","correct":false},{"id":1688800,"option":"Either 1 or 3","correct":true}]},{"q":"<p>You run the query given alongside in Azure Synapse to check if a query was executed with a result cache hit or miss.</p>\n\n<p><strong>Query</strong></p>\n\n<pre class=\"prettyprint\"><code>SELECT request_id, command, result_cache_hit FROM sys.dm_pdw_exec_requests\nWHERE request_id = &lt;'Your_Query_Request_ID'&gt;</code></pre>\n\n<p>What value will be returned by result_cache_hit column if result set caching was not used?</p>","a":[{"id":1688793,"option":"<0","correct":true},{"id":1688794,"option":"0","correct":false},{"id":1688795,"option":"1","correct":false},{"id":1688796,"option":">1","correct":false}]},{"q":"<p>You are given that result set caching is turned ON for a database in an Azure Synapse dedicated pool. In the given scenario, for which of these queries are the results not cached until the cache is full:</p>\n\n<p><strong>Queries</strong></p>\n\n<ol>\n\t<li>Queries returning large result sets</li>\n\t<li>Queries with built-in functions</li>\n\t<li>Queries using user-defined functions</li>\n\t<li>Queries using tables with either row-level security or column level security enabled</li>\n</ol>","a":[{"id":1688789,"option":"Only 1, 2 and 3","correct":false},{"id":1688790,"option":"Only 2, 3 and 4","correct":false},{"id":1688791,"option":"Only 1, 3 and 4","correct":false},{"id":1688792,"option":"All 1, 2, 3 and 4","correct":true}]},{"q":"<p>You have created an ordered CCI (Clustered columnstore index) on a large table in an Azure Synapse dedicated pool. However, you want to reduce segment overlapping during the creation. Which of the following options will be suitable in the given scenario?</p>","a":[{"id":1688785,"option":"By using xlargerc resource class on a lower DWU","correct":false},{"id":1688786,"option":"By creating ordered CCI with MAXDOP set to 0","correct":false},{"id":1688787,"option":"By creating ordered CCI with MAXDOP set to 1","correct":true},{"id":1688788,"option":"By creating ordered CCI with MAXDOP set to -1","correct":false}]},{"q":"<p>You want to load data directly to a dedicated SQL pool in Azure Synapse Analytics without going through Azure Blob storage. Which of the following loading options can be used to achieve this?</p>","a":[{"id":1688781,"option":"bcp","correct":true},{"id":1688782,"option":"SqlBulkCopy API","correct":false},{"id":1688783,"option":"COPY statement ","correct":false},{"id":1688784,"option":"PolyBase with SSIS ","correct":false}]},{"q":"<p>You are working with Data Warehouse Units in Synapse SQL. You are given a task to scan a large number of rows using a standard data warehousing query and later perform a complex aggregation. What will be the nature of the performance of the respective operation?</p>","a":[{"id":1688777,"option":"I/O intensive","correct":false},{"id":1688778,"option":"CPU intensive","correct":false},{"id":1688779,"option":"Network intensive","correct":false},{"id":1688780,"option":"Both 1 and 2","correct":true}]},{"q":"<p>Which of the following join types can used for non-equi joins in Azure Synapse Analytics?</p>","a":[{"id":1688773,"option":"Left Outer","correct":false},{"id":1688774,"option":"Right Outer","correct":false},{"id":1688775,"option":"Inner Join","correct":false},{"id":1688776,"option":"Custom cross join","correct":true}]},{"q":"<p>You get the error message: “Error 403” when working with a copy activity in Azure Synapse Analytics. What could be a valid cause for such an error to occur?</p>","a":[{"id":1688769,"option":"Invalid Python file URI","correct":false},{"id":1688770,"option":"Databricks cluster does not exist ","correct":false},{"id":1688771,"option":"Databricks cluster has been deleted","correct":false},{"id":1688772,"option":"The Databricks access token has expired  ","correct":true}]},{"q":"<p>Which of the following operators in Azure Synapse Analytics can be used to access a specific element when dealing with columns or functions that return array types?</p>","a":[{"id":1688765,"option":"( )","correct":false},{"id":1688766,"option":"{ }","correct":false},{"id":1688767,"option":"[ ]","correct":true},{"id":1688768,"option":"| |","correct":false}]},{"q":"<p>Which of the following URLs should you choose to use serverless SQL pool in Azure Synapse:</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;-sql.transact.azuresynapse.net</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;-transact.sql.azuresynapse.net</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;.sql.azuresynapse.net</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>&lt;Azure Synapse workspace name&gt;-ondemand.sql.azuresynapse.net</code></pre>\n\n<p>&nbsp;</p>","a":[{"id":1688757,"option":"1","correct":false},{"id":1688758,"option":"2","correct":false},{"id":1688759,"option":"3","correct":false},{"id":1688760,"option":"4","correct":true}]},{"q":"<p>When working on a SQL database in Azure SQL, you get the following error message.</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>\"SqlErrorNumber=47073\". Error: Cannot connect to SQL Database: '%server;', Database: '%database;', User: '%user;'. Check the linked service configuration is correct, and make sure the SQL Database firewall allows the integration runtime to access.</code></pre>\n\n<p>What might be the potential cause for this error to occur?</p>","a":[{"id":1688753,"option":"Some specific operation failed","correct":false},{"id":1688754,"option":"An unstable network connection","correct":false},{"id":1688755,"option":"A mismatch between the source and sink column sizes","correct":false},{"id":1688756,"option":"The public network access is denied in the connectivity setting","correct":true}]},{"q":"<p>You get the following error message when working with Azure SQL Database.<br>\n&nbsp;<strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>A database operation failed. Check the SQL errors.</code></pre>\n\n<p>During troubleshooting, you find that such issues occur in the SQL sink and the error is related to SqlDateTime overflow. Which of the following is the best possible way to resolve this kind of error?</p>","a":[{"id":1688749,"option":"Cast the type to the string in the source SQL query","correct":false},{"id":1688750,"option":"Retry the activity and review the SQL database side metrics","correct":false},{"id":1688751,"option":"Change the column type to string in the copy activity column mapping","correct":false},{"id":1688752,"option":"Update the corresponding column type to the datetime2 type in the sink table","correct":true}]},{"q":"<p>You get the following error message when working with Azure SQL Database.</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>Column '%column;' does not exist in the table '%tableName;', ServerName: '%serverName;', DatabaseName: '%dbName;'.</code></pre>\n\n<p>What might be the possible reason behind this?</p>","a":[{"id":1688745,"option":"The credentials are incorrect","correct":false},{"id":1688746,"option":"A SQL database transient failure","correct":false},{"id":1688747,"option":"The query does not return any data","correct":false},{"id":1688748,"option":"The configuration might be incorrect","correct":true}]},{"q":"<p>When working with the Azure SQL database, you encountered an issue in the SQL source related to SqlDateTime overflow. Which of the following error codes will be displayed as a result of this?</p>","a":[{"id":1688741,"option":"SqlInvalidColumnName","correct":false},{"id":1688742,"option":"SqlInvalidDbQueryString","correct":false},{"id":1688743,"option":"SqlDataTypeNotSupported","correct":true},{"id":1688744,"option":"SqlInvalidDbStoredProcedure","correct":false}]},{"q":"<p>You get the following transient fault error when working on an Azure SQL Database service.</p>\n\n<p><strong>Error</strong></p>\n\n<pre class=\"prettyprint\"><code>\"The service has encountered an error processing your request. Please try again. Error code %d\".</code></pre>\n\n<p>What might be the possible cause for such an error to occur?</p>","a":[{"id":1688737,"option":"Requests are blocked for resource optimization","correct":false},{"id":1688738,"option":"There is already an existing dedicated administrator connection established to the SQL database","correct":false},{"id":1688739,"option":"The service is busy processing multiple create or update requests for your subscription or server","correct":false},{"id":1688740,"option":"The service is down due to software or hardware upgrades, hardware failures, or any other failover problems","correct":true}]},{"q":"<p>You get a transient error during the Azure SQL Database and SQL Managed Instance query command. What will you do in the given scenario?</p>","a":[{"id":1688733,"option":"After a delay of several seconds, retry the connection","correct":false},{"id":1688734,"option":"Immediately retry the command","correct":false},{"id":1688735,"option":"After a delay, freshly establish the connection and do not retry the command","correct":false},{"id":1688736,"option":"After a delay, freshly establish the connection and then retry the command","correct":true}]},{"q":"<p>You get the following error when working with the Azure SQL database.</p>\n\n<p><strong>Error</strong></p>\n\n<pre class=\"prettyprint\"><code>\"PdwManagedToNativeInteropException\". Error: A database operation failed. Please search error to get more details.</code></pre>\n\n<p>What might be the potential cause for the respective error?</p>","a":[{"id":1688729,"option":"Invalid input data","correct":false},{"id":1688730,"option":"Some specific operation failed","correct":false},{"id":1688731,"option":"An Azure SQL Database firewall issue","correct":false},{"id":1688732,"option":"A mismatch between the source and sink column sizes","correct":true}]},{"q":"<p>In which of the given scenarios is sharding required when working with Azure SQL databases?</p>\n\n<p><strong>Scenarios</strong></p>\n\n<ol>\n\t<li>The total amount of data is too large to fit within the constraints of an individual database.</li>\n\t<li>Transaction throughput of the overall workload exceeds the capabilities of an individual database.</li>\n\t<li>Separate databases are needed for each tenant.</li>\n</ol>","a":[{"id":1688725,"option":"1 and 2","correct":false},{"id":1688726,"option":"2 and 3","correct":false},{"id":1688727,"option":"1 and 3","correct":false},{"id":1688728,"option":"1, 2, and 3","correct":true}]},{"q":"<p>Which of these techniques is the most common way to horizontally scale an Azure SQL database?</p>","a":[{"id":1688721,"option":"Sharding","correct":true},{"id":1688722,"option":"Transaction","correct":false},{"id":1688723,"option":"Splitting","correct":false},{"id":1688724,"option":"None of these","correct":false}]},{"q":"<p>You want to scale a Azure SQL database using elastic database tools. In the given context, which of these dimensions of scaling can be used if you want to remove databases to adjust overall performance?</p>","a":[{"id":1688717,"option":"Horizontal scaling","correct":true},{"id":1688718,"option":"Vertical Scaling","correct":false},{"id":1688719,"option":"Sharding","correct":false},{"id":1688720,"option":"Either 2 or 3","correct":false}]},{"q":"<p>An Azure SQL database is unavailable because of an incident in the region where the database is hosted. Which of these business continuity options in an elastic pool can be used to provide default recovery for the database?</p>","a":[{"id":1688713,"option":"Point-in-time restore","correct":false},{"id":1688714,"option":"Geo-restore","correct":true},{"id":1688715,"option":"Active geo-replication","correct":false},{"id":1688716,"option":"Active db-restore","correct":false}]},{"q":"<p>In Azure, you have a pipeline in Data Factory with the following three consecutive activities where each activity runs only if the previous activity succeeds.<br>\nActivity A -&gt; Activity B -&gt; Activity C<br>\nIn the given scenario, what will be the status of B and C if A fails to run?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Skipped\nCompleted</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Failed \nFailed </code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Failed\nSkipped</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>Skipped\nSkipped</code></pre>","a":[{"id":1678113,"option":"1","correct":false},{"id":1678114,"option":"2","correct":false},{"id":1678115,"option":"3","correct":false},{"id":1678116,"option":"4","correct":true}]},{"q":"<p>Which of the following is irrelevant when choosing the best size for an elastic pool in Azure SQL Database?</p>","a":[{"id":1688709,"option":"Maximum compute resources and storage bytes utilized by all databases in the pool","correct":false},{"id":1688710,"option":"Maximum storage bytes utilized by a single database in the pool","correct":false},{"id":1688711,"option":"Maximum compute resources utilized by a single database in the pool","correct":false},{"id":1688712,"option":"Both 2 and 3","correct":true}]},{"q":"<p>You are using in-memory technology in Azure SQL to optimize performance. Which of these workloads can be used to update smaller sets of data when doing so?</p>","a":[{"id":1688705,"option":"Transactional (online transactional processing (OLTP))","correct":true},{"id":1688706,"option":"Analytic (online analytical processing (OLAP)) ","correct":false},{"id":1688707,"option":"Mixed (hybrid transaction/analytical processing (HTAP))","correct":false},{"id":1688708,"option":"Either 1 or 2","correct":false}]},{"q":"<p>You want to integrate Azure SQL Database telemetry with your custom monitoring solution or hot pipelines. Which of these Azure resources can be used to achieve this?</p>","a":[{"id":1688701,"option":"Azure Event Hubs","correct":true},{"id":1688702,"option":"Azure Storage","correct":false},{"id":1688703,"option":"Azure Monitor Logs","correct":false},{"id":1688704,"option":"None of these","correct":false}]},{"q":"<p>What is the benefit of using Access Control (IAM) module in Azure SQL?</p>","a":[{"id":1688697,"option":"You can control role assignments","correct":false},{"id":1688698,"option":"You can manage access to Azure resources","correct":false},{"id":1688699,"option":"Both 1 and 2","correct":true},{"id":1688700,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You are running an application that accesses SQL Server on the same VM in Azure SQL. Which of the following SQL Server connectivity option is the most secure in the given context?</p>","a":[{"id":1688693,"option":"Local(inside VM only)","correct":true},{"id":1688694,"option":"Private(within virtual network)","correct":false},{"id":1688695,"option":"Public(Internet)","correct":false},{"id":1688696,"option":"Both 1 and 3","correct":false}]},{"q":"<p>Which of these services allows you to run SQL Server inside a fully managed virtual machine (VM) in Azure?</p>","a":[{"id":1688689,"option":"Platform as a Service","correct":false},{"id":1688690,"option":"Infrastructure as a Service","correct":true},{"id":1688691,"option":"Software as a Service","correct":false},{"id":1688692,"option":"Both Platform as a Service and Infrastructure as a Service","correct":false}]},{"q":"<p>In Azure, which of the measures given alongside should be taken if you receive the following error when working with clusters in Databricks:</p>\n\n<p><strong>Error</strong></p>\n\n<pre class=\"prettyprint\"><code>REQUEST_LIMIT_EXCEEDED</code></pre>\n\n<p><strong>Measures</strong></p>\n\n<ol>\n\t<li>Retry your request</li>\n\t<li>Use auto-scaling clusters</li>\n\t<li>Use clusters with a larger node type and the smaller number of nodes</li>\n\t<li>Spread out your recurring workflow evenly in the planned time frame</li>\n</ol>","a":[{"id":1688685,"option":"Only 1,2 and 3","correct":false},{"id":1688686,"option":"Only 2, 3 and 4","correct":false},{"id":1688687,"option":"Only 1, 3 and 4","correct":false},{"id":1688688,"option":"All 1, 2, 3 and 4","correct":true}]},{"q":"<p>Assume that Azure Databricks was able to launch the cluster, but lost the connection to the instance hosting the Spark driver. What could be a valid reason for such an issue?</p>","a":[{"id":1688681,"option":"Your subscription was disabled","correct":false},{"id":1688682,"option":"The driver virtual machine was powered off","correct":true},{"id":1688683,"option":"You have reached a quota limit that your subscription can launch","correct":false},{"id":1688684,"option":"You have reached the limit of the public IPs that you can have running","correct":false}]},{"q":"<p>You get the following error message when creating an Azure Databricks workspace in a virtual network (VNet).</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>Subnet requires any of the following delegation(s) [Microsoft.Databricks/workspaces] to reference service association link.</code></pre>\n\n<p>What might be a valid cause for this?</p>","a":[{"id":1688669,"option":"You are creating a workspace with host and container subnets that are already being used by an existing Azure Databricks workspace","correct":false},{"id":1688670,"option":"You are creating a workspace whose host and container subnets have not been delegated to the Microsoft.Databricks/workspaces service","correct":true},{"id":1688671,"option":"You are creating a workspace whose host and container subnets have been delegated to the Microsoft.Databricks/workspaces service","correct":false},{"id":1688672,"option":"You are creating a workspace whose host and container subnets have been delegated to the Microsoft.Databricks/workspaces service and are used by an existing Azure Databricks workspace","correct":false}]},{"q":"<p>You have a virtual network (VNet) for a Databricks workspace that uses secure cluster connectivity. What will be the type of access modifiers for the container and host subnets, respectively, in such a VNet?</p>","a":[{"id":1688665,"option":"Private and Private","correct":true},{"id":1688666,"option":"Private and Public","correct":false},{"id":1688667,"option":"Protected and Default","correct":false},{"id":1688668,"option":"Public and Private","correct":false}]},{"q":"<p>You realize that the Azure Databricks user interface that you have been working on is running slow. What could be a valid reason behind this issue?</p>","a":[{"id":1688661,"option":"Network latency","correct":false},{"id":1688662,"option":"Network traffic is affected","correct":false},{"id":1688663,"option":"Database query taking more time than expected","correct":false},{"id":1688664,"option":"Either 1 or 3","correct":true}]},{"q":"<p>You get the following exception when setting the network configuration of Azure Data Lake Storage(ADLS) Gen 1 in Databricks.</p>\n\n<p><strong>Exception</strong></p>\n\n<pre class=\"prettyprint\"><code>ADLException: Error getting info for file</code></pre>\n\n<p>Which of these configurations given alongside could have caused this exception:</p>\n\n<p><strong>Configurations</strong></p>\n\n<ol>\n\t<li>ADLS Gen1 storage firewall is enabled</li>\n\t<li>Azure Databricks workspace is deployed in your own virtual network</li>\n\t<li>Traffic is allowed via ADLS credential passthrough</li>\n\t<li>Azure Active Directory service endpoint is enabled for the Azure Databricks workspace virtual network</li>\n</ol>\n\n<p>&nbsp;</p>","a":[{"id":1688657,"option":"Only 1, 2 and 3","correct":false},{"id":1688658,"option":"Only 2, 3 and 4","correct":false},{"id":1688659,"option":"Only 1, 2 and 4","correct":false},{"id":1688660,"option":"All 1, 2, 3 and 4","correct":true}]},{"q":"<p>You want to optimize the runtime of a DNASeq pipeline notebook when working with custom reference genomes in Azure Databricks Runtime 6.6. What should you set the Spark configuration \"spark.sql.shuffle.partitions\" as in order to achieve this successfully?</p>","a":[{"id":1688653,"option":"Two times the number of cores of the cluster.","correct":false},{"id":1688654,"option":"Three times the number of cores of the cluster.","correct":true},{"id":1688655,"option":"Five times the number of cores of the cluster.","correct":false},{"id":1688656,"option":"Ten times the number of cores of the cluster.","correct":false}]},{"q":"<p>Which of the following options is true about the Data engineering workload in Azure Databricks?</p>","a":[{"id":1688649,"option":"Data Engineering is an automated workload running on a job cluster","correct":true},{"id":1688650,"option":"Data Engineering is an interactive workload running on an all-purpose cluster","correct":false},{"id":1688651,"option":"Data Engineering is an automated workload running on an all-purpose cluster","correct":false},{"id":1688652,"option":"Data Engineering is an interactive workload running on a job cluster","correct":false}]},{"q":"<p>In Azure, for which of the following items would you have to select values when creating a Data Lake Analytics account?</p>\n\n<p><strong>Items</strong></p>\n\n<ol>\n\t<li>Subscription</li>\n\t<li>Resource Group</li>\n\t<li>Server Address</li>\n\t<li>Location</li>\n</ol>","a":[{"id":1688645,"option":"1 and 3","correct":false},{"id":1688646,"option":"1, 3, and 4","correct":false},{"id":1688647,"option":"2 and 3","correct":false},{"id":1688648,"option":"1, 2, and 4","correct":true}]},{"q":"<p>Which of the following is the fastest naming option for writing files in Azure Data Factory?</p>","a":[{"id":1688641,"option":"Default","correct":true},{"id":1688642,"option":"Pattern","correct":false},{"id":1688643,"option":"Pre partition","correct":false},{"id":1688644,"option":"As data in column","correct":false}]},{"q":"<p>Which of the following conditional paths is passed as default when handling errors during pipeline execution in Data Factory?</p>","a":[{"id":1688637,"option":"Upon Completion","correct":false},{"id":1688638,"option":"Upon Failure","correct":false},{"id":1688639,"option":"Upon Skip","correct":false},{"id":1688640,"option":"Upon Success ","correct":true}]},{"q":"<p>You encounter validation errors at the source with multiline CSV files when mapping data flows in Azure Data Factory. In the given scenario, which of these error messages will not be visible on the Azure Data Factory portal?</p>","a":[{"id":1688629,"option":"Schema validation at source fails","correct":false},{"id":1688630,"option":"The last column is null or missing","correct":false},{"id":1688631,"option":"During Data Flow activity execution: Hit unexpected exception and execution failed","correct":true},{"id":1688632,"option":"Schema import fails to show correctly in the UX and the last column has a new line character in the name","correct":false}]},{"q":"<p>You receive the error given alongside in your Data Factory portal when joining SQL Server Integration Services (SSIS) Integration Runtime (IR) to an Azure virtual network.</p>\n\n<p><strong>Error</strong></p>\n\n<pre class=\"prettyprint\"><code>\"SubnetId is not enabled for current account. Microsoft.Batch resource provider is not registered under the same subscription of VNet.\"</code></pre>\n\n<p>What does such an error indicate?</p>","a":[{"id":1688625,"option":"It indicates that the virtual network does not exist","correct":false},{"id":1688626,"option":"It indicates that the specified subnet does not exist","correct":false},{"id":1688627,"option":"It indicates that the Azure Batch service cannot access the virtual network","correct":true},{"id":1688628,"option":"It indicates that the  Azure Batch service cannot access the specified subnet","correct":false}]},{"q":"<p>You are given an error code:</p>\n\n<p>\"InternalServerError is displayed in the Data Factory portal when provisioning SQL Server Integration Services (SSIS) Integration Runtime(IR).\"<br>\nIn Azure, which of these management issues could be a reason for such an error to occur?</p>","a":[{"id":1688621,"option":"Transient issues","correct":true},{"id":1688622,"option":"Custom setup issues","correct":false},{"id":1688623,"option":"Virtual network configuration problems","correct":false},{"id":1688624,"option":"Azure SQL Database or Azure SQL Managed Instance issues","correct":false}]},{"q":"<p>In Azure, you encounter the error message given alongside in your Data Factory portal when executing SQL Server Integration Services (SSIS) packages in the SSIS integration runtime.</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>\"Connection Timeout Expired\" or \"The service has encountered an error processing your request. Please try again.\"</code></pre>\n\n<p>Which of the following is a potential cause for this error?</p>","a":[{"id":1688617,"option":"The data source or destination is overloaded","correct":false},{"id":1688618,"option":"The network between the SSIS integration runtime and the data source or destination is unstable","correct":false},{"id":1688619,"option":"The local disk is used up in the SSIS integration runtime node","correct":false},{"id":1688620,"option":"Either 1 or 2","correct":true}]},{"q":"<p>Which of the following benefits would you get if you use the Azure Data Lake Analytics to transform your data:</p>\n\n<p><strong>Benefits</strong></p>\n\n<ol>\n\t<li>Dynamic Scaling</li>\n\t<li>Faster development</li>\n\t<li>Use of U-SQL</li>\n\t<li>Integration with existing IT investments</li>\n</ol>","a":[{"id":1688609,"option":"3 and 4","correct":false},{"id":1688610,"option":"2, 3, and 4","correct":false},{"id":1688611,"option":"1 and 3","correct":false},{"id":1688612,"option":"All of these","correct":true}]},{"q":"<p>When creating an Azure Data Lake Storage Gen2 account, you want to configure this account to be able to process analytical data workloads for best performance. What can you do to achieve this?</p>","a":[{"id":1688605,"option":"On the Basic tab, set the Performance option to Standard","correct":false},{"id":1688606,"option":"On the Basic tab, set the Performance option to ON","correct":false},{"id":1688607,"option":"On the Advanced tab, set the Hierarchical Namspace to Enable","correct":true},{"id":1688608,"option":"On the Advance tab, set the Hierarchical Namspace to Disable","correct":false}]},{"q":"<p>In Azure, you are setting up the instance details in the basic tab when creating a Data Lake Storage Gen2 store. In the given context, what would you select from the Account kind drop-down menu if your default deployment model is Resource Manager?</p>","a":[{"id":1688601,"option":"StorageV2 (general-purpose v1)","correct":false},{"id":1688602,"option":"StorageV1 (general-purpose v2)","correct":false},{"id":1688603,"option":"StorageV2 (general-purpose v2)","correct":true},{"id":1688604,"option":"StorageV1 (general-purpose v1)","correct":false}]},{"q":"<p>In Azure, you have chosen to use the default deployment model i.e. Resource Manager, when creating the Data Lake Storage Gen2. In the given context, which of the following options would you select?</p>","a":[{"id":1688597,"option":"set Access Tier as 'Hot'","correct":true},{"id":1688598,"option":"set Performance as 'Premium'","correct":false},{"id":1688599,"option":"set Access Tier as 'Cool'","correct":false},{"id":1688600,"option":"set Performance as 'Basic'","correct":false}]},{"q":"<p>Which of the following can be set using the basic tab when creating an Azure Data Lake Storage Gen2 account:</p>\n\n<ol>\n\t<li>Performance</li>\n\t<li>Account Kind</li>\n\t<li>Replication</li>\n\t<li>Access Tier</li>\n</ol>","a":[{"id":1688593,"option":"1, 2, and 4","correct":false},{"id":1688594,"option":"All of these","correct":true},{"id":1688595,"option":"1 and 3","correct":false},{"id":1688596,"option":"2, 3, and 4","correct":false}]},{"q":"<p>You want to enable the \"secure transfer required\" setting when creating an Azure Data Lake Storage Gen2 account using the portal. Which of these tabs can be used to achieve this?</p>","a":[{"id":1688589,"option":"In tags tab","correct":false},{"id":1688590,"option":"In Review + create tab","correct":false},{"id":1688591,"option":"In Advance tab","correct":true},{"id":1688592,"option":"In Basic tab","correct":false}]},{"q":"<p>You are creating a Data Lake Storage Gen2 data store that requires Azure Storage account with the Hierarchical namespace enabled.<br>\nIn which of these tabs would you find the option to enable this?</p>","a":[{"id":1688585,"option":"In Basic tab","correct":false},{"id":1688586,"option":"In Advance tab","correct":true},{"id":1688587,"option":"In tags tab","correct":false},{"id":1688588,"option":"In Review + create tab","correct":false}]},{"q":"<p>How can permissions for an item be inherited in a Data Lake Storage Gen2 that uses the POSIX style model?</p>","a":[{"id":1688581,"option":"No need to take permission","correct":false},{"id":1688582,"option":"From the Permission repository","correct":false},{"id":1688583,"option":"From the Parent item","correct":false},{"id":1688584,"option":"From the item itself","correct":true}]},{"q":"<p>In a scenario from Azure Data Lake, instead of repeatedly using EXTRACT to read from the same source file, you want to take a different approach.<br>\nIf you have a single query expression, which of the following will be most suited for encapsulating that expression:</p>\n\n<ol>\n\t<li>Table-valued function</li>\n\t<li>U-SQL View</li>\n</ol>","a":[{"id":1688573,"option":"Only 1","correct":false},{"id":1688574,"option":"Only 2","correct":true},{"id":1688575,"option":"Both 1 and 2 are equally suited","correct":false},{"id":1688576,"option":"Using either 1 or 2 in such a scenario will transform the data","correct":false}]},{"q":"<p>In Azure Data Lake, you observe that the outputter makes use of the System.IO.StreamWriter to write the output data to a file.<br>\nIn the given context, the stream parameter should be set to which of the following when doing so?</p>","a":[{"id":1688569,"option":"output.BaseStream","correct":true},{"id":1688570,"option":"output.IUnstructuredWriter ","correct":false},{"id":1688571,"option":"output.StreamWriter","correct":false},{"id":1688572,"option":"output.IOutputter","correct":false}]},{"q":"<p>In Azure Data Lake, you are using a user-defined outputter to write data in a custom-defined format. You are also using the output object to set output data to the target file. Which of these can be called to determine the individual column names when doing so?</p>","a":[{"id":1688565,"option":"row.Get<>()","correct":false},{"id":1688566,"option":"row.schema","correct":true},{"id":1688567,"option":"row.Col","correct":false},{"id":1688568,"option":"col.Get<>(\"row Name\")","correct":false}]},{"q":"<p>In Azure Data Lake, you are using the Partition_Label to specify an explicit partition into which the data needs to be inserted. In which of the following cases will an error be raised if the target table is not partitioned:</p>\n\n<p><strong>Cases</strong></p>\n\n<ol>\n\t<li>Integrity_Clause is specified</li>\n\t<li>Partition_Label is specified</li>\n</ol>","a":[{"id":1688561,"option":"Only 1","correct":false},{"id":1688562,"option":"Only 2","correct":false},{"id":1688563,"option":"Both 1 and 2","correct":true},{"id":1688564,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>In Azure Data Lake, you are using the Job Heat Map Display to see the time and throughput heat map of a job.<br>\nIn the given context, how can you see the duration of time it would take to execute all work in the stage with only 1 vertex?</p>","a":[{"id":1688557,"option":"Using Compute time","correct":true},{"id":1688558,"option":"Using Average execution time per node","correct":false},{"id":1688559,"option":"Using Total execution time","correct":false},{"id":1688560,"option":"Using Input/Output time","correct":false}]},{"q":"<p>In Azure Data Lake, you observe that the data you are using does not require quoting when using the outputter. What will you do to achieve faster data processing in this scenario?</p>","a":[{"id":1688553,"option":"Set the quoting parameter to true","correct":false},{"id":1688554,"option":"Set the quoting parameter to false","correct":true},{"id":1688555,"option":"Set the quoting parameter to null","correct":false},{"id":1688556,"option":"Set the quoting parameter to 0","correct":false}]},{"q":"<p>In Azure Data Lake, which of the following usages of the User-Defined Types(UDT) will raise an error if used without serialization:</p>\n\n<ol>\n\t<li>Used in EXTRACTOR</li>\n\t<li>Used in OUTPUTTER</li>\n</ol>","a":[{"id":1688549,"option":"Only 1","correct":false},{"id":1688550,"option":"Only 2","correct":false},{"id":1688551,"option":"Both 1 and 2","correct":true},{"id":1688552,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Which of these data modeling constructs is supported by Azure Analysis Services?</p>\n\n<p><strong>Data Modeling Constructs</strong></p>\n\n<ol>\n\t<li>Tabular model in in-memory mode</li>\n\t<li>Tabular model in DirectQuery mode</li>\n\t<li>Multidimensional models</li>\n\t<li>PowerPivot in SharePoint</li>\n</ol>","a":[{"id":1688545,"option":"only 2","correct":false},{"id":1688546,"option":"1, 3, and 4","correct":false},{"id":1688547,"option":"3 and 4","correct":false},{"id":1688548,"option":"1 and 2","correct":true}]},{"q":"<p>Which of these security benefits does Azure analysis service provide at a server level?</p>","a":[{"id":1688541,"option":"Only Firewall protection","correct":false},{"id":1688542,"option":"Only Server administrator roles ","correct":false},{"id":1688543,"option":"Only Server-Side Encryption","correct":false},{"id":1688544,"option":"It provides Firewall protection, Server administrator roles and Server-Side Encryption","correct":true}]},{"q":"<p>In Azure Analysis Services, you are performing a subsequent scale out operation to increase the number of replicas from three to six.<br>\nWhich of these statements is valid in the given context?</p>","a":[{"id":1688537,"option":"Data on primary server should be processed first","correct":false},{"id":1688538,"option":"Synchronization should be performed after the scale-out operation ","correct":false},{"id":1688539,"option":"Concurrent synchronization and scale-out operations should not be run at the same time","correct":false},{"id":1688540,"option":"Only 1 and 3","correct":true}]},{"q":"<p>Which level of protection would you use for protection against Distributed denial of service (DDoS) attacks when working with Azure analysis service?</p>","a":[{"id":1688533,"option":"Server level","correct":false},{"id":1688534,"option":"Basic Level","correct":true},{"id":1688535,"option":"Data Model level","correct":false},{"id":1688536,"option":"Row level","correct":false}]},{"q":"<p>In Azure, you are working with Azure Analysis Services. When creating a server, you observe that you need to spread the query workload by distributing the client queries among multiple query replicas in a query pool.<br>\nWhat could be the reason behind this?</p>","a":[{"id":1688529,"option":"Memory use by high query workloads can be reduced","correct":false},{"id":1688530,"option":"Total cost for high query workloads can be reduced","correct":false},{"id":1688531,"option":"Billing of query replicas is lowered than server","correct":false},{"id":1688532,"option":"Response times during high query workloads can be reduced","correct":true}]},{"q":"<p>In Azure Analysis Services, you want to create a query pool with up to seven additional query replica resources. Which of the following tier would you choose to do so?</p>","a":[{"id":1688525,"option":"Standard","correct":true},{"id":1688526,"option":"Basic","correct":false},{"id":1688527,"option":"Advance","correct":false},{"id":1688528,"option":"Developer","correct":false}]},{"q":"<p>In Azure Analysis Services, which of the following tier would you choose for evaluation, development, and test scenarios?</p>","a":[{"id":1688521,"option":"Basic","correct":false},{"id":1688522,"option":"Developer","correct":true},{"id":1688523,"option":"Advance","correct":false},{"id":1688524,"option":"Standard","correct":false}]},{"q":"<p>Assume that you have created a server and selected a plan within a tier. Which of these actions can you further take with the selected tier:</p>\n\n<p><strong>Actions</strong></p>\n\n<ol>\n\t<li>Change plans up within the same tier</li>\n\t<li>Change plans down within the same tier</li>\n\t<li>Upgrade to a higher tier</li>\n\t<li>Downgrade to a lower tier</li>\n</ol>","a":[{"id":1688517,"option":"1, 2, and 3","correct":true},{"id":1688518,"option":"1 and 3","correct":false},{"id":1688519,"option":"2 and 4","correct":false},{"id":1688520,"option":"All of these","correct":false}]},{"q":"<p>You want to perform some ad hoc data analysis with the help of data models in Azure Analysis Services.<br>\nWhich of these tools can you use to do this:</p>\n\n<p><strong>Tools</strong></p>\n\n<ol>\n\t<li>Power BI</li>\n\t<li>PowerPoint</li>\n\t<li>Excel</li>\n\t<li>Spreadsheet</li>\n</ol>","a":[{"id":1688513,"option":"Only 1","correct":false},{"id":1688514,"option":"1 and 2","correct":false},{"id":1688515,"option":"1, 2, and 3","correct":false},{"id":1688516,"option":"1 and 3","correct":true}]},{"q":"<p>Which of the following can be used to combine data from multiple data sources with the help of Azure Analysis Services?</p>","a":[{"id":1688509,"option":"JOIN query","correct":false},{"id":1688510,"option":"Data Integration method ","correct":false},{"id":1688511,"option":"advanced mashup and modeling features","correct":true},{"id":1688512,"option":"CombineData()","correct":false}]},{"q":"<p>Which of these steps should be taken first when trying to restore the Wide World Importers database to SQL Managed Instance in SQL Server Management Studio?</p>\n\n<ol>\n\t<li>Open SSMS</li>\n\t<li>Connect to your managed instance.</li>\n\t<li>Open SSMS and then connect to your managed instance</li>\n\t<li>Open object explorer and then connect to your managed instance</li>\n</ol>","a":[{"id":1687133,"option":"1","correct":false},{"id":1687134,"option":"2","correct":false},{"id":1687135,"option":"3","correct":true},{"id":1687136,"option":"4","correct":false}]},{"q":"<p>In which of these scenarios should you consider using instance pools?</p>\n\n<ol>\n\t<li>Scenarios where minimal IP address allocation in a VNet subnet is important.</li>\n\t<li>Scenarios where having a fixed cost or spending limit is important.</li>\n\t<li>Scenarios where minimal IP address allocation in a VNet subnet is important.</li>\n</ol>","a":[{"id":1687129,"option":"Only 1 and 2","correct":false},{"id":1687130,"option":"Only 2 and 3","correct":false},{"id":1687131,"option":"Only 1 and 3","correct":false},{"id":1687132,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>You have created a SQL Server virtual machine(VM) with a gallery image. You are running an application that accesses the SQL Server on the same VM.<br>\nWhich of the following is the most secure choice to provide accessibility to the application?</p>","a":[{"id":1687125,"option":"Local(inside VM only)","correct":true},{"id":1687126,"option":"Private(within virtual network)","correct":false},{"id":1687127,"option":"Public(Internet)","correct":false},{"id":1687128,"option":"None of these","correct":false}]},{"q":"<p>You have saved a few commands when working with a Azure Databricks workspace. Where will these commands be retained?</p>","a":[{"id":1687121,"option":"Command plane","correct":false},{"id":1687122,"option":"Data plane","correct":true},{"id":1687123,"option":"Streaming data plane","correct":false},{"id":1687124,"option":"Residual data plane","correct":false}]},{"q":"<p>Which of the following is true about Synapse workspace role assignments?</p>\n\n<ol>\n\t<li>If a principal is assigned the same role at different scopes, you'll see multiple assignments for the principal.</li>\n\t<li>If a role is assigned to a security group, you'll see the roles explicitly assigned to the group but not roles inherited from parent groups.</li>\n</ol>","a":[{"id":1687117,"option":"Only 1","correct":false},{"id":1687118,"option":"Only 2","correct":false},{"id":1687119,"option":"Both 1 and 2","correct":true},{"id":1687120,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>A guest user from a different Azure directory tenant is assigned the Synapse Administrator role.<br>\nWhich of these deductions can be made in the given context?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Guest users cannot see or manage role assignments</li>\n\t<li>Guest users can see or manage role assignments</li>\n\t<li>Guest users can list Synapse RBAC role assignments for all scopes</li>\n\t<li>Guest users can include assignments for objects they don't have access to</li>\n</ol>","a":[{"id":1687113,"option":"1","correct":true},{"id":1687114,"option":"2","correct":false},{"id":1687115,"option":"3","correct":false},{"id":1687116,"option":"4","correct":false}]},{"q":"<p>You are trying to connect to Azure Synapse workspace resources from a restricted network. To achieve this, you are creating private endpoints for your workspace resource.<br>\nWhat should you do in the given scenario to access the resources inside your Azure Synapse Analytics Studio workspace resource?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Create at least one Private link endpoint with a SQL type </li>\n\t<li>Create at least one Private link endpoint with a Workspace type </li>\n\t<li>Create at least one Private link endpoint with a SqlOnDemand type </li>\n\t<li>Create atleast one Private link endpoint with a Target sub-resource type</li>\n</ol>","a":[{"id":1687109,"option":"1","correct":false},{"id":1687110,"option":"2","correct":false},{"id":1687111,"option":"3","correct":false},{"id":1687112,"option":"4","correct":true}]},{"q":"<p>When trying to expand the storage structure in Azure Synapse Studio by selecting the arrow to Data -&gt; Linked you get the following error message in the left panel of the linked storage node.</p>\n\n<p>“REQUEST_SEND_ERROR: Failed to send the request to storage server”<br>\nWhat might be the possible reason behind this?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>The container resource you are accessing either has been deleted or does not exist</li>\n\t<li>The workspace tenant that you used to log in is not the same with the tenant of the storage account</li>\n\t<li>The storage resource is not behind a virtual network but the Blob service endpoint is not accessible due to firewall configured</li>\n\t<li>The storage resource being accessed is Azure Data Lake Storage Gen2 and is behind a firewall and a virtual network at the same time</li>\n</ol>","a":[{"id":1687105,"option":"1","correct":false},{"id":1687106,"option":"2","correct":false},{"id":1687107,"option":"3","correct":true},{"id":1687108,"option":"4","correct":false}]},{"q":"<p>You are organising your data in Azure SQL database such that the rows in the table are placed in columnar format enabling the data to be compressed and execute fast analytical queries and reports on the table.<br>\nWhich of the following columnstore model will you use to achieve this?</p>\n\n<ol>\n\t<li>Clustered columnstore</li>\n\t<li>Non-clustered columnstore</li>\n\t<li>You can use both the models in the given scenario</li>\n\t<li>There is no model that highly compresses the data in the given scenario</li>\n</ol>","a":[{"id":1687101,"option":"1","correct":true},{"id":1687102,"option":"2","correct":false},{"id":1687103,"option":"3","correct":false},{"id":1687104,"option":"4","correct":false}]},{"q":"<p>While working with elastic pools in Azure SQL database, you found that the in-Memory OLTP storage is shared across all databases in the pool. This causes the usage in one database to potentially affect other databases.<br>\nWhat can you do to mitigate this issue?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Configure a Max-eDTU that is lower than the eDTU count for the pool as a whole</li>\n\t<li>Configure a Min-eDTU or MinvCore that is greater than 0</li>\n\t<li>Configure a Min-eDTU or MinvCore that is smaller than 0</li>\n\t<li>Configure a MaxvCore that is higher than the vCore count for the pool as a whole</li>\n</ol>","a":[{"id":1687097,"option":"1","correct":true},{"id":1687098,"option":"2","correct":false},{"id":1687099,"option":"3","correct":false},{"id":1687100,"option":"4","correct":false}]},{"q":"<p>You get an out-ofquota error and you are not able to insert or update the data when working with in-Memory OLTP in Azure SQL database.<br>\nWhat will be the right course of action to mitigate this issue in the given scenario?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Delete the data</li>\n\t<li>Increase the pricing tier of the database</li>\n</ol>\n\n<p> </p>","a":[{"id":1687093,"option":"1","correct":false},{"id":1687094,"option":"2","correct":false},{"id":1687095,"option":"Either 1 or 2","correct":true},{"id":1687096,"option":"None of these","correct":false}]},{"q":"<p>Which of the following items count towards your <em>In-Memory</em> OLTP storage cap when working with Azure SQL Database?</p>\n\n<ol>\n\t<li>Active user data rows in memory-optimized tables and table variables</li>\n\t<li>Indexes on memory-optimized tables</li>\n\t<li>Operational overhead of ALTER TABLE operations</li>\n</ol>","a":[{"id":1687089,"option":"1 and 2","correct":false},{"id":1687090,"option":"2 and 3","correct":false},{"id":1687091,"option":"1 and 3","correct":false},{"id":1687092,"option":"1, 2 and 3","correct":true}]},{"q":"<p>You decide to use the Azure SQL Database single databases using the DTU purchasing model. If you are going by the Basic Service Tier, what would be the Max DTUs in this tier?</p>","a":[{"id":1687085,"option":"2","correct":false},{"id":1687086,"option":"5","correct":true},{"id":1687087,"option":"7","correct":false},{"id":1687088,"option":"9","correct":false}]},{"q":"<p>You want to organize your in-memory data in Azure SQL Database in such a way that every row becomes a separate memory object.<br>\nWhich of the following methods will you use to do so?</p>\n\n<ol>\n\t<li>Memory-optimized rowstore</li>\n\t<li>Memory-optimized columnstore</li>\n\t<li>Memory-optimized Non-durable tables</li>\n\t<li>Memory-optimized durable tables</li>\n</ol>","a":[{"id":1687081,"option":"1","correct":true},{"id":1687082,"option":"2","correct":false},{"id":1687083,"option":"3","correct":false},{"id":1687084,"option":"4","correct":false}]},{"q":"<p>You decide to use the in-memory technology - clustered columnstore indexes to fit more data in your Azure SQL database and improve performance. How will you achieve this?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>By using it with fact tables in your data marts to fit more data in your database</li>\n\t<li>By concurrently running analytics queries very quickly on the same data</li>\n\t<li>By running an expensive extract, transform, and load (ETL) process</li>\n\t<li>By ingesting data from events or IoT devices</li>\n</ol>","a":[{"id":1687077,"option":"1","correct":true},{"id":1687078,"option":"2","correct":false},{"id":1687079,"option":"3","correct":false},{"id":1687080,"option":"4","correct":false}]},{"q":"<p>Which of these in-memory technology can you use in Azure SQL Database to do the following?<br>\na. Increase the number of transactions per second.<br>\nb. Reduce the latency for transaction processing.</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Clustered columnstore indexes</li>\n\t<li>In-Memory OLTP</li>\n\t<li>Nonclustered columnstore indexes</li>\n\t<li>Memory-optimized clustered columnstore indexes</li>\n</ol>","a":[{"id":1687073,"option":"1","correct":false},{"id":1687074,"option":"2","correct":true},{"id":1687075,"option":"3","correct":false},{"id":1687076,"option":"4","correct":false}]},{"q":"<p>You get the following error message when working with Azure SQL Database. SQL transaction commits failed. Which of these error codes will be displayed as a result of this?</p>","a":[{"id":1687069,"option":"SqlBatchWriteTimeout","correct":false},{"id":1687070,"option":"SqlUnauthorizedAccess","correct":false},{"id":1687071,"option":"SqlOpenConnectionTimeout","correct":false},{"id":1687072,"option":"SqlBatchWriteTransactionFailed","correct":true}]},{"q":"<p>You get the following error message when working with Azure SQL Database<br>\n\"Type '%dataType;' in source side cannot be mapped to a type that supported by sink side(column name:'%columnName;') in auto create the table.\"<br>\nWhich of the following is the best possible way to resolve this issue?</p>\n\n<ol>\n\t<li>Update the column type in mappings</li>\n\t<li>Manually create the sink table in the target server</li>\n\t<li>Verify the column in the query, structure in the dataset, and mappings in the activity</li>\n</ol>","a":[{"id":1687065,"option":"1","correct":false},{"id":1687066,"option":"2","correct":false},{"id":1687067,"option":"3","correct":false},{"id":1687068,"option":"Either 1 or 2","correct":true}]},{"q":"<p>You want to monitor pipelines, triggers, and integration runtimes in your Synapse workspace using the Monitor Hub. Under which of the following tabs in the Monitor Hub can this be achieved?</p>","a":[{"id":1687057,"option":"Under Integration","correct":true},{"id":1687058,"option":"Under Activities","correct":false},{"id":1687059,"option":"Under Runtime","correct":false},{"id":1687060,"option":"Under Requests","correct":false}]},{"q":"<p>You are creating a pipeline containing a copy activity that ingests data from Azure SQL Database into a dedicated Azure Synapse SQL pools. When doing so, you decide to use a previously created Azure Data Lake Storage Gen2 linked service to stage data before it loads into Azure Synapse Analytics by using PolyBase.<br>\nWhat happens to the interim data in Azure Data Lake Storage Gen2 after the copy is completed?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>It is automatically cleaned up.</li>\n\t<li>It is automatically validated</li>\n\t<li>It is stored in a new container</li>\n\t<li>It is stored in a new table</li>\n</ol>","a":[{"id":1687053,"option":"1","correct":true},{"id":1687054,"option":"2","correct":false},{"id":1687055,"option":"3","correct":false},{"id":1687056,"option":"4","correct":false}]},{"q":"<p>You are trying to interact with Azure Cosmos DB using Apache Spark in Azure Synapse Link. Which of the following are valid ways to query the Azure Cosmos DB analytical store from Spark when doing so?</p>\n\n<ol>\n\t<li>Load to Spark DataFrame</li>\n\t<li>Create Spark table</li>\n\t<li>Create a full-fidelity schema</li>\n</ol>","a":[{"id":1687049,"option":"1","correct":false},{"id":1687050,"option":"2","correct":false},{"id":1687051,"option":"3","correct":false},{"id":1687052,"option":"Both 1 and 2","correct":true}]},{"q":"<p>You want to build a serverless SQL pool database and views over Synapse Link for Azure Cosmos DB. Which of these are valid prerequisites to achieve this?</p>\n\n<ol>\n\t<li>Create a Synapse workspace named SynapseLinkBI.</li>\n\t<li>Enable Azure Synapse Link for your Azure Cosmos account</li>\n\t<li>Connect the Azure Cosmos database to the Synapse workspace.</li>\n</ol>","a":[{"id":1687045,"option":"Only 1 and 2","correct":false},{"id":1687046,"option":"Only 2 and 3","correct":false},{"id":1687047,"option":"Only 1 and 3","correct":false},{"id":1687048,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>You are trying to access an Azure Cosmos DB database from Azure Synapse Analytics studio. When doing so, you want to run large-scale analytics into Azure Cosmos DB without impacting your operational performance.<br>\nWhat should you do to achieve your objective?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Use a linked service to connect to an Azure Cosmos DB database</li>\n\t<li>Enable Synapse Link for Azure Cosmos DB</li>\n\t<li>Enable HTAP capability</li>\n</ol>","a":[{"id":1687041,"option":"1","correct":false},{"id":1687042,"option":"2","correct":true},{"id":1687043,"option":"3","correct":false},{"id":1687044,"option":"Either 1 or 2","correct":false}]},{"q":"<p>Assume that you have created serverless Apache Spark pools in your Synapse workspace to user Spark analytics. In the given scenario, which of these are valid ways to use Spark within Azure Synapse?</p>\n\n<ol>\n\t<li>Using Spark Notebooks</li>\n\t<li>Using Spark job definitions</li>\n</ol>","a":[{"id":1687037,"option":"Only 1","correct":false},{"id":1687038,"option":"Only 2","correct":false},{"id":1687039,"option":"Both 1 and 2","correct":true},{"id":1687040,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Which of the following can be achieved by using Azure Synapse Link for Azure Cosmos DB?</p>\n\n<ol>\n\t<li>You can run near real-time analytics over operational data in Azure Cosmos DB</li>\n\t<li>You can create fully isolated column store.</li>\n\t<li>You can run near real-time business intelligence pipelines</li>\n</ol>","a":[{"id":1687033,"option":"1","correct":true},{"id":1687034,"option":"2","correct":false},{"id":1687035,"option":"3","correct":false},{"id":1687036,"option":"None of these","correct":false}]},{"q":"<p>You are loading data using dedicated SQL pools in Azure Synapse Analytics. In the given scenario, what should you do to ensure that the loading speed is the fastest?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Running a single load job at a time</li>\n\t<li>Running multiple load jobs in parallel</li>\n\t<li>For larger jobs, scaling down the SQL pool before the load operation</li>\n\t<li>For smaller jobs, scaling up the SQL pool before the load operation</li>\n</ol>","a":[{"id":1687029,"option":"1","correct":true},{"id":1687030,"option":"2","correct":false},{"id":1687031,"option":"3","correct":false},{"id":1687032,"option":"4","correct":false}]},{"q":"<p>Which of the following wait types for dedicated SQL pools in Azure Synapse Analytics has the maximum value as 1?</p>\n\n<ol>\n\t<li>UserConcurrencyResourceType</li>\n\t<li>DmsConcurrencyResourceType</li>\n\t<li>BackupConcurrencyResourceType</li>\n\t<li>LocalQueriesConcurrencyResourceType</li>\n</ol>","a":[{"id":1687025,"option":"1","correct":false},{"id":1687026,"option":"2","correct":false},{"id":1687027,"option":"3","correct":true},{"id":1687028,"option":"4","correct":false}]},{"q":"<p>Which of the following SQL pool tools is the best option for loading and exporting large amounts of data in a faster manner in Synapse Analytics?</p>","a":[{"id":1687021,"option":"BCP","correct":false},{"id":1687022,"option":"PolyBase","correct":true},{"id":1687023,"option":"Azure Data Factory","correct":false},{"id":1687024,"option":"Azure Data Lake storage","correct":false}]},{"q":"<p>What is the correct sequence in which the operations given alongside takes place during a scale-up operation in Azure Synapse SQL?</p>","a":[{"id":1687017,"option":"(I) -> (III) -> (II)","correct":true},{"id":1687018,"option":"(I) -> (III) -> (IV)","correct":false},{"id":1687019,"option":"(I) -> (IV)","correct":false},{"id":1687020,"option":"(III) -> (I) -> (IV)","correct":false}]},{"q":"<p>Which of the following is a valid method of executing pipeline runs in Data Factory when working with Azure Synapse Analytics?</p>","a":[{"id":1687013,"option":"Dataset execution","correct":false},{"id":1687014,"option":"Manual execution","correct":false},{"id":1687015,"option":"Trigger execution","correct":false},{"id":1687016,"option":"Both 2 and 3","correct":true}]},{"q":"<p>You want to combine different Azure Open Datasets using serverless SQL pool. You then want to visualize the results in Synapse Studio for Azure Synapse Analytics.<br>\nIn the given scenario, which of these functions can you use to access files in Azure Storage?</p>","a":[{"id":1687009,"option":" OPENROWSET function ","correct":true},{"id":1687010,"option":"CAST function","correct":false},{"id":1687011,"option":"OPENFILESET function","correct":false},{"id":1687012,"option":"All 1, 2 or 3","correct":false}]},{"q":"<p>Which of the following can be used to create small tables with less than 60 million rows in Azure Synapse Analytics?</p>","a":[{"id":1687001,"option":"Heap tables","correct":true},{"id":1687002,"option":"Temporary tables","correct":false},{"id":1687003,"option":"Clustered indexes","correct":false},{"id":1687004,"option":"Non-clustered indexes","correct":false}]},{"q":"<p>In which of the given scenarios can you consider designing a hash-distributed table using a dedicated SQL pool in Azure Synapse Analytics?</p>\n\n<ol>\n\t<li>There is no obvious joining key</li>\n\t<li>The table is a temporary staging table</li>\n\t<li>The table has a frequent insert, update, and delete operations</li>\n\t<li>The table does not share a common join key with other tables</li>\n</ol>","a":[{"id":1686997,"option":"1","correct":false},{"id":1686998,"option":"2","correct":false},{"id":1686999,"option":"3","correct":true},{"id":1687000,"option":"4","correct":false}]},{"q":"<p>What is the maximum value to which MAXDOP (max_degree_of_parallelism) can be set while creating columnstore indexes in Azure Synapse Analytics?</p>","a":[{"id":1686993,"option":"1","correct":false},{"id":1686994,"option":"4","correct":false},{"id":1686995,"option":"16","correct":false},{"id":1686996,"option":"64","correct":true}]},{"q":"<p>You want to reduce the memory requirements for compressing rowgroups into columnstore indexes in Synapse SQL. Which of the following techniques can be used to do so?</p>","a":[{"id":1686989,"option":"Use fewer columns","correct":false},{"id":1686990,"option":"Use fewer string columns","correct":false},{"id":1686991,"option":"Avoid over-partitioning","correct":false},{"id":1686992,"option":"All of these","correct":true}]},{"q":"<p>Your company is planning to design a system that will consist of Azure virtual machine and an Azure SQL database. Since the database will not have internet connectivity, you need to implement a solution that can ensure the accessibility of the virtual machine to the database.<br>\nWhich of the following solutions will you choose to implement in the given scenario?</p>","a":[{"id":1686985,"option":"Add an application gateway","correct":false},{"id":1686986,"option":"Add a virtual network gateway","correct":false},{"id":1686987,"option":"Add an Azure load balancer","correct":false},{"id":1686988,"option":"Add a virtual network service endpoint","correct":true}]},{"q":"<p><strong>Scenario</strong></p>\n\n<p>Your company has been assigned a task to set up an Azure SQL database that will be used to store sensitive data. The following requirements need to be met by your solution:</p>\n\n<ul>\n\t<li>Encryption can be performed only by the application accessing the data.</li>\n\t<li>Access keys for encrypting and decrypting the data should be there with the client application.</li>\n\t<li>The data must not appear in plaintext in the database.</li>\n\t<li>The strongest encryption method must be used on the database</li>\n</ul>\n\n<p><strong>Problem</strong><br>\nWhich of the following would you use as the encryption method for Non-Searchable data in the given scenario?</p>\n\n<ol>\n\t<li>Always Encrypted with randomized encryption</li>\n\t<li>CREATE SYMMETRIC KEY statement</li>\n\t<li>Always Encrypted with deterministic encryption</li>\n\t<li>CREATE CERTIFICATE statement </li>\n</ol>","a":[{"id":1686981,"option":"1","correct":true},{"id":1686982,"option":"2","correct":false},{"id":1686983,"option":"3","correct":false},{"id":1686984,"option":"4","correct":false}]},{"q":"<p><strong>Scenario</strong></p>\n\n<p>Your company has been assigned a task to set up an Azure SQL database that will be used to store sensitive data. The following requirements need to be met by your solution:</p>\n\n<ul>\n\t<li>Encryption can be performed only by the application accessing the data</li>\n\t<li>Access keys for encrypting and decrypting the data should be there with the client application</li>\n\t<li>The data must not appear in plaintext in the database</li>\n\t<li>The strongest encryption method must be used on the database.</li>\n</ul>\n\n<p><strong>Problem</strong><br>\nWhich of the following would you use as the encryption method for Searchable data in the given scenario?</p>\n\n<ol>\n\t<li>Always Encrypted with randomized encryption</li>\n\t<li>CREATE SYMMETRIC KEY statement</li>\n\t<li>Always Encrypted with deterministic encryption</li>\n\t<li>CREATE CERTIFICATE statement </li>\n</ol>","a":[{"id":1686977,"option":"1","correct":false},{"id":1686978,"option":"2","correct":false},{"id":1686979,"option":"3","correct":true},{"id":1686980,"option":"4","correct":false}]},{"q":"<p>Your company uses an Azure SQL data warehouse. You have been asked to implement a solution that can ensure the availability of data in case of a data center failure. Also, the recovery point objective, in this case, should be an hour.<br>\nWhich of the following solutions will you implement in the given scenario?</p>\n\n<ol>\n\t<li>Restore the data warehouse from a geo-redundant backup</li>\n\t<li>Restore the data warehouse from a user-defined restore point</li>\n\t<li>Ensure to modify the Azure firewall rules of the data warehouse</li>\n</ol>","a":[{"id":1686973,"option":"1","correct":false},{"id":1686974,"option":"2","correct":true},{"id":1686975,"option":"3","correct":false},{"id":1686976,"option":"Both 1 and 2","correct":false}]},{"q":"<p>Your company is setting up an Azure SQL Data warehouse. The company wants to use users to use two-factor authentication when they access data from data warehouse using SQL Server Management Studio.<br>\nWhich of the following solutions should the company implement to achieve the desired objective?</p>\n\n<ol>\n\t<li>Azure AD Privileged Identity Management</li>\n\t<li>Azure AD Identity Protection</li>\n\t<li>Azure key vault</li>\n\t<li>Azure conditional access policies</li>\n</ol>","a":[{"id":1686969,"option":"1","correct":false},{"id":1686970,"option":"2","correct":false},{"id":1686971,"option":"3","correct":false},{"id":1686972,"option":"4","correct":true}]},{"q":"<p>In an SQL database, you are want to create SQL logins with limited administrative permissions.<br>\nWhen will you use the ALTER ROLE statement in the given scenario?</p>\n\n<ol>\n\t<li>To add the user account to dbmanager role </li>\n\t<li>To add the user account to loginmanager role</li>\n\t<li>To add the user account to dbmanager and loginmanager role</li>\n</ol>","a":[{"id":1686965,"option":"1","correct":false},{"id":1686966,"option":"2","correct":false},{"id":1686967,"option":"3","correct":true},{"id":1686968,"option":"None of these","correct":false}]},{"q":"<p>You want to create SQL logins with full administrative permissions in an Azure SQL Managed instance. Which of the following statements will you use to add the login to the sysadmin fixed server role in the given context?</p>","a":[{"id":1686961,"option":" ALTER SERVER ROLE","correct":true},{"id":1686962,"option":"CREATE LOGIN","correct":false},{"id":1686963,"option":"ALTER AD LOGIN","correct":false},{"id":1686964,"option":" CREATE SERVER ROLE","correct":false}]},{"q":"<p>You want to use the FileTable options while working with the CREATE and ALTER TABLE statements.<br>\nWhich of the following statements will hold true in the given scenario?</p>\n\n<ol>\n\t<li>FileTable options can't be used on SQL Database because FILESTREAM isn't supported</li>\n\t<li>FileTable options can be used on SQL Database because FILESTREAM is supported</li>\n\t<li>FileTable options can't be used on SQL Database even though FILESTREAM is supported</li>\n\t<li>FileTable options can be used on SQL Database even though FILESTREAM isn't supported</li>\n</ol>","a":[{"id":1686957,"option":"1","correct":true},{"id":1686958,"option":"2","correct":false},{"id":1686959,"option":"3","correct":false},{"id":1686960,"option":"4","correct":false}]},{"q":"<p>You are processing data using Azure Data lake storage. In which of these phases of this process would you identify the technology and processes that are used to acquire the source data?</p>","a":[{"id":1686953,"option":"Store","correct":false},{"id":1686954,"option":"Prep and train","correct":false},{"id":1686955,"option":"Model and Serve","correct":false},{"id":1686956,"option":"Ingestion","correct":true}]},{"q":"<p>You are using blob APIs and Data Lake Storage Gen2 APIs to operate on the same data. Which of these delimiters can you use while performing the List Blobs operation in the given context?</p>","a":[{"id":1686949,"option":"Backward slash (\\)","correct":false},{"id":1686950,"option":"Forward slash (/)","correct":true},{"id":1686951,"option":"Semicolon (;)","correct":false},{"id":1686952,"option":"Double Quote (\")","correct":false}]},{"q":"<p>You want to load data into Azure Synapse Analytics using the COPY statement. If your staging Data Lake Storage Gen2 is configured with an Azure Virtual Network endpoint then which of these types of authentication would you use to do so?</p>","a":[{"id":1686945,"option":"Managed identity authentication","correct":true},{"id":1686946,"option":"Account key authentication","correct":false},{"id":1686947,"option":"Service principal authentication","correct":false},{"id":1686948,"option":"Lookup activity property","correct":false}]},{"q":"<p>You are troubleshooting an error that appeared on your screen when deploying Azure Databricks in your VNet. When doing so, you find that the traffic from the control plane to workers is blocked causing the respective error.<br>\nWhich of these failures might have occurred in the given scenario?</p>","a":[{"id":1686941,"option":"Cluster Terminated","correct":false},{"id":1686942,"option":"Instances Unreachable","correct":true},{"id":1686943,"option":"Spark Startup Failure","correct":false},{"id":1686944,"option":"Unexpected Launch Failure","correct":false}]},{"q":"<p>You are attempting to rerun an Apache Spark write operation by canceling the currently running job in Azure Databricks. When doing so the error given alongside occurs. What could be the possible reason for this?</p>\n\n<ol>\n\t<li>The job is interrupted</li>\n\t<li>A temporary network issue occurred</li>\n\t<li>The cluster is terminated during the execution of the write operation </li>\n</ol>","a":[{"id":1686937,"option":"1","correct":false},{"id":1686938,"option":"2","correct":false},{"id":1686939,"option":"3","correct":false},{"id":1686940,"option":"Either 1 or 2 or 3","correct":true}]},{"q":"<p>A Databricks notebook returns the error given alongside because it is undergoing a memory bottleneck. Which of these is a possible reason for the memory bottleneck?</p>\n\n<ol>\n\t<li>Execution of memory-intensive operations on the driver</li>\n\t<li>Undesirable driver instance type for the load executed on the driver</li>\n\t<li>A large number of notebooks or jobs are running in parallel on the same cluster</li>\n</ol>","a":[{"id":1686933,"option":"1","correct":false},{"id":1686934,"option":"2","correct":false},{"id":1686935,"option":"3","correct":false},{"id":1686936,"option":"ALL of these","correct":true}]},{"q":"<p>What is NOT the cause of Job failure due to Azure Data Lake Storage (ADLS) CREATE limits?</p>\n\n<ol>\n\t<li>The current limit for the subscription is too low</li>\n\t<li>Your application creates a large number of small files</li>\n\t<li>The external applications create a large number of files</li>\n\t<li>You are running batch jobs on a shared interactive cluster</li>\n</ol>","a":[{"id":1686929,"option":"1","correct":false},{"id":1686930,"option":"2","correct":false},{"id":1686931,"option":"3","correct":false},{"id":1686932,"option":"4","correct":true}]},{"q":"<p>You have a streaming job in Databricks whose performance is degrading over time. However, when you start a new streaming job with the same configuration and the same source, it shows better performance than the existing job.<br>\n<br>\nWhich of the following is NOT a valid solution to fix this issue?</p>\n\n<ol>\n\t<li>Increase the cluster capacity</li>\n\t<li>Change the checkpoint directory</li>\n\t<li>Perform memory-intensive operations</li>\n\t<li>Avoid restarting old streaming jobs with the same checkpoint directories</li>\n</ol>","a":[{"id":1686925,"option":"1","correct":false},{"id":1686926,"option":"2","correct":false},{"id":1686927,"option":"3","correct":true},{"id":1686928,"option":"4","correct":false}]},{"q":"<p>Which of the following statements correctly describes the term “Databricks SQL endpoint”?</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>It is a representation of query visualizations and commentary</li>\n\t<li>It is a connection to a set of internal data objects for running queries</li>\n\t<li>It is a connection to a set of external data objects for running queries</li>\n\t<li>It is a graphical representation of the results obtained on running a query</li>\n</ol>","a":[{"id":1686921,"option":"1","correct":false},{"id":1686922,"option":"2","correct":true},{"id":1686923,"option":"3","correct":false},{"id":1686924,"option":"4","correct":false}]},{"q":"<p>Which of the following workspace objects in Databricks can be assigned the least number of permissions using workspace object access control?</p>","a":[{"id":1686917,"option":"Folders","correct":false},{"id":1686918,"option":"Notebooks","correct":false},{"id":1686919,"option":"MLflow Experiments","correct":true},{"id":1686920,"option":"MLflow Models","correct":false}]},{"q":"<p>You want to parameterize some notebooks when working with Azure Databricks. Which of these Databricks Utilities can be used to achieve this?</p>","a":[{"id":1686913,"option":"File system utilities","correct":false},{"id":1686914,"option":"Notebook workflow utilities","correct":false},{"id":1686915,"option":"Widget utilities","correct":true},{"id":1686916,"option":"Secrets utilities","correct":false}]},{"q":"<p>Which of the following Databricks cluster attributes cannot be restricted in a cluster policy?</p>\n\n<ol>\n\t<li>Libraries handled by Libraries API</li>\n\t<li>Number of clusters created per user </li>\n\t<li>Cluster permissions handled by a separate API</li>\n</ol>\n\n<p> </p>","a":[{"id":1686905,"option":"1","correct":false},{"id":1686906,"option":"2","correct":false},{"id":1686907,"option":"3","correct":false},{"id":1686908,"option":"All of these","correct":true}]},{"q":"<p>What is the basic requirement for running one or more notebook cells in Databricks?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. The notebook must be attached to the workspace</p>\n\n<p>2. The notebook must be attached to a cluster</p>\n\n<p>3. The notebook must be attached to a job</p>\n\n<p>4. The notebook must be attached to a library</p>","a":[{"id":1686901,"option":"1","correct":false},{"id":1686902,"option":"2","correct":true},{"id":1686903,"option":"3","correct":false},{"id":1686904,"option":"4","correct":false}]},{"q":"<p>What is the maximum limit of all-purpose Databricks clusters terminated in the last 30 days for which cluster configuration information can be retained?</p>","a":[{"id":1686897,"option":"0","correct":false},{"id":1686898,"option":"30","correct":false},{"id":1686899,"option":"50","correct":false},{"id":1686900,"option":"70","correct":true}]},{"q":"<p>Which of the following workspace assets can be used for running codes in Databricks?</p>","a":[{"id":1686893,"option":"Data","correct":false},{"id":1686894,"option":"Experiments","correct":false},{"id":1686895,"option":"Jobs","correct":true},{"id":1686896,"option":"Libraries","correct":false}]},{"q":"<p>Which of the following languages is not supported in Databricks notebooks and jobs?</p>","a":[{"id":1686889,"option":"R","correct":false},{"id":1686890,"option":"Java","correct":true},{"id":1686891,"option":"Python","correct":false},{"id":1686892,"option":"Scala","correct":false}]},{"q":"<p>You want to use Databricks runtime to run notebook jobs and interactive workloads. Which of these runtimes cannot be used to do so?</p>","a":[{"id":1686885,"option":"Databricks Light","correct":true},{"id":1686886,"option":"Databricks Runtime for Genomics","correct":false},{"id":1686887,"option":"Databricks Runtime for Machine Learning","correct":false},{"id":1686888,"option":"Both 2 and 3","correct":false}]},{"q":"<p>Which of the following visualization types is enabled by default in SQL Analytics?</p>","a":[{"id":1686881,"option":"Boxplot","correct":false},{"id":1686882,"option":"Chart","correct":false},{"id":1686883,"option":"Map","correct":false},{"id":1686884,"option":"Table","correct":true}]},{"q":"<p>Which of the following statements correctly describes \"Workspace User Interface\" in Databricks?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. It is the primary space where data practitioners perform their daily work</p>\n\n<p>2. It is a platform that enables you to keep track of your work on Databricks</p>\n\n<p>3. It is an online web app that you will work in to access Databricks functionality</p>\n\n<p>4. It is an integration tool that allows you to connect your custom applications to Databricks clusters</p>","a":[{"id":1686877,"option":"1","correct":false},{"id":1686878,"option":"2","correct":false},{"id":1686879,"option":"3","correct":true},{"id":1686880,"option":"4","correct":false}]},{"q":"<p>Which of the following parameter types can be used to restrict the scope of all possible parameter values when running a query?</p>","a":[{"id":1686873,"option":"Text","correct":false},{"id":1686874,"option":"Number","correct":false},{"id":1686875,"option":"Date and Time","correct":false},{"id":1686876,"option":"Drop Down List","correct":true}]},{"q":"<p>Which of these icons does the selection widget show when a date or date range parameter is added to a query?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. White lightning bolt icon</p>\n\n<p>2. Green lightning bolt icon</p>\n\n<p>3. Blue lightning bolt icon</p>\n\n<p>4. Yellow lightning bolt icon</p>","a":[{"id":1686869,"option":"1","correct":false},{"id":1686870,"option":"2","correct":false},{"id":1686871,"option":"3","correct":true},{"id":1686872,"option":"4","correct":false}]},{"q":"<p>Which of the following is the correct representation of a string as a query parameter?</p>","a":[{"id":1686865,"option":"( ( String ) )","correct":false},{"id":1686866,"option":"{ { String } }","correct":true},{"id":1686867,"option":"[ [ String ] ]","correct":false},{"id":1686868,"option":"< < String > >","correct":false}]},{"q":"<p>Which of the following are NOT valid multi-model features provided by Azure SQL family of products?<br>\n1. Graph features<br>\n2. Spatial features<br>\n3. Key-value pairs<br>\n4. XQuery features</p>","a":[{"id":1686861,"option":"Only 1","correct":false},{"id":1686862,"option":"Only 2","correct":false},{"id":1686863,"option":"Only 3","correct":false},{"id":1686864,"option":"Only 4","correct":true}]},{"q":"<p>What should you do if the number of databases in a pool approaches the maximum supported databases?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. Consider Resource management in dense elastic pools</p>\n\n<p>2. Consider assessing database utilisation pattern </p>\n\n<p>3. Consider multiple databases with persistent medium-high utilization</p>\n\n<p>4. Use standard pools</p>","a":[{"id":1686857,"option":"1","correct":true},{"id":1686858,"option":"2","correct":false},{"id":1686859,"option":"3","correct":false},{"id":1686860,"option":"4","correct":false}]},{"q":"<p>You notice a downtime for a period of time when moving databases out of a pool.<br>\nWhen does this downtime occur?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. When database connections are started</p>\n\n<p>2. When database connections are dropped</p>\n\n<p>3. When databases are moved</p>","a":[{"id":1686853,"option":"1","correct":false},{"id":1686854,"option":"2","correct":true},{"id":1686855,"option":"3","correct":false},{"id":1686856,"option":"Both 1 and 2","correct":false}]},{"q":"<p>How many servers are required for the databases in a Azure SQL Database elastic pool?</p>","a":[{"id":1686849,"option":"Single server","correct":true},{"id":1686850,"option":"Multiple server","correct":false},{"id":1686851,"option":"Two servers","correct":false},{"id":1686852,"option":"Cannot be determined","correct":false}]},{"q":"<p>You are using the Activity dispatch operation to route activity to a target compute service when working with Data Factory.<br>\nWhat is the benefit of doing so?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. There will be no need to scale up the compute size</p>\n\n<p>2. The elastically gets scaled up</p>","a":[{"id":1686845,"option":"1","correct":true},{"id":1686846,"option":"2","correct":false},{"id":1686847,"option":"Both 1 and 2","correct":false},{"id":1686848,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Which of the following can be used to manually run your pipeline in Azure Data Factory?<br>\n1. .NET SDK<br>\n2. Python SDK<br>\n3. Azure Synapse</p>","a":[{"id":1686841,"option":"Only 1 and 2","correct":true},{"id":1686842,"option":"Only 2 and 3","correct":false},{"id":1686843,"option":"Only 1 and 3","correct":false},{"id":1686844,"option":"All 1, 2 and 3","correct":false}]},{"q":"<p>Which of these Azure Data Factory control activities allows a pipeline to invoke another pipeline?</p>","a":[{"id":1686837,"option":"Append Pipeline","correct":false},{"id":1686838,"option":"Until Activity","correct":false},{"id":1686839,"option":"Set Pipeline","correct":false},{"id":1686840,"option":"Execute Pipeline","correct":true}]},{"q":"<p>You are running and monitoring a Power Query data wrangling activity in Data Factory.<br>\nIn the given scenario, what does \"Trigger now\" do once your pipeline is published?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1. It executes an on-demand run of the last published pipeline</p>\n\n<p>2. It schedules all existing Azure Data Factory triggers</p>\n\n<p>3. It pipelines the canvas</p>\n\n<p>4. It visualizes the output of a triggered Power Query activity run</p>","a":[{"id":1686833,"option":"1","correct":true},{"id":1686834,"option":"2","correct":false},{"id":1686835,"option":"3","correct":false},{"id":1686836,"option":"4","correct":false}]},{"q":"<p>Which of the following can be used as a source dataset for a Power Query mash-up in Azure Data Factory?</p>\n\n<ol>\n\t<li>An existing dataset</li>\n\t<li>Create a new dataset</li>\n\t<li>Select a wrangling dataset</li>\n</ol>","a":[{"id":1686829,"option":"1","correct":false},{"id":1686830,"option":"2","correct":false},{"id":1686831,"option":"3","correct":false},{"id":1686832,"option":"Either 1 or 2","correct":true}]},{"q":"<p>You are creating a data factory when transforming data with mapping data flows. Which of the following steps should you take when doing so?<br>\n1. Select a globally unique name for the Azure data factory.<br>\n2. Select an existing resource group or create a new one.<br>\n3. Select a location for the data factory</p>","a":[{"id":1686825,"option":"Only 1 and 2","correct":false},{"id":1686826,"option":"Only 1 and 3","correct":false},{"id":1686827,"option":"Only 2 and 3","correct":false},{"id":1686828,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>When scaling out, you observe that you have a number of query replicas in the query pool.<br>\nIn the given scenario, how would the processing workloads be distributed among these query replicas?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Based on Round Robin technique</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Not distributed among query replicas</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Distributed among primary, secondary and query server</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>On FCFS basis</code></pre>\n\n<p> </p>","a":[{"id":1686821,"option":"1","correct":false},{"id":1686822,"option":"2","correct":true},{"id":1686823,"option":"3","correct":false},{"id":1686824,"option":"4","correct":false}]},{"q":"<p>How does the DirectQuery mode achieve parity with the in-memory models on Azure analysis service?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Through support for a wide array of data sources</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>With the help of extensions</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Through database roles</code></pre>\n\n<p> </p>","a":[{"id":1686817,"option":"1","correct":true},{"id":1686818,"option":"2","correct":false},{"id":1686819,"option":"3","correct":false},{"id":1686820,"option":"None of these","correct":false}]},{"q":"<p>Which of the following can you automate by using service principals with Azure Automation in Analysis Services?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Provisioning servers</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Deploying models</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Data refresh</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>Scale up/down</code></pre>\n\n<p>5.</p>\n\n<pre class=\"prettyprint\"><code>Pause/resume</code></pre>\n\n<p> </p>","a":[{"id":1686813,"option":"a & c","correct":false},{"id":1686814,"option":"a,b and c","correct":false},{"id":1686815,"option":"a,b,c and e","correct":false},{"id":1686816,"option":"a,b,c,d and e","correct":true}]},{"q":"<p>Which of these data modelling constructs can you change depending on need and available resources for each region when working with Azure Analysis Service?<br>\n<strong>Options:\\</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Plan Availability</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Query Replica Availability</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Multidimensional models</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>PowerPivot in SharePoint</code></pre>\n\n<p> </p>","a":[{"id":1686809,"option":"a & c","correct":false},{"id":1686810,"option":"a,b and c","correct":true},{"id":1686811,"option":"a and b","correct":false},{"id":1686812,"option":"a and d","correct":false}]},{"q":"<p>Which of the given statements is true?</p>\n\n<p><strong>Statements</strong></p>\n\n<p>S1:Synchronization is allowed only when there are no replicas in the query pool.<br>\nS2: During scale-out operations, all servers in the query pool, including the primary server, are temporarily offline.</p>","a":[{"id":1686805,"option":"Only S1","correct":false},{"id":1686806,"option":"Only S2","correct":true},{"id":1686807,"option":"Both S1 and S2","correct":false},{"id":1686808,"option":"Neither S1 nor S2 ","correct":false}]},{"q":"<p>Which of the following serves as the processing server during Scale out in Azure analysis service?</p>","a":[{"id":1686801,"option":"Primary server","correct":true},{"id":1686802,"option":"Secondary server","correct":false},{"id":1686803,"option":"Model Database","correct":false},{"id":1686804,"option":"Query Pool ","correct":false}]},{"q":"<p>Which of the following unit of measurement can be used to enhance relative computational performance when working with Azure Analysis Services?</p>","a":[{"id":1686797,"option":"Query Pool","correct":false},{"id":1686798,"option":"MPU","correct":false},{"id":1686799,"option":"SLA","correct":false},{"id":1686800,"option":"QPU","correct":true}]},{"q":"<p>In which of the following modes do the tabular models in the Azure analysis service run?</p>","a":[{"id":1686793,"option":"In-memory mode ","correct":false},{"id":1686794,"option":"DirectQuery mode","correct":false},{"id":1686795,"option":"Both 1 and 2","correct":true},{"id":1686796,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>How can you grant access to a non-administrative end user so that they can use Analysis Services?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>By adding it through gateway</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Add its data using INSERT query</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Through database roles</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>It is be automatically accessible </code></pre>\n\n<p> </p>","a":[{"id":1686789,"option":"1","correct":false},{"id":1686790,"option":"2","correct":false},{"id":1686791,"option":"3","correct":true},{"id":1686792,"option":"4","correct":false}]},{"q":"<p>You have used an account when creating a server in Azure analysis service.<br>\nWhat can you do to include this account in the server admins role?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>Use the INSERT query</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>Add the account through a gateway</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>Use the Query Processing Units (QPU)</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>It will be automatically included</code></pre>\n\n<p> </p>","a":[{"id":1686785,"option":"1","correct":false},{"id":1686786,"option":"2","correct":false},{"id":1686787,"option":"3","correct":false},{"id":1686788,"option":"4","correct":true}]},{"q":"<p>What determines the number of managed instances that can be deployed in the subnet of a Virtual network when deploying a Azure SQL Managed Instance?</p>","a":[{"id":1678397,"option":"The size of the subnet or subnet range.","correct":true},{"id":1678398,"option":"The tier you selected during provisioning","correct":false},{"id":1678399,"option":"The IP addresses in a subnet","correct":false},{"id":1678400,"option":"None of these","correct":false}]},{"q":"<p>You want to migrate your user databases from SQL Server to an instance of SQL Server on Azure Virtual Machines (VMs).</p>\n\n<p>In which of the following should the SQL Server run in order to achieve this?<br>\n1. Amazon Web Services (AWS) EC2<br>\n2. SQL Server on Virtual Machines<br>\n3. Amazon Relational Database Service</p>","a":[{"id":1678393,"option":"2 and 3","correct":false},{"id":1678394,"option":"1 and 3","correct":false},{"id":1678395,"option":"1 and 2","correct":false},{"id":1678396,"option":"1, 2 and 3","correct":true}]},{"q":"<p>You are migrating an Azure SQL Database from the DTU-based purchasing model to the vCore-based purchasing model. Which of the following actions is similar to the above scenario?</p>\n\n<p><strong>Actions</strong></p>\n\n<p>1. Calling between service objectives in the Basic service tier with similar duration and a minimal downtime at the end of the migration process</p>\n\n<p>2. Calling between service objectives in the Standard service tier with similar duration and a minimal downtime at the end of the migration process</p>\n\n<p> </p>","a":[{"id":1678389,"option":"1","correct":false},{"id":1678390,"option":"2","correct":false},{"id":1678391,"option":"Both 1 and 2","correct":true},{"id":1678392,"option":"None of these","correct":false}]},{"q":"<p>You are migrating a database from SQL server to Azure SQL using Database Migration Service. Which of the following resource providers should you register in your subscription when doing so?</p>","a":[{"id":1678385,"option":"Microsoft.DataMigration ","correct":true},{"id":1678386,"option":"Microsoft.MigrationSource","correct":false},{"id":1678387,"option":"Microsoft.MigrationTarget","correct":false},{"id":1678388,"option":"Both 1 and 2","correct":false}]},{"q":"<p>Which of the following connections are allowed when the <em>Deny public network access</em> is set to \"Yes\" in Azure SQL connectivity settings?</p>","a":[{"id":1678381,"option":"Connections via private endpoints","correct":true},{"id":1678382,"option":"Connections via public endpoints","correct":false},{"id":1678383,"option":"Connections with virtual-network-based firewall rules","correct":false},{"id":1678384,"option":"Connections using Azure Private Link","correct":false}]},{"q":"<p>You have created a server-level firewall rule using the Azure portal when working with Azure SQL Database. In the given scenario, which of the following firewall rules enables all Azure services to pass through the server-level firewall rule?</p>","a":[{"id":1678377,"option":"0.0.0.0 ","correct":true},{"id":1678378,"option":"1.1.1.1","correct":false},{"id":1678379,"option":"0.1.0.1","correct":false},{"id":1678380,"option":"1.0.1.0","correct":false}]},{"q":"<p>On which of the following clusters is the <em>Premium/Business Critical service tier model</em> of the Azure SQL Database based?<br>\n1. It is based on a cluster of database engine processes<br>\n2. It is based on a cluster available on the database engine nodes<br>\n3. It is based on a cluster that has a minimal performance impact on your workload</p>","a":[{"id":1678373,"option":"1","correct":false},{"id":1678374,"option":"2 and 3","correct":false},{"id":1678375,"option":"1 and 3","correct":false},{"id":1678376,"option":"1, 2 and 3","correct":true}]},{"q":"<p>Which of these Azure SQL database services tiers is <em>not</em> available for SQL Managed Instances?</p>","a":[{"id":1678369,"option":"Hyperscale","correct":true},{"id":1678370,"option":"Compute","correct":false},{"id":1678371,"option":"Buffer","correct":false},{"id":1678372,"option":"Column Store","correct":false}]},{"q":"<p>What does <em>Azure AD authentication</em> use to authenticate identities at the database level in Azure SQL Database?</p>","a":[{"id":1678357,"option":"Role based access control","correct":false},{"id":1678358,"option":"Access keys","correct":false},{"id":1678359,"option":"Shared access signature","correct":false},{"id":1678360,"option":"A contained database user","correct":true}]},{"q":"<p>You are using the <em>point-in-time restore</em> to recover from data corruptions when working with Azure SQL Database. However, the maximum retention period is not sufficient for your application. Which of the following retention policies can you use to store your backups for a long period of time?</p>","a":[{"id":1678353,"option":"Long-term retention policy","correct":true},{"id":1678354,"option":"Short-term retention policy","correct":false},{"id":1678355,"option":"Database retention policy","correct":false},{"id":1678356,"option":"Backup retention policy","correct":false}]},{"q":"<p>You decide to use Auto-failover group to enable business continuity in case of datacenter outage when working with Azure SQL Database. What will happen as a result of this?</p>","a":[{"id":1678349,"option":"Application will automatically recover","correct":true},{"id":1678350,"option":"Readable replicas will be created","correct":false},{"id":1678351,"option":"Row versions will be restored from any point in time","correct":false},{"id":1678352,"option":"Backups will be kept for 10 years","correct":false}]},{"q":"<p>You want to create readable replicas and manual failover to any replica in case of a datacenter outage. Which of the following business continuity features of Azure SQL Database can be used to achieve this?</p>","a":[{"id":1678345,"option":"Temporal tables","correct":false},{"id":1678346,"option":" Point in Time Restore","correct":false},{"id":1678347,"option":"Active geo-replication","correct":true},{"id":1678348,"option":"Auto-failover group","correct":false}]},{"q":"<p>A set of data is transferred from an Azure Data Lake Storage account using Azure Data Factory. This data is then loaded into a data warehouse in Azure Synapse using Azure Polybase. Also, the data in the storage account is accessed via a virtual network service endpoint.<br>\nWhich of the following authentication method will you use to access the data in storage account?</p>","a":[{"id":1678341,"option":"Managed identity authentication","correct":true},{"id":1678342,"option":"Account key authentication","correct":false},{"id":1678343,"option":"Service principal authentication","correct":false},{"id":1678344,"option":"Shared access key authentication","correct":false}]},{"q":"<p>You are trying to connect to Synapse workspace resources from a restricted network. When doing so, you want to access the Azure Synapse Analytics Studio. What can you do to achieve this?</p>","a":[{"id":1678337,"option":"Create a private endpoint from the Azure portal.","correct":true},{"id":1678338,"option":"Create a Region value from the Azure portal","correct":false},{"id":1678339,"option":"Both 1 and 2","correct":false},{"id":1678340,"option":"None of these","correct":false}]},{"q":"<p>You have set the Analytical Time-to-Live (TTL) on a container using the <em>AnalyticalStoreTimeToLiveInSeconds</em> property when working with Azure Synapse Analytics.<br>\n<br>\nWhat among the following will happen if you set the value of <em>TTL as \"-1\"</em>?<br>\n1. The analytical store will have infinite retention of your operational data<br>\n2. The analytical store will retain all historical data, irrespective of the retention of the data in the transactional store<br>\n3. The items will expire from the analytical store \"n\" seconds after their last modified time in the transactional store</p>","a":[{"id":1678333,"option":"1 and 3","correct":false},{"id":1678334,"option":"2 and 3","correct":false},{"id":1678335,"option":"1 and 2","correct":true},{"id":1678336,"option":"1, 2 and 3","correct":false}]},{"q":"<p>You are using the <em>FIELDTERMINATOR ='field_terminator'</em><strong> </strong>option to adjust parsing rules to custom CSV format when querying CSV source data in Synapse SQL. Which of the following will be considered as the default field terminator in the given scenario?</p>","a":[{"id":1678329,"option":"Colon (“:”)","correct":false},{"id":1678330,"option":"Comma (\",\")","correct":true},{"id":1678331,"option":"Semicolon (“;”)","correct":false},{"id":1678332,"option":"Newline character (\\r\\n)","correct":false}]},{"q":"<p>You want to manage some historical data in temporary tables in Azure SQL database. To do so, you want to move the oldest portion of the historical data from the history table and keep the size of the retained part constant in terms of age. Which of the following approach will be suitable to achieve this?</p>","a":[{"id":1678321,"option":"Table Partitioning","correct":true},{"id":1678322,"option":"Custom Cleanup Script","correct":false},{"id":1678323,"option":"Either Table Partitioning or Custom Cleanup Script","correct":false},{"id":1678324,"option":"None of these","correct":false}]},{"q":"<p>You are doing a <em>Run All</em> in a Databricks notebook which consists of steps for both mount and unmounts. Which of these issues could this lead to?</p>","a":[{"id":1678317,"option":"It leads to a race condition","correct":false},{"id":1678318,"option":"It can possibly corrupt the mount points","correct":false},{"id":1678319,"option":"Both 1 and 2","correct":true},{"id":1678320,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You are getting the following error message when working with Azure Synapse Analytics.<br>\n“This job was rejected because it requires 24 AUs. This account's administrator-defined policy prevents a job from using more than 5 AUs”.</p>\n\n<p>Which of the following could be a potential reason behind this problem?</p>","a":[{"id":1678309,"option":"Throttling on Data Lake Analytics","correct":true},{"id":1678310,"option":"Incorrect path to the U-SQL file","correct":false},{"id":1678311,"option":"Incorrect Azure Active Directory tenant","correct":false},{"id":1678312,"option":"Incorrect Data Lake Analytics account in the linked service ","correct":false}]},{"q":"<p>You get the following error message when working with Azure Synapse Analytics: <em>“Cannot find the 'Azure Data Lake Store' file or folder”.</em><br>\nWhich of the following is a valid way to resolve this?</p>","a":[{"id":1678305,"option":"Increase the limits on Data Lake Analytics","correct":false},{"id":1678306,"option":"Reduce the number of submitted jobs to Data Lake Analytics","correct":false},{"id":1678307,"option":"Verify the path and credentials provided in the linked service","correct":true},{"id":1678308,"option":"Change Data Factory triggers and concurrency settings on activities","correct":false}]},{"q":"<p>You notice that the definition for an Azure function activity in Synapse Analytics is not complete. Which of the error messages given alongside will be thrown in the given scenario?</p>\n\n<p>1. Azure function activity missing function key.<br>\n2. Azure function activity missing function name.<br>\n3. Azure function activity missing Method in JSON.<br>\n4. Azure function activity missing LinkedService definition in JSON.</p>","a":[{"id":1678301,"option":"Either 3 or 4","correct":false},{"id":1678302,"option":"Either 1, 2 or 3","correct":false},{"id":1678303,"option":"Either 1 or 2","correct":false},{"id":1678304,"option":"Either 1, 2,3 or 4","correct":true}]},{"q":"<p>In which of the given scenarios does a Azure Synapse workspace use a default storage container?<br>\n1. Storing the backing data files for Spark tables<br>\n2. Execution logs for Spark jobs<br>\n3. Managing libraries that you choose to install</p>","a":[{"id":1678297,"option":"1, 2","correct":false},{"id":1678298,"option":"2, 3","correct":false},{"id":1678299,"option":"1, 3","correct":false},{"id":1678300,"option":"1, 2, 3","correct":true}]},{"q":"<p>You observe that copying data into Azure SQL Database from an Azure Synapse Analytics datastore is very slow. When analyzing this problem, the root cause turns out to be triggered by the bottleneck of the Azure SQL Database. What might be the possible reason behind this bottleneck?</p>","a":[{"id":1678293,"option":"Indexes are not set properly","correct":true},{"id":1678294,"option":"Azure SQL Database tier is too high","correct":false},{"id":1678295,"option":"Azure SQL Database DTU usage is less than 100%","correct":false},{"id":1678296,"option":"Instead of stored procedure, bulk insert is being used","correct":false}]},{"q":"<p>Which of the security groups should be assigned to the following users in your Azure Synapse Analytics workspace?</p>\n\n<p><strong>Users</strong><br>\n1. Users who need complete control over the workspace.<br>\n2. Users who need to manage and monitor Apache Spark pools and Integration runtimes.</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdmins\n2. workspace1_SynapseAdministrators</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdmins\n2. workspace1_SynapseCredentialUsers</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdministrators\n2. workspace1_SynapseComputeOperators</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>1. workspace1_SynapseAdministrators\n2. workspace1_SynapseContributors</code></pre>\n\n<p> </p>","a":[{"id":1678289,"option":"1","correct":false},{"id":1678290,"option":"2","correct":false},{"id":1678291,"option":"3","correct":true},{"id":1678292,"option":"4","correct":false}]},{"q":"<p>You want to represent a high-level representation of usage across a SQL pool in your Azure Synapse Analytics workspace.<br>\nWhich of these Dedicated SQL pool metrics can be used to do so?<br>\n1. DWULimit<br>\n2. DWUUsedPercent<br>\n3. DWUUsed</p>","a":[{"id":1678285,"option":"Only 2","correct":false},{"id":1678286,"option":"Either 1 or 3","correct":false},{"id":1678287,"option":"Either 1 or 2","correct":false},{"id":1678288,"option":"Either 2 or 3","correct":true}]},{"q":"<p>Which one of the following is <em>not</em> a potential cause for poor segment quality in Azure Synapse Analytics columnstore indices?</p>","a":[{"id":1678281,"option":"Too many partitions","correct":false},{"id":1678282,"option":"Too many string columns","correct":true},{"id":1678283,"option":"Memory pressure when index was built","correct":false},{"id":1678284,"option":"High volume of DML operations","correct":false}]},{"q":"<p>What is the maximum memory required to compress one row group in Azure Synapse SQL?</p>\n\n<p><strong>Options</strong></p>\n\n<p>1.</p>\n\n<pre class=\"prettyprint\"><code>72 MB + #rows * #columns * 8 bytes + #rows * #short-string-columns * 32 bytes + #long-string-columns * 16 MB for compression dictionary</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>32 MB+#rows * #columns * 8 bytes + #rows * #short-string-columns * 32 bytes + #long-string-columns * 16 MB for compression dictionary</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>72 MB + #rows * #columns * 8 bytes + #rows * #short-string-columns * 16 bytes + #long-string-columns * 16 MB for compression dictionary</code></pre>\n\n<p>4.</p>\n\n<pre class=\"prettyprint\"><code>64 MB+#rows * #columns * 8 bytes + #rows * #short-string-columns * 32 bytes </code></pre>\n\n<p> </p>","a":[{"id":1678277,"option":"1","correct":true},{"id":1678278,"option":"2","correct":false},{"id":1678279,"option":"3","correct":false},{"id":1678280,"option":"4","correct":false}]},{"q":"<p>What is the maximum retention period after which Azure Synapse Dedicated pool automatically deletes a restoration point?</p>","a":[{"id":1678273,"option":"24 hours","correct":false},{"id":1678274,"option":"1 week","correct":true},{"id":1678275,"option":"14 days","correct":false},{"id":1678276,"option":"1 month","correct":false}]},{"q":"<p>Which of the following statements regarding sharding patterns in Azure Synapse Analytics is <em>not</em> correct?</p>","a":[{"id":1678265,"option":"A round-robin distributed table distributes data evenly across a table but without any further optimization","correct":false},{"id":1678266,"option":"A replicated table provides the fastest query performance for small tables","correct":false},{"id":1678267,"option":"A round-robin table delivers slow performance when used as a staging table for loads","correct":true},{"id":1678268,"option":"A hash distributed table can deliver the highest query performance for joins and aggregations on large tables","correct":false}]},{"q":"<p>Assume that you have created a network connection between Azure Synapse Analytics Studio and a workstation within a restricted network. In the given scenario, which of these network outbound security rules is optional?</p>","a":[{"id":1678261,"option":"AzureMonitor","correct":true},{"id":1678262,"option":"AzureActiveDirectory","correct":false},{"id":1678263,"option":"AzureResourceManager","correct":false},{"id":1678264,"option":"AzureFrontDoor.Frontend","correct":false}]},{"q":"<p>Assume that you have created an Azure Synapse workspace using the Azure portal. In the given scenario, which of these methods can you use to open Synapse studio after the workspace is created?</p>","a":[{"id":1678257,"option":"Go to the https://web.azuresynapse.net and sign in to your workspace.","correct":false},{"id":1678258,"option":"Open your Synapse workspace in the Azure portal. Select Launch Synapse Studio from the top of the Overview section","correct":false},{"id":1678259,"option":"Either 1 or 2","correct":true},{"id":1678260,"option":"None of these","correct":false}]},{"q":"<p>You want to configure and use Azure Active Directory authentication when working with Azure Synapse Analytics. What is the correct sequence in which the steps given below need to be followed to achieve this?</p>\n\n<p>1. Create an Azure Active Directory identity<br>\n2. Create and populate Azure Active Directory<br>\n3. Connect to Synapse Studio by using Azure Active Directory identities<br>\n4.  Assign role to created Azure Active Directory identity in Synapse workspace</p>","a":[{"id":1678253,"option":"2, 1, 4, 3","correct":true},{"id":1678254,"option":"1, 2, 3, 4","correct":false},{"id":1678255,"option":"4, 3, 2, 1","correct":false},{"id":1678256,"option":"3, 4, 1, 2","correct":false}]},{"q":"<p>Which of the following statements are valid with respect to creating an Azure Synapse workspace using the Azure portal?<br>\n1. You will not be able to move the workspace to another Azure Active Directory tenant.<br>\n2. You cannot create a Synapse Analytics workspace in a Cloud Solution Provider (CSP) subscription.</p>","a":[{"id":1678249,"option":"Only 1","correct":false},{"id":1678250,"option":"Only 2","correct":false},{"id":1678251,"option":"All of these","correct":true},{"id":1678252,"option":"None of these","correct":false}]},{"q":"<p>Which of the following built-in database roles is most commonly used to grant full permission to users in Azure Synapse Analytics?</p>","a":[{"id":1678245,"option":"db_owner","correct":true},{"id":1678246,"option":"db_ddladmin","correct":false},{"id":1678247,"option":"db_datawriter","correct":false},{"id":1678248,"option":"db_datareader","correct":false}]},{"q":"<p>There is a company planning to use the Azure SQL data warehouse service for its requirement where data would be uploaded to the data warehouse every week. Checks would be made every time the data is uploaded to ensure that the data is not corrupted. If the data is corrupted, the uploaded data has to be removed.<br>\nAdditionally, the upload process and data corruption check process must not impact the processes running against the warehouse. Which of the following solutions would meet the requirement of the company discussed above?</p>","a":[{"id":1678241,"option":"The company can configure transactions and then perform a rollback if data corruption is detected","correct":false},{"id":1678242,"option":"The company can create user-defined restore points before the data is uploaded and then delete the restore point after the corruption checks are complete","correct":true},{"id":1678243,"option":"The company can configure database-level auditing and set a retention period as part of implementation process","correct":false},{"id":1678244,"option":"Both 1 and 2","correct":false}]},{"q":"<p>A company wants to use the Azure SQL database service. Given that business apps will be accessing the database and the application data must be available in the event of a region-wide outage. Below are the key requirements that need to be fulfilled:<br>\n1. Data must be available in the secondary region if the primary region goes down.<br>\n2. The storage and compute layers for the SQL database must be integrated and replicated together.</p>\n\n<p>Which of the following would you use as the Service tier for the database?</p>","a":[{"id":1678233,"option":"Basic \r\n","correct":false},{"id":1678234,"option":"Standard","correct":false},{"id":1678235,"option":"Premium ","correct":true},{"id":1678236,"option":"High","correct":false}]},{"q":"<p>You have been assigned a task to deploy a set of databases using the Azure SQL database service. The databases will be organized into separate groups based on their usage during the deployment of the Azure SQL. Additionally, you need to ensure that there is a defined maximum limit on the resources that will be able for each group.<br>\nWhich of the following solutions will you recommend for meeting the requirements in the given scenario?</p>","a":[{"id":1678229,"option":"Azure SQL Database Hyperscale","correct":false},{"id":1678230,"option":"Azure SQL Database Single Instance","correct":false},{"id":1678231,"option":"Azure SQL Database Elastic Pools","correct":true},{"id":1678232,"option":"Azure SQL Database Sharding","correct":false}]},{"q":"<p>Your company is using an Azure SQL database containing tables and columns having sensitive data. You have been asked to ensure that when this sensitive data is accessed from the columns, it is encrypted in transit. Which of the following would you use in the given scenario?</p>","a":[{"id":1678225,"option":"Dynamic data masking","correct":false},{"id":1678226,"option":"Always encrypted","correct":true},{"id":1678227,"option":"Row-level security","correct":false},{"id":1678228,"option":"Transparent data encryption ","correct":false}]},{"q":"<p>You are tasked with building an Azure SQL database service for certain business apps in your company. You have already decided to use an Azure SQL database managed instance to make the data available to the business apps via this service.<br>\nAdditionally, you need to ensure that, in case of full or partial loss of the Azure SQL database service in the primary region, the database will automatically recover from the loss.<br>\nWhich of the following can be used to achieve the desired objective?</p>","a":[{"id":1678221,"option":"Azure SQL data sync","correct":false},{"id":1678222,"option":"SQL Replication","correct":false},{"id":1678223,"option":"Failover-groups","correct":true},{"id":1678224,"option":"Active geo-replication","correct":false}]},{"q":"<p>Your company is using an on-premise Microsoft SQL Server and wants to migrate the database to Azure SQL databases. Which of the following must be used as an underlying storage type for the exported data in the given scenario?</p>","a":[{"id":1678217,"option":"Disk","correct":false},{"id":1678218,"option":"Blob","correct":true},{"id":1678219,"option":"File","correct":false},{"id":1678220,"option":"Queue","correct":false}]},{"q":"<p>You are configuring the SQL database to store resource usage, workers, and sessions. Which of the following benefits would you get by connecting Azure SQL Database to Azure Monitor logs in the given scenario?<br>\n<strong>Benefits</strong><br>\na) You can archive vast amounts of telemetry for a small price.<br>\nb) You can perform integration of SQL Database telemetry with your custom monitoring solution or hot pipelines<br>\nc) You can perform identification of the potential performance issues<br>\nd) You will have a built-in monitoring solution with reporting, alerting, and mitigating capabilities</p>","a":[{"id":1678213,"option":"Only (b)","correct":false},{"id":1678214,"option":"(b), (c) and (d)","correct":false},{"id":1678215,"option":"(b) and (d)","correct":false},{"id":1678216,"option":"Only (d)","correct":true}]},{"q":"<p>You are working on an application where you would be using Azure SQL database and Azure storage accounts. The functions of the application include extracting data, converting it to text documents, and storing them in storage accounts where the text documents would be accessible from an SMB network share.<br>\nWhich of the following will you use as the underlying service type for the Azure storage account in the given scenario?</p>","a":[{"id":1678205,"option":"Blob","correct":false},{"id":1678206,"option":"Queue","correct":false},{"id":1678207,"option":"Files","correct":true},{"id":1678208,"option":"Table","correct":false}]},{"q":"<p>Which of these Azure SQL Database purchasing models can automatically do the following tasks?<br>\n<strong>Tasks</strong><br>\n1. Pause databases during inactive periods when only storage is billed<br>\n2. Resume databases when activity returns.</p>","a":[{"id":1678201,"option":"Tabular Model","correct":false},{"id":1678202,"option":"POSIX-style model","correct":false},{"id":1678203,"option":"DTU-based purchasing model","correct":false},{"id":1678204,"option":"Serverless model","correct":true}]},{"q":"<p>You want to use automatic scaling property after building your app on a single Azure SQL database. Which of the following would you use to do so?<br>\na) Scripts<br>\nb) Dynamic scalability<br>\nc) Elastic Pool</p>","a":[{"id":1678185,"option":"c","correct":false},{"id":1678186,"option":"b and c","correct":false},{"id":1678187,"option":"a and b","correct":false},{"id":1678188,"option":"a and c","correct":true}]},{"q":"<p>You have an Azure SQL single database in the <em>general-purpose service tier</em>. You want to ensure that this database can transparently respond to rapidly changing resource requirements. Which of these properties would you configure to meet your objective?</p>","a":[{"id":1678181,"option":"Dynamic scalability","correct":true},{"id":1678182,"option":"SQL Managed Instance","correct":false},{"id":1678183,"option":"Elastic Pool","correct":false},{"id":1678184,"option":"autoscale","correct":false}]},{"q":"<p>For which of the following items would you need to set values for, while creating a Data Lake Analytics account?</p>\n\n<p>a) Subscription<br>\nb) Resource Group<br>\nc) Server Address<br>\nd) Location</p>","a":[{"id":1678173,"option":"(a) and (c)","correct":false},{"id":1678174,"option":"(a), (c) and (d)","correct":false},{"id":1678175,"option":"(b) and (c)","correct":false},{"id":1678176,"option":"(a), (b) and (d)","correct":true}]},{"q":"<p>Assume that you have attached the<strong> </strong><em>Azure Data Lake Gen 2 storage</em> to a workspace for configuring the Dataflow Storage. Now, you want to detach Azure Data Lake Gen 2 storage from the workspace. How would you be able to do this?</p>","a":[{"id":1678169,"option":"First ensure the firewall is disconnected then select Disconnect in workspace settings","correct":false},{"id":1678170,"option":"First ensure no dataflows in the workspace are deleted then Disconnect","correct":false},{"id":1678171,"option":"First ensure all dataflows in the workspace are deleted then Disconnect","correct":true},{"id":1678172,"option":"select Disconnect in the workspace settings","correct":false}]},{"q":"<p>Which of the following users can see the <em>\"Admin Log In tab\"</em> on the sign-in page, as shown alongside when setting up a single<strong> </strong><em>sign-on</em> in Databricks?</p>","a":[{"id":1678165,"option":"Admin user with \"No permission\"","correct":false},{"id":1678166,"option":"Admin user with \"Can use permission\"","correct":true},{"id":1678167,"option":"Non-admin user with \"No permission\"","correct":false},{"id":1678168,"option":"Non-admin user with \"Can use permission\"","correct":false}]},{"q":"<p>What will be the nature of access for the objects in the home folder for a Databricks user if workspace access control is enabled?</p>","a":[{"id":1678161,"option":"Public","correct":false},{"id":1678162,"option":"Private","correct":true},{"id":1678163,"option":"Protected","correct":false},{"id":1678164,"option":"Either 1 or 3","correct":false}]},{"q":"<p>You want to keep an \"all-purpose\" cluster configuration in your workspace even after a Databricks cluster has been terminated for more than 30 days. What should you do to achieve this?</p>","a":[{"id":1678153,"option":"You should ensure to pin the cluster","correct":true},{"id":1678154,"option":"You should ensure to clone the cluster","correct":false},{"id":1678155,"option":"You should ensure to disable termination of cluster","correct":false},{"id":1678156,"option":"You should ensure to create an Azure runbook to start the cluster","correct":false}]},{"q":"<p>You want to create a cluster in Databricks that provides maximum resource utilization and minimum query latencies. Which of the following cluster modes should you use to achieve this?</p>","a":[{"id":1678149,"option":"Standard cluster with Auto Scaling enabled","correct":false},{"id":1678150,"option":"Standard cluster with Auto Termination enabled ","correct":false},{"id":1678151,"option":"High Concurrency cluster ","correct":true},{"id":1678152,"option":"Single node cluster","correct":false}]},{"q":"<p>What is the maximum limit of concurrent jobs that can run in a Databricks workspace?</p>","a":[{"id":1678141,"option":"500","correct":false},{"id":1678142,"option":"1000","correct":true},{"id":1678143,"option":"1500","correct":false},{"id":1678144,"option":"2000","correct":false}]},{"q":"<p>An Apache spark-submit job failed with a <em>\"Failed to parse byte string: -1\"</em> error message. What might be a possible reason behind this error?</p>","a":[{"id":1678133,"option":"The value of the spark.driver.maxResultSize property is positive","correct":false},{"id":1678134,"option":"The value of the spark.driver.maxResultSize property is negative","correct":true},{"id":1678135,"option":"The value of the spark.driver.maxResultSize property is 0","correct":false},{"id":1678136,"option":"The value of the spark.driver.maxResultSize property is NULL","correct":false}]},{"q":"<p>You are asked to execute a pipeline run in Data Factory using a trigger that operates on a periodic interval while also retaining the state. Which of the following triggers can you use to achieve the mentioned scenario?</p>","a":[{"id":1678121,"option":"Schedule trigger","correct":false},{"id":1678122,"option":"Event-based trigger","correct":false},{"id":1678123,"option":"Equal interval trigger","correct":false},{"id":1678124,"option":"Tumbling window trigger","correct":true}]},{"q":"<p>Consider a pipeline in a Data Factory with two activities in the sequence Activity X -&gt; Activity Y, where Y runs only if X has a final status of either succeeded or failed.<br>\nWhat will be the dependency condition of Activity Y on Activity X in the given scenario?</p>","a":[{"id":1678117,"option":"Failed","correct":false},{"id":1678118,"option":"Skipped","correct":false},{"id":1678119,"option":"Completed","correct":true},{"id":1678120,"option":"Succeeded","correct":false}]},{"q":"<p>You want to create a pipeline in a data factory that copies data from one folder to another folder in Azure blob storage. Which of the following tools can be used to achieve this?</p>","a":[{"id":1678109,"option":"Azure Data Studio","correct":false},{"id":1678110,"option":"Azure CLI","correct":false},{"id":1678111,"option":"Azure CloudShell","correct":false},{"id":1678112,"option":"Azure PowerShell","correct":true}]},{"q":"<p>You are copying data between two data stores that are publicly accessible through the internet from any IP.<br>\nWhich of these integration runtimes can you use to run the given copy activity in Data Factory?</p>","a":[{"id":1678101,"option":"Self-hosted IR","correct":false},{"id":1678102,"option":"System-operated IR","correct":false},{"id":1678103,"option":"Azure IR","correct":true},{"id":1678104,"option":"Azure-SSIS IR","correct":false}]},{"q":"<p>Which of the following provides a bridge between an activity and a linked service in Azure Data Factory?</p>","a":[{"id":1678097,"option":"Dataset","correct":false},{"id":1678098,"option":"Integration runtime ","correct":true},{"id":1678099,"option":"Pipeline","correct":false},{"id":1678100,"option":"Trigger","correct":false}]},{"q":"<p>Which of the following parameters can be considered as a strongly-typed parameter for a pipeline in Data Factory?</p>\n\n<ol>\n\t<li>Activities</li>\n\t<li>Datasets</li>\n\t<li>Linked Services</li>\n</ol>","a":[{"id":1678093,"option":"1 and 2","correct":false},{"id":1678094,"option":"1 and 3","correct":false},{"id":1678095,"option":"2 and 3","correct":true},{"id":1678096,"option":"All of these","correct":false}]},{"q":"<p>You are creating a Spark cluster in the portal using a Databricks workspace. Which of the following default values should you select in the new cluster page when doing so?</p>","a":[{"id":1678089,"option":"Select Terminate after __ minutes of inactivity checkbox","correct":true},{"id":1678090,"option":"Set Keys to source","correct":false},{"id":1678091,"option":"Select Plot Options","correct":false},{"id":1678092,"option":"All of these","correct":false}]},{"q":"<p>You have added a textbox in a newly created SQL Analytics dashboard. Which of the following can you use to style the textbox?</p>","a":[{"id":1678077,"option":"Widget","correct":false},{"id":1678078,"option":"Content block","correct":false},{"id":1678079,"option":"Markdown","correct":true},{"id":1678080,"option":"Filters","correct":false}]},{"q":"<p>Which of these interfaces does Azure Databricks support in order to access the assets of Azure Databricks SQL Analytics?</p>","a":[{"id":1678073,"option":"UI","correct":false},{"id":1678074,"option":"API","correct":false},{"id":1678075,"option":"SQL endpoints","correct":false},{"id":1678076,"option":"Both 1 and 2","correct":true}]},{"q":"<p>Which of the following can be used to perform the tasks mentioned below?</p>\n\n<p><strong>Tasks</strong><br>\n1. Run SQL queries on your data lake.<br>\n2. Create multiple visualization types to explore query results.</p>","a":[{"id":1678069,"option":"Azure Databricks SQL Analytics","correct":true},{"id":1678070,"option":"Azure Databricks Workspace","correct":false},{"id":1678071,"option":"Both 1 and 2","correct":false},{"id":1678072,"option":"None of these","correct":false}]},{"q":"<p>You have moved an Azure Synapse workspace to another Azure Active Directory tenant through subscription migration. What will happen as a result of this?</p>","a":[{"id":1678065,"option":"You will not be able to perform any actions within the workspace","correct":false},{"id":1678066,"option":"You may lose access to the entire workspace","correct":false},{"id":1678067,"option":"You may lose access to the artifacts within the workspace","correct":true},{"id":1678068,"option":"Both 1 and 3","correct":false}]},{"q":"<p>You are analyzing some data in the blob storage using Spark when working with Synapse.<br>\nWhich of the following code snippet can you use to see the schema of the dataframe run a cell when doing so?</p>\n\n<p><strong>Code Snippet</strong></p>\n\n<p>1. </p>\n\n<pre class=\"prettyprint\"><code>data_df.print()</code></pre>\n\n<p>2.</p>\n\n<pre class=\"prettyprint\"><code>data_df.print(Schema)</code></pre>\n\n<p>3.</p>\n\n<pre class=\"prettyprint\"><code>data_df.printSchema()</code></pre>\n\n<p> </p>","a":[{"id":1678061,"option":"1","correct":false},{"id":1678062,"option":"2","correct":false},{"id":1678063,"option":"3","correct":true},{"id":1678064,"option":"Either 1 or 2","correct":false}]},{"q":"<p>You want to choose the best size for a pool depending on the aggregate resources needed for all databases in the pool.<br>\nWhich of the following should you determine in order to achieve this?</p>","a":[{"id":1678057,"option":"Maximum compute resources utilized by all databases in the pool.","correct":false},{"id":1678058,"option":"Maximum storage bytes utilized by all databases in the pool.","correct":false},{"id":1678059,"option":"Both 1 and 2","correct":true},{"id":1678060,"option":"None of these","correct":false}]},{"q":"<p>How can you exclude the processing server from the query servers when configuring the scale-out process?</p>","a":[{"id":1678053,"option":"Select Yes to Separate the processing server from the querying pool","correct":true},{"id":1678054,"option":"Reduce the number of replicas to zero","correct":false},{"id":1678055,"option":"Maximize the number replicas as possible","correct":false},{"id":1678056,"option":"Select No to Separate the processing server from the querying pool","correct":false}]},{"q":"<p>Which of the following are valid benefits of performing parallel synchronization of query replicas?</p>\n\n<p><strong>Options</strong><br>\na) Significant reduction in synchronization time<br>\nb) Data across replicas are consistent<br>\nc) Clients do not need to reconnect<br>\nd) Faster in-memory chahe updation incrementally</p>","a":[{"id":1678049,"option":"b, c and d","correct":false},{"id":1678050,"option":"a, b and d","correct":false},{"id":1678051,"option":"a, c and d","correct":true},{"id":1678052,"option":"a, b and c","correct":false}]},{"q":"<p>How can you specify that query replica synchronization occurs in parallel when using the synchronization mode of the Azure analysis service?</p>","a":[{"id":1678045,"option":"using Test-AzAnalysisServicesServer","correct":false},{"id":1678046,"option":"using AzureRmContext","correct":false},{"id":1678047,"option":"using Sync-AzAnalysisServicesInstance","correct":false},{"id":1678048,"option":"using the ReplicaSyncMode setting","correct":true}]},{"q":"<p>You are trying to delete a model database on the replicas in the query pool.<br>\nHow can you check if a model database exists on replicas in the query pool but not on the primary server when doing so?</p>","a":[{"id":1678041,"option":"ensure the \"Separate the processing server from querying pool\" setting is set to No","correct":false},{"id":1678042,"option":"Check using IsExist()","correct":false},{"id":1678043,"option":"ensure the \"Separate the processing server from querying pool\" setting is set to Yes","correct":true},{"id":1678044,"option":"perform DBScan","correct":false}]},{"q":"<p>What is the status of the primary server and all the other servers in the query pool when performing scale-out operations?</p>","a":[{"id":1678033,"option":"Always online","correct":false},{"id":1678034,"option":"temporarily offline","correct":true},{"id":1678035,"option":"Online for a while","correct":false},{"id":1678036,"option":"permanently offline","correct":false}]},{"q":"<p>You are processing (refresh) models on the primary server during the scale-out process.<br>\nWhich of the following is important after the processing operations are completed?</p>","a":[{"id":1678029,"option":"second set of files are updated","correct":false},{"id":1678030,"option":"Replicas in the query pool are then dehydrated","correct":false},{"id":1678031,"option":"encryption will be done","correct":false},{"id":1678032,"option":"synchronization must be performed","correct":true}]},{"q":"<p>Assume that you are configuring scale-out for the first time.<br>\nIn the given scenario, how would you synchronize model databases on your primary server with new replicas in a new query pool?</p>","a":[{"id":1678025,"option":"using Query Processing Units (QPU)","correct":false},{"id":1678026,"option":"Using Server-side query resolver","correct":false},{"id":1678027,"option":"Automatically synchronised","correct":true},{"id":1678028,"option":"Using DBSynchroniser","correct":false}]},{"q":"<p>Assume that query replicas are added and new connections are load balanced when scaling in in Azure analysis service.<br>\nIn the given scenario, what will happen if an existing client connection to a query pool resource is removed?<br>\n </p>","a":[{"id":1678021,"option":"existing client connections are terminated","correct":true},{"id":1678022,"option":"Existing client connections are not changed","correct":false},{"id":1678023,"option":"leave the resource for the new connection","correct":false},{"id":1678024,"option":"Changed to a new resource","correct":false}]},{"q":"<p>What happens when all new query replicas are up and running during scale out in the Azure analysis service?</p>","a":[{"id":1678017,"option":"New client connections are load balanced across resources in the query pool","correct":true},{"id":1678018,"option":"Clients can reconnect to a remaining query pool resource","correct":false},{"id":1678019,"option":"All of these","correct":false},{"id":1678020,"option":"None of these","correct":false}]},{"q":"<p>You are loading data into a dedicated SQL pool in <em>Azure</em> Synapse Analytics. Which of the following tools can be used to move data to Azure Data Lake Store Gen2 when doing so?</p>","a":[{"id":1678401,"option":"AZCopy utility ","correct":false},{"id":1678402,"option":"Azure ExpressRoute","correct":false},{"id":1678403,"option":"Azure Data Factory ","correct":false},{"id":1678404,"option":"All of these","correct":true}]},{"q":"<p>You want to maintain statistics when developing data warehouse solutions for <em>Azure</em> Synapse SQL. On which of the following columns should you maintain the statistics in order to gain the most benefit?</p>\n\n<ol>\n\t<li>On columns involved in joins</li>\n\t<li>On columns used in the WHERE clause</li>\n\t<li>On columns found in GROUP BY</li>\n</ol>","a":[{"id":1677969,"option":"Only 1","correct":false},{"id":1677970,"option":"Only 2","correct":false},{"id":1677971,"option":"Only 3","correct":false},{"id":1677972,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>Consider the given scenario:<br>\nThe member name parameter specified during the classification process for an <em>Azure</em> Synapse SQL pool is a database user instead of a database role. Which of the following can be inferred from the given scenario?</p>","a":[{"id":1677965,"option":"The weighting for the user is higher","correct":true},{"id":1677966,"option":"The weighting for the user is lower","correct":false},{"id":1677967,"option":"The weighting for the user cannot be determined","correct":false},{"id":1677968,"option":"None of these","correct":false}]},{"q":"<p>You are trying to connect to <em>Azure</em> Synapse workspace resources from a restricted network. When doing so, you want to find the Private DNS zone in the Azure portal. Which of these private DNS zone dedicated names should you use for the private endpoint of accessing the Azure Synapse Analytics Studio gateway?</p>","a":[{"id":1677961,"option":"privatelink.azuresynapse.net","correct":true},{"id":1677962,"option":"privatelink.sql.azuresynapse.net","correct":false},{"id":1677963,"option":"privatelink.dfs.core.windows.net","correct":false},{"id":1677964,"option":"privatelink.dev.azuresynapse.net","correct":false}]},{"q":"<p>You are in process of creating a private DNS zone when trying to connect to <em>Azure</em> Synapse workspace resources from a restricted network. You have added the virtual network link and you now need to add the DNS record set in the private DNS zone. In the given scenario, which of these dedicated name strings should you use for the private endpoint of SQL query execution in the built-in pool?</p>","a":[{"id":1677957,"option":"Web","correct":false},{"id":1677958,"option":"SQL","correct":false},{"id":1677959,"option":"YourWorkSpaceName","correct":false},{"id":1677960,"option":"YourWorkSpaceName-ondemand","correct":true}]},{"q":"<p>A query in a serverless SQL pool in <em>Azure</em> Synapse Analytics fails with the following error message:</p>\n\n<p><em>“Failed to execute query. Error: CREATE EXTERNAL TABLE/DATA SOURCE/DATABASE SCOPED CREDENTIAL/FILE FORMAT is not supported in master database”.</em><br>\nWhat might have caused such an error to occur in the given scenario?</p>","a":[{"id":1677953,"option":"The master database in serverless SQL pool does not support the creation of external tables","correct":false},{"id":1677954,"option":"The master database in serverless SQL pool does not support the creation of external file formats","correct":false},{"id":1677955,"option":"The master database in serverless SQL pool does not support the creation of external data sources","correct":false},{"id":1677956,"option":"All of these","correct":true}]},{"q":"<p>A query in <em>Azure</em> Synapse Analytics fails with the following error message:</p>\n\n<p><em>“File cannot be opened because it does not exist or it is used by another process”.</em><br>\nWhat might be the cause of this?</p>","a":[{"id":1677949,"option":"You do not have permissions to log into serverless SQL pool","correct":false},{"id":1677950,"option":"Your network prevents communication to Azure Synapse backend","correct":false},{"id":1677951,"option":"Your Azure Active Directory identity does not have rights to access the file","correct":true},{"id":1677952,"option":"The serverless SQL pool is not able to execute it at this moment due to resource constraints","correct":false}]},{"q":"<p>Which of the following scope levels is located at the top of the hierarchy in <em>Azure</em> Synapse role-based access control?</p>","a":[{"id":1677945,"option":"Workspace","correct":true},{"id":1677946,"option":"Spark pool","correct":false},{"id":1677947,"option":"Integration runtime","correct":false},{"id":1677948,"option":"Linked service","correct":false}]},{"q":"<p>Which of these capabilities does <em>Azure</em> Integration runtime provide for a private network in Data Factory:</p>\n\n<ol>\n\t<li>Data Flow</li>\n\t<li>Data movement</li>\n\t<li>Activity dispatch</li>\n</ol>","a":[{"id":1677933,"option":"Only 1 and 2","correct":false},{"id":1677934,"option":"Only 2 and 3","correct":false},{"id":1677935,"option":"Only 1 and 3","correct":false},{"id":1677936,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>You are working with big data in Data Lake Storage Gen2. When doing so, you have decided to use <em>Azure</em> Active Directory security groups instead of assigning individual users to directories and files. What is the benefit of doing so?</p>","a":[{"id":1677941,"option":"Adding or removing users from the group doesn’t require any updates to Data Lake Storage Gen2","correct":false},{"id":1677942,"option":"It ensures that you don't exceed the maximum number of access control entries per access control list","correct":false},{"id":1677943,"option":"It enables you to access to data in a storage account with hierarchical namespace enabled,","correct":true},{"id":1677944,"option":"Both 1 and 2","correct":false}]},{"q":"<p>Which of these are valid ways to use POSIX access controls for <em>Azure</em> Active Directory users offered by Azure Data Lake Storage Gen2?</p>","a":[{"id":1677937,"option":"Setting the access controls to existing files and directories","correct":false},{"id":1677938,"option":"Create default permissions that can be automatically applied to new files","correct":false},{"id":1677939,"option":"Both 1 and 2","correct":true},{"id":1677940,"option":"None of these","correct":false}]},{"q":"<p>When working with <em>Azure</em> SQL Database, you get the following error message:</p>\n\n<p><em>\"SQL transaction commits failed.\"</em><br>\n<br>\nDuring troubleshooting, you find out that the issue arises due to a transaction timeout. What could be the possible reason behind this?</p>","a":[{"id":1677929,"option":"A SQL database side issue","correct":false},{"id":1677930,"option":"The network latency between the integration runtime and the database is greater than the default threshold of 30 seconds","correct":true},{"id":1677931,"option":"The data value exceeds the logic type range","correct":false},{"id":1677932,"option":"The data value exceeds the allowed range in the sink table","correct":false}]},{"q":"<p>You want to use <em>Azure</em> SQL Analytics to its fullest extent for monitoring Azure SQL Database. Which of the following roles will not allow you to do so?</p>","a":[{"id":1677925,"option":"Owner","correct":false},{"id":1677926,"option":"Contributor","correct":false},{"id":1677927,"option":"SQL DB Contributor","correct":false},{"id":1677928,"option":"Reader","correct":true}]},{"q":"<p>You are observing an <em>Azure</em> SQL database for its usage in Azure portal. When doing so, you found a <em>1</em>-minute window with the following four data points:</p>\n\n<ul>\n\t<li> 0.1</li>\n\t<li> 0.1</li>\n\t<li> 0.9</li>\n\t<li> 0.1</li>\n</ul>\n\n<p> What could be the average of these data points in the given scenario?</p>","a":[{"id":1677921,"option":"0.3","correct":false},{"id":1677922,"option":"0.25","correct":true},{"id":1677923,"option":"0.433","correct":false},{"id":1677924,"option":"0","correct":false}]},{"q":"<p>Consider a scenario where a SQL Database UPDATE statement in <em>Azure</em> fails with a transient error. Which of the following statements is incorrect with respect to retrying the connection for the occurred transient error?</p>","a":[{"id":1677917,"option":"Establish a fresh connection before you retry the UPDATE","correct":false},{"id":1677918,"option":"Retry the UPDATE without establishing a fresh connection","correct":true},{"id":1677919,"option":"Retry logic must ensure that the entire transaction is rolled back","correct":false},{"id":1677920,"option":"Retry logic must ensure that the entire database transaction finished","correct":false}]},{"q":"<p>While establishing a connection to the \"ExampleServer\" in <em>Azure</em> SQL Database, you get the following error message:</p>\n\n<p><em>\"Error 5: Cannot connect to ExampleServer\"</em><br>\nWhat will you do to resolve this issue?</p>","a":[{"id":1677913,"option":"Make sure that port 1433 is open for outbound connections on all firewalls between the client and the internet","correct":true},{"id":1677914,"option":"Configure firewall settings on SQL Database through the Azure portal","correct":false},{"id":1677915,"option":"Contact your service administrator to provide you with a valid username and password","correct":false},{"id":1677916,"option":"None of these","correct":false}]},{"q":"<p>You want to configure the automatic high availability option for a recently migrated SQL database in <em>Azure</em> Virtual Machine. Which of the following deductions are invalid in the given context?</p>","a":[{"id":1677909,"option":"We can configure your high availability solution manually","correct":false},{"id":1677910,"option":"We can use templates in the Azure portal for automatic configuration","correct":false},{"id":1677911,"option":"SQL Server in VM can be entirely configured automatically","correct":true},{"id":1677912,"option":"Configure SQL Server Availability Groups","correct":false}]},{"q":"<p>Assume that there is a table located on a remote databases in <em>Azure</em> SQL Database with the same schema. You have a vertical partitioning scenario that requires access to this table. Given below are given steps to configure the elastic database queries for third vertical partitioning in the given scenario:</p>\n\n<ol>\n\t<li>CREATE DATABASE SCOPED CREDENTIAL</li>\n\t<li>CREATE/DROP EXTERNAL TABLE</li>\n\t<li>CREATE MASTER KEY</li>\n\t<li>CREATE/DROP EXTERNAL DATA SOURCE</li>\n</ol>\n\n<p>Which of the following options depicts the correct sequence in which these steps take place?</p>","a":[{"id":1677905,"option":"3->1->4->2","correct":true},{"id":1677906,"option":"2->4->3->1","correct":false},{"id":1677907,"option":"3->4->1->2","correct":false},{"id":1677908,"option":"3->4->2->1","correct":false}]},{"q":"<p>Your company wants to use an <em>Azure</em> SQL database warehouse instance. In this warehouse, millions of rows of data will be loaded every day and staging tables have been set up for the data load process. Which of the following will you use as the distribution option to optimize the data loading process in the given scenario?</p>","a":[{"id":1677901,"option":"Replicated","correct":false},{"id":1677902,"option":"Hash-distributed","correct":false},{"id":1677903,"option":"Round-robin","correct":true},{"id":1677904,"option":"External","correct":false}]},{"q":"<p>You have been asked to use an <em>Azure</em> SQL database for a new project. In the given scenario, which of the following is the third type of backup that will be taken by the SQL Database service?</p>","a":[{"id":1677897,"option":"A full daily backup","correct":false},{"id":1677898,"option":"A differential backup taken from the last 12 hours","correct":true},{"id":1677899,"option":"Differential backups taken since last full backup","correct":false},{"id":1677900,"option":"Log backups taken since last full backup","correct":false}]},{"q":"<p>You have been asked to use an <em>Azure</em> SQL database for a new project. In the given scenario, which of the following is the first type of backup that will be taken by the SQL database service?</p>","a":[{"id":1677893,"option":"A full daily backup","correct":false},{"id":1677894,"option":"A full weekly backup","correct":true},{"id":1677895,"option":"A differential daily backup","correct":false},{"id":1677896,"option":"A differential weeklybackup","correct":false}]},{"q":"<p>You are loading data using dedicated SQL pools in <em>Azure</em> Synapse Analytics. What can you do to minimize latency when doing so?</p>","a":[{"id":1677889,"option":"Colocate your storage layer and your dedicated SQL pool.","correct":true},{"id":1677890,"option":"Create loading users designated for running loads","correct":false},{"id":1677891,"option":"Either 1 or 2","correct":false},{"id":1677892,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You need to transform data in various formats and also import this data into a relational platform when working with <em>Azure</em> Synapse Analytics. Which of the following can be used to achieve this?</p>","a":[{"id":1677885,"option":"Azure Data Lake Storage","correct":false},{"id":1677886,"option":"Azure Synapse","correct":false},{"id":1677887,"option":"Azure PolyBase","correct":true},{"id":1677888,"option":"Azure Data Factory","correct":false}]},{"q":"<p>You are working on a large set of data that has been stored in <em>Azure</em> Data Lake Storage in the parquet format. You have been asked to query this data from Azure Synapse Analytics using T-SQL. Which of the following will you use to query the data in the given scenario?</p>","a":[{"id":1677881,"option":"PolyBase","correct":true},{"id":1677882,"option":"Azure Databricks","correct":false},{"id":1677883,"option":"Azure HDInsight","correct":false},{"id":1677884,"option":"Azure Data Factory","correct":false}]},{"q":"<p>You receive the following error when trying to copy data from SQL Server into <em>Azure</em> Synapse Analytics using staged copy and PolyBase:</p>\n\n<p><em>\" ErrorCode=FailedDbOperation,Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException, Message=Error happened when loading data into Azure Synapse Analytics., \"</em><br>\n<br>\nWhich of the following is the best possible way to resolve this error?</p>","a":[{"id":1677877,"option":"Reduce column width to less than 1 MB","correct":false},{"id":1677878,"option":"Use a bulk insert approach by disabling PolyBase","correct":false},{"id":1677879,"option":"Set the use type default option to false in the copy activity sink, under PolyBase settings","correct":true},{"id":1677880,"option":"Set the use type default option to true in the copy activity sink, under PolyBase settings","correct":false}]},{"q":"<p>You receive the following error when trying to use SQL query to pull data from <em>Azure</em> Synapse Analytics:</p>\n\n<p><em>\"...StorageException: The condition specified using HTTP conditional header(s) is not met...\"</em><br>\n<br>\nWhat might be the potential cause behind this type of issue?</p>","a":[{"id":1677873,"option":"The schema is too large ","correct":false},{"id":1677874,"option":"Azure Synapse Analytics PolyBase cannot convert an empty string to a GUID","correct":false},{"id":1677875,"option":"Azure Synapse Analytics PolyBase cannot insert an empty string (null value) into a decimal column","correct":false},{"id":1677876,"option":"Azure Synapse Analytics encountered an issue while querying the external table in Azure Storage","correct":true}]},{"q":"<p>You receive the following error when trying to copy data into <em>Azure</em> Synapse Analytics by using PolyBase:</p>\n\n<p><em>\"Message=110802;An internal DMS error occurred that caused this operation to fail. Details: Exception: Microsoft.SqlServer.DataWarehouse.DataMovement.Common.ExternalAccess.HdfsAccessException,\"</em></p>\n\n<p>When troubleshooting the issue you found out that the potential cause might be that the schema is too large. As a result, you are required to check the schema of the target Azure Synapse Analytics table by adding the size of all columns. In the given scenario, what should the size of the column “Nvarchar(n)” be added as?</p>","a":[{"id":1677869,"option":"n bytes","correct":false},{"id":1677870,"option":"n*2 bytes","correct":true},{"id":1677871,"option":"n*4 bytes","correct":false},{"id":1677872,"option":"n*8 bytes","correct":false}]},{"q":"<p>Suppose when trying to run the Copy activity in <em>Azure</em> Data Lake Storage Gen1 you receive the following error message:<br>\n<br>\n<em>The remote server returned an error: (403) Forbidden. Response details: {\"RemoteException\":{\"exception\":\"AccessControlException\"\"message\":\"CREATE failed with error 0x83090aa2 (Forbidden. ACL verification failed. Either the resource does not exist or the user is not authorized to perform the requested operation.)....</em><br>\nWhich of the following is the best possible way to correct this problem when working with Azure Synapse Analytics?</p>","a":[{"id":1677865,"option":"By rerunning the copy activity after several minutes","correct":false},{"id":1677866,"option":"By providing the Service Token Server which is owned by Azure Active Directory","correct":false},{"id":1677867,"option":"By granting appropriate permissions to all the folders and subfolders you need to copy","correct":true},{"id":1677868,"option":"By using the staged copy to skip the Transport Layer Security validation for Azure Data Lake Storage Gen1","correct":false}]},{"q":"<p>A copy activity in <em>Azure</em> Synapse SQL fails because the Service Token Server (STS) owned by Azure Active Directory is unavailable. Which of the following HTTP errors will be thrown in this scenario?</p>","a":[{"id":1677861,"option":"HTTP error 500","correct":false},{"id":1677862,"option":"HTTP error 502","correct":false},{"id":1677863,"option":"HTTP error 503","correct":true},{"id":1677864,"option":"HTTP error 507","correct":false}]},{"q":"<p>When working with <em>Azure</em> Synapse Analytics, you encountered an issue where the network connection to the Databricks service was interrupted. Which of the following error messages will be visible on your computer screen in the respective scenario?</p>","a":[{"id":1677857,"option":"Job execution failed.","correct":false},{"id":1677858,"option":"An error occurred while sending the request.","correct":true},{"id":1677859,"option":"The cluster is in Terminated state, not available to receive jobs. Please fix the cluster or retry later.","correct":false},{"id":1677860,"option":"User: SimpleUserContext {userId=..., name=user@company.com, orgId=...} is not authorized to access cluster.","correct":false}]},{"q":"<p>Suppose you are getting the following error message:</p>\n\n<p><em>“Invalid Python file URI... Please visit Databricks user guide for supported URI schemes”.</em><br>\nWhich of the following is the best possible way to resolve such type of issue?</p>","a":[{"id":1677853,"option":"Verify the linked service definition","correct":false},{"id":1677854,"option":"Verify that the Databricks cluster exists","correct":false},{"id":1677855,"option":"Create a new token and update the linked service","correct":false},{"id":1677856,"option":"Specify the absolute path for workspace-addressing schemes","correct":true}]},{"q":"<p>Which of these options is the correct order in which the ELT (Extract, Load, and Transform) process should be used to load data in <em>Azure</em> Synapse Analytics?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Transform the data.</li>\n\t<li>Prepare the data for loading.</li>\n\t<li>Extract the source data into text files.</li>\n\t<li>Land the data into Azure Blob storage.</li>\n\t<li>Insert the data into production tables.</li>\n\t<li>Load the data into staging tables with the COPY command.</li>\n</ol>","a":[{"id":1677849,"option":"1 -> 2 -> 3 -> 4 -> 5 -> 6","correct":false},{"id":1677850,"option":"3 -> 4 -> 2 -> 6 -> 1 -> 5","correct":true},{"id":1677851,"option":"4 -> 3 -> 6 -> 2 -> 5 -> 1","correct":false},{"id":1677852,"option":"5 -> 1 -> 6 -> 2 -> 4 -> 3","correct":false}]},{"q":"<p>You are using <em>Azure</em> Active Directory Authentication for authentication with Synapse SQL. In the given scenario, which of the following authentication methods is not supported for Azure Active Directory server principals?</p>","a":[{"id":1677845,"option":"Azure Active Directory Password","correct":false},{"id":1677846,"option":"Azure Active Directory Integrated","correct":false},{"id":1677847,"option":"Azure Active Directory Universal with MFA","correct":false},{"id":1677848,"option":"Using Application token authentication","correct":true}]},{"q":"<p>Which of the following permissions should an <em>Azure</em> Synapse Analytics workspace managed identity have in order to either encrypt or decrypt static data?</p>","a":[{"id":1677841,"option":"WrapKey ","correct":false},{"id":1677842,"option":"UnwrapKey ","correct":false},{"id":1677843,"option":"Get","correct":false},{"id":1677844,"option":"All of these","correct":true}]},{"q":"<p>You want to enhance query performance when working with columnstore indexes in dedicated <em>Azure</em> Synapse SQL pools. What should you do to achieve this?</p>","a":[{"id":1677837,"option":"Maximize the number of rows in each rowgroup ","correct":true},{"id":1677838,"option":"Compress all the rows designated for each rowgroup","correct":false},{"id":1677839,"option":"Bulk load the columnstore into a cluster","correct":false},{"id":1677840,"option":"None of these","correct":false}]},{"q":"<p>You are running a query that uses a serverless SQL pool in <em>Azure</em> Synapse Analytics for the first time. How much time will it take for the serverless SQL pool to gather the SQL resources required for running the queries?</p>","a":[{"id":1677833,"option":"1 second","correct":false},{"id":1677834,"option":"5 seconds","correct":false},{"id":1677835,"option":"10 seconds","correct":true},{"id":1677836,"option":"30 seconds","correct":false}]},{"q":"<p>Which of the following are valid considerations that you need to be followed when naming a dedicated SQL pool in <em>Azure</em> Synapse SQL?</p>","a":[{"id":1677829,"option":"The name should contain reserved words","correct":false},{"id":1677830,"option":"The name should be unique in the workspace","correct":true},{"id":1677831,"option":"The name should contain special characters","correct":false},{"id":1677832,"option":"The name must be minimum 20 characters","correct":false}]},{"q":"<p>Which of these <em>Azure</em> Synapse SQL clients will allow you to run SQL queries on your On-demand database?</p>","a":[{"id":1677825,"option":"Azure Data Studio ","correct":false},{"id":1677826,"option":"SQL Server Management Studio","correct":true},{"id":1677827,"option":"Azure Synapse Studio ","correct":false},{"id":1677828,"option":"SQL Server Data Tools","correct":false}]},{"q":"<p>Alice is working on Compute Nodes in Azure Synapse. What is the correct range of compute nodes that should be available to the <em>Azure</em> Synapse dedicated SQL pool?</p>","a":[{"id":1677821,"option":"1-20","correct":false},{"id":1677822,"option":"1-40","correct":false},{"id":1677823,"option":"1-60","correct":true},{"id":1677824,"option":"1-80","correct":false}]},{"q":"<p>Which of these can be used in<em> Azure</em> Synapse to move data between services and orchestrate activities?</p>","a":[{"id":1677817,"option":"Pipelines","correct":true},{"id":1677818,"option":"Synapse SQL","correct":false},{"id":1677819,"option":"Data Flows","correct":false},{"id":1677820,"option":"Activities","correct":false}]},{"q":"<p>You want to have more flexibility, control, and transparency while choosing a purchasing model in <em>Azure</em> SQL. Which of the following purchasing models would you choose in the given scenario?</p>","a":[{"id":1677813,"option":"DTU-based","correct":false},{"id":1677814,"option":"vCore Based","correct":true},{"id":1677815,"option":"Either DTU-based and vCore-Based","correct":false},{"id":1677816,"option":"Neither DTU-based and vCore-Based","correct":false}]},{"q":"<p>Which of the following cannot be considered as a vCore purchasing model for an <em>Azure</em> SQL project?</p>","a":[{"id":1677809,"option":"General Purpose","correct":false},{"id":1677810,"option":"Business Critical","correct":false},{"id":1677811,"option":"Premium ","correct":true},{"id":1677812,"option":"Hyperscale","correct":false}]},{"q":"<p>Alice is working on the DTU purchasing model in Azure SQL. Which of the following cannot be considered as a DTU purchasing model for an <em>Azure</em> SQL project?</p>","a":[{"id":1677805,"option":"Basic","correct":false},{"id":1677806,"option":"Standard","correct":false},{"id":1677807,"option":"Business Critical","correct":true},{"id":1677808,"option":"Premium","correct":false}]},{"q":"<p>Which of the following set of action groups is included in the default auditing policy of the <em>Azure</em> SQL auditing server:</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>BATCH_COMPLETED_GROUP</li>\n\t<li>SUCCESSFUL_DATABASE_AUTHENTICATION_GROUP</li>\n\t<li>FAILED_DATABASE_AUTHENTICATION_GROUP</li>\n</ol>","a":[{"id":1677801,"option":"1 and 2","correct":false},{"id":1677802,"option":"2 and 3","correct":false},{"id":1677803,"option":"1 and 3","correct":false},{"id":1677804,"option":"All of these","correct":true}]},{"q":"<p>How should auditing be enabled for <em>Azure</em> SQL databases when performing database-level auditing for secondary geo-replicated databases?</p>","a":[{"id":1677797,"option":"Auditing must be enabled on the primary database ","correct":true},{"id":1677798,"option":"Auditing must be enabled on the primary server","correct":false},{"id":1677799,"option":"Auditing must be enabled on the secondary server","correct":false},{"id":1677800,"option":"Auditing must be enabled on the secondary database","correct":false}]},{"q":"<p>What is the maximum concurrent worker value for a Gen5 compute model with 2 vCores per database in<em> Azure</em> SQL elastic pools?</p>","a":[{"id":1677793,"option":"10","correct":false},{"id":1677794,"option":"100","correct":false},{"id":1677795,"option":"200","correct":true},{"id":1677796,"option":"50","correct":false}]},{"q":"<p>You want to refer to the underlying data source when partitioning an <em>Azure</em> SQL database vertically. Which of these permissions do you need to have in order to do so?</p>","a":[{"id":1677789,"option":"ALTER ANY EXTERNAL DATA SOURCE","correct":true},{"id":1677790,"option":"ALTER DATABASE","correct":false},{"id":1677791,"option":"ALTER EXTERNAL DATABASE ","correct":false},{"id":1677792,"option":"ALTER DATA SOURCE","correct":false}]},{"q":"<p>What should you do to immediately invalidate a client agent in the <em>Azure</em> SQL Data Sync service?</p>","a":[{"id":1677785,"option":"Regenerate its key in the portal but do not submit it in the Agent UI","correct":true},{"id":1677786,"option":"Delete the agent","correct":false},{"id":1677787,"option":"Record its key with SQL Data Sync service","correct":false},{"id":1677788,"option":"An agent can only be invalidated when it is online","correct":false}]},{"q":"<p>Bob is working on Data Sync Agent in <em>Azure</em> SQL. He wants to create a new key. What should be ensured when creating a new key for an existing client agent in Azure SQL Data Sync service?</p>","a":[{"id":1677781,"option":"Make sure that the same key is recorded with the client agent","correct":false},{"id":1677782,"option":"Make sure that the same key is recorded with the SQL Data Sync service","correct":false},{"id":1677783,"option":"Make sure that the same key is recorded with the client agent and with the SQL Data Sync service","correct":true},{"id":1677784,"option":"Don't record the same key with the client agent and with the SQL Data Sync service","correct":false}]},{"q":"<p>You are using Data Sync Agent to sync data with <em>Azure</em> SQL Server databases. How many instances of the local agent UI can be run in the given scenario?</p>","a":[{"id":1677777,"option":"0","correct":false},{"id":1677778,"option":"1","correct":true},{"id":1677779,"option":"2","correct":false},{"id":1677780,"option":"3","correct":false}]},{"q":"<p>You observe that an <em>Azure</em> Databricks job started running before the required libraries were installed causing import errors. On further analysis, you find that the cause for this is the delayed installation of the required libraries. Which of these scenarios given alongside may have resulted in such a delay?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>You have started a new cluster that uses a shared library</li>\n\t<li>You have started a new cluster with libraries in terminated state</li>\n\t<li>You have started an existing cluster that uses a shared library</li>\n\t<li>You have started an existing cluster with libraries in terminated state</li>\n</ol>","a":[{"id":1677773,"option":"Both 1 and 2","correct":false},{"id":1677774,"option":"Both 3 and 4","correct":false},{"id":1677775,"option":"Both 1 and 4","correct":true},{"id":1677776,"option":"Both 2 and 3","correct":false}]},{"q":"<p>An Apache Spark job in an <em>Azure</em> Databricks notebook failed with the following error</p>\n\n<p>\"<em>Driver is temporarily unavailable.</em>\"<br>\nWhich of the following is the easiest way to resolve this issue in the absence of specific details?</p>","a":[{"id":1677769,"option":"Increase the driver memory","correct":true},{"id":1677770,"option":"Avoid memory intensive operations","correct":false},{"id":1677771,"option":"Avoid running batch jobs on a shared interactive cluster","correct":false},{"id":1677772,"option":"Distribute the workloads into different clusters","correct":false}]},{"q":"<p>Which of the following type of users can get access to the special backup folder created in the workspace when a user is removed from <em>Azure</em> Databricks?</p>","a":[{"id":1677765,"option":"Databricks account owner","correct":false},{"id":1677766,"option":"Databricks account admins","correct":false},{"id":1677767,"option":"Admin users","correct":true},{"id":1677768,"option":"Non-admin users","correct":false}]},{"q":"<p>You accidentally cancel a running streaming cell in a notebook attached to an <em>Azure</em> Databricks Runtime 5.0 cluster. In the given scenario, what can you do to run subsequent commands in the notebook without restarting the cluster?</p>","a":[{"id":1677761,"option":"Go to notebook’s Clear menu and select Clear Results","correct":false},{"id":1677762,"option":"Go to notebook’s Clear menu and select Clear State","correct":true},{"id":1677763,"option":"Go to notebook’s Clear menu and select Clear State & Results","correct":false},{"id":1677764,"option":"Go to notebook’s Clear menu and select Clear State & Run All","correct":false}]},{"q":"<p>A user X in an <em>Azure</em> Databricks governance model 'M' owns a table T and also a view V1 on T.  Another user Y in the same model 'M' owns view V2 on the same table T. Given that X has granted SELECT privileges on table T for user A and Y has granted SELECT privileges on V2 for user A. Analyse the given scenario and choose the most valid inference from the options.</p>","a":[{"id":1677757,"option":"User A can select view V1 only","correct":false},{"id":1677758,"option":"User A can select view V2 only","correct":true},{"id":1677759,"option":"User A can select both the views","correct":false},{"id":1677760,"option":"User A cannot select either of the views","correct":false}]},{"q":"<p>You get the following error when trying to SELECT a view from a particular table in an <em>Azure</em> Databricks data governance model.</p>\n\n<p>\"<em>User does not have privilege SELECT on table</em>\"<br>\nWhat might be a possible reason for this error to occur in the given scenario:<br>\n<strong>Note</strong>: Assume that you have SELECT privileges on the view.</p>","a":[{"id":1677753,"option":"The table has no registered owner ","correct":false},{"id":1677754,"option":"The grantor of the SELECT privilege on view of the table is not the owner of the table","correct":false},{"id":1677755,"option":"Either 1 or 2","correct":true},{"id":1677756,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You are attempting to create a table using an <em>Azure</em> Databricks cluster that has table access controls enabled. However, when doing so, the following error occurs.</p>\n\n<p>\"Error in SQL statement: SecurityException: User does not have permission SELECT on any file.\"<br>\nWhat can be a valid cause for this issue?</p>","a":[{"id":1677749,"option":"You are not an administrator ","correct":false},{"id":1677750,"option":"You do not have sufficient privileges to create a table","correct":false},{"id":1677751,"option":"Both 1 and 2","correct":true},{"id":1677752,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You are editing the endpoint property \"multi-cluster load balancing\" when configuring a SQL endpoint in <em>Azure</em> Databricks. In the given scenario, what will the maximum number of clusters be if the property is set to \"On\"?</p>","a":[{"id":1677745,"option":"0","correct":false},{"id":1677746,"option":"1","correct":true},{"id":1677747,"option":"2","correct":false},{"id":1677748,"option":"None of these","correct":false}]},{"q":"<p>You are using <em>Azure</em> Active Directory credential passthrough to access Azure Data Lake Storage resources when troubleshooting Databricks data sources. However, after one hour, you find that the job failed with an HTTP401 error message. What can you do to eliminate such errors?</p>","a":[{"id":1677741,"option":"Rewrite the queries","correct":true},{"id":1677742,"option":"Increasing the lifetime of Azure AD credential passthrough token","correct":false},{"id":1677743,"option":"Submit notebooks or jars as jobs","correct":false},{"id":1677744,"option":"Remove idle execution contexts","correct":false}]},{"q":"<p>Assume that an <em>Azure</em> Databricks job execution stops with the following error message.</p>\n\n<p><em>\"Can't attach this notebook because the cluster has reached the attached notebook limit. Detach a notebook and retry.\"</em></p>\n\n<p>What can you do to avoid such issues from occurring?</p>\n\n<ol>\n\t<li>Use fewer notebooks</li>\n\t<li>Configure context auto-eviction</li>\n\t<li>Use a job cluster instead of an interactive cluster</li>\n</ol>","a":[{"id":1677737,"option":"Both 1 and 2","correct":false},{"id":1677738,"option":"Both 2 and 3","correct":false},{"id":1677739,"option":"Both 1 and 3","correct":false},{"id":1677740,"option":"All of these","correct":true}]},{"q":"<p>Which of the following might get terminated when you try to upgrade or downgrade a <em>Azure</em> Databricks workspace?</p>","a":[{"id":1677733,"option":"Users","correct":false},{"id":1677734,"option":"Notebooks","correct":false},{"id":1677735,"option":"Active clusters","correct":true},{"id":1677736,"option":"Cluster configurations","correct":false}]},{"q":"<p>Which of the access controls given alongside have the same number of permission levels assigned in <em>Azure</em> Databricks SQL Analytics:</p>\n\n<ol>\n\t<li>Alert</li>\n\t<li>Dashboard</li>\n\t<li>SQL endpoint</li>\n\t<li>Query</li>\n</ol>","a":[{"id":1677729,"option":"Both 1 and 2","correct":false},{"id":1677730,"option":"Both 1 and 3","correct":false},{"id":1677731,"option":"Both 3 and 4","correct":false},{"id":1677732,"option":"All of these","correct":true}]},{"q":"<p>You want to include a new notebook within another notebook when working with <em>Azure</em> DataBricks. Which of the following magic commands can you use to do so?</p>","a":[{"id":1677725,"option":"%fs","correct":false},{"id":1677726,"option":"%md","correct":false},{"id":1677727,"option":"%sh","correct":false},{"id":1677728,"option":"%run","correct":true}]},{"q":"<p>Alice is working on Jobs in <em>Azure</em> Databricks. What is the maximum number of jobs that a workspace can create in an hour in Databricks?</p>","a":[{"id":1677721,"option":"<1000","correct":false},{"id":1677722,"option":"1000","correct":false},{"id":1677723,"option":"3000","correct":false},{"id":1677724,"option":"5000","correct":true}]},{"q":"<p>Which of these access controls in <em>Azure</em> Databricks have the highest number of permissions levels assigned to it?</p>","a":[{"id":1677717,"option":"Clusters","correct":false},{"id":1677718,"option":"Pools","correct":false},{"id":1677719,"option":"Jobs","correct":true},{"id":1677720,"option":"Data tables","correct":false}]},{"q":"<p>Which of these utilities is supported by <em>Azure</em> Databricks Connect?</p>","a":[{"id":1677713,"option":"Libraries","correct":false},{"id":1677714,"option":"Secrets","correct":true},{"id":1677715,"option":"Widgets","correct":false},{"id":1677716,"option":"Notebook workflows","correct":false}]},{"q":"<p>Read the following statements about <em>Azure</em> Databricks clusters and choose the statements that are correct:</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>A user can create a job cluster</li>\n\t<li>A user cannot create an all-purpose cluster</li>\n\t<li>A user cannot restart a job cluster</li>\n\t<li>A user can manually terminate and restart an all-purpose cluster</li>\n</ol>","a":[{"id":1677709,"option":"Both 1 and 2","correct":false},{"id":1677710,"option":"Both 3 and 4","correct":true},{"id":1677711,"option":"Both 1 and 3","correct":false},{"id":1677712,"option":"All 1, 2, 3 and 4","correct":false}]},{"q":"<p>Alice is working on Jobs in <em>Azure</em> Databricks. How many owners she can be assigned to a single job in a Databricks workspace?</p>","a":[{"id":1677705,"option":"Only One","correct":true},{"id":1677706,"option":"Two","correct":false},{"id":1677707,"option":"Four","correct":false},{"id":1677708,"option":"Five","correct":false}]},{"q":"<p>Which of the following actions is prohibited for a special folder in the <em>Azure</em> Databricks workspace?</p>","a":[{"id":1677701,"option":"Create","correct":false},{"id":1677702,"option":"Clone","correct":false},{"id":1677703,"option":"Rename","correct":true},{"id":1677704,"option":"Import","correct":false}]},{"q":"<p>Which of the following interfaces is not supported by <em>Azure</em> Databricks while accessing user workspace assets?</p>","a":[{"id":1677697,"option":"UI","correct":false},{"id":1677698,"option":"CLI","correct":false},{"id":1677699,"option":"REST API","correct":false},{"id":1677700,"option":"Web API","correct":true}]},{"q":"<p>Which of the following can be considered as one of the limitations while working with query filters in SQL analytics on <em>Azure</em> Databricks:</p>\n\n<ol>\n\t<li>Not suitable for large data sets</li>\n\t<li>Not suitable for small data sets</li>\n\t<li>Not suitable for query results with large number of distinct field values</li>\n</ol>","a":[{"id":1677693,"option":"1 and 2","correct":false},{"id":1677694,"option":"2 and 3","correct":false},{"id":1677695,"option":"1 and 3","correct":true},{"id":1677696,"option":"1, 2 and 3","correct":false}]},{"q":"<p>You are creating a query in <em>Azure</em> Databricks for the first time for which the list of available connections is displayed in alphabetical order. In the given context, which of the following connections would be selected for creating the query for the second time?</p>","a":[{"id":1677689,"option":"The first used connection","correct":false},{"id":1677690,"option":"The last used connection ","correct":true},{"id":1677691,"option":"All the previous used connections","correct":false},{"id":1677692,"option":"All the previous used connections except first and last","correct":false}]},{"q":"<p>Which of the following shortcut keys can be used to write faster queries in <em>Azure</em> Databricks SQL Analytics?</p>","a":[{"id":1677685,"option":"Alt + D ","correct":false},{"id":1677686,"option":"Ctrl + S","correct":false},{"id":1677687,"option":"Ctrl + Enter","correct":false},{"id":1677688,"option":"Ctrl + Space","correct":true}]},{"q":"<p>Which of these paths includes all the useful tools and concepts required for building, testing, and training machine learning models with <em>Azure</em> Databricks?</p>","a":[{"id":1677681,"option":"Platform Administrator","correct":false},{"id":1677682,"option":"Data/SQL Analyst","correct":false},{"id":1677683,"option":"Data Engineer","correct":false},{"id":1677684,"option":"Data Scientist","correct":true}]},{"q":"<p>Which of the following MLflow primary components on <em>Azure</em> Databricks allows a user to package ML code in such a manner that it can be shared with other data scientists?</p>","a":[{"id":1677677,"option":"Projects","correct":true},{"id":1677678,"option":"Tracking","correct":false},{"id":1677679,"option":"Models","correct":false},{"id":1677680,"option":"Model Serving","correct":false}]},{"q":"<p>Alice wants to configure permission in order to access workspace objects in <em>Azure</em> Databricks. Which of the following architectural components can she be accessed for the desired purpose?</p>","a":[{"id":1677673,"option":"Databricks Access Control Lists","correct":true},{"id":1677674,"option":"Databricks Administration Console","correct":false},{"id":1677675,"option":"Databricks Notebook","correct":false},{"id":1677676,"option":"Databricks SQL Analytics","correct":false}]},{"q":"<p>Which of the following tasks can be performed only by the account owner in the Databricks workspace in <em>Azure</em> Databricks:</p>\n\n<ol>\n\t<li>Job scheduling</li>\n\t<li>Subscription upgrades</li>\n\t<li>Audit logging configuration</li>\n</ol>","a":[{"id":1677669,"option":"1 and 3","correct":false},{"id":1677670,"option":"1 and 2","correct":false},{"id":1677671,"option":"2 and 3","correct":true},{"id":1677672,"option":"1, 2 and 3","correct":false}]},{"q":"<p>Which of the following Databricks workspace-level configurations can be administered in the administration console in <em>Azure</em> Databricks:</p>\n\n<ol>\n\t<li>Storage purging</li>\n\t<li>Users and groups addition</li>\n\t<li>Single sign-on management</li>\n\t<li>Access control management</li>\n</ol>","a":[{"id":1677665,"option":"1, 2 and 3","correct":false},{"id":1677666,"option":"1, 3 and 4","correct":false},{"id":1677667,"option":"1, 2 and 4","correct":false},{"id":1677668,"option":"1, 2, 3 and 4","correct":true}]},{"q":"<p>Which of the following should be created for running computations on a Databricks Runtime cluster in <em>Azure</em> Databricks:</p>","a":[{"id":1677661,"option":"Cluster","correct":false},{"id":1677662,"option":"Folder","correct":false},{"id":1677663,"option":"Notebook","correct":true},{"id":1677664,"option":"Table","correct":false}]},{"q":"<p>Which of the following tools would you use for monitoring and auditing when working with <em>Azure</em><strong> </strong>Data Lake Analytics:</p>","a":[{"id":1677657,"option":"Azure COSMOS DB","correct":false},{"id":1677658,"option":"On the Advance tab, set the Hierarchical Namespace to Disable","correct":false},{"id":1677659,"option":"Azure Kubernetes Service (AKS)","correct":false},{"id":1677660,"option":"No tools needed. It comes with Built-in monitoring and auditing","correct":true}]},{"q":"<p>Which of the following roles can you assign to yourself when you prepare an existing storage account for use with <em>Azure</em> Synapse Analytics?</p>","a":[{"id":1677653,"option":"Owner role only ","correct":false},{"id":1677654,"option":"Storage Blob Data Owner role only ","correct":false},{"id":1677655,"option":"Both Owner role and Storage Blob Data Owner role","correct":true},{"id":1677656,"option":"None of these","correct":false}]},{"q":"<p>You submitted SQL requests via the <em>Azure</em> Synapse Studio in a workspace enabled dedicated SQL pool. Where can these requests be viewed in your Synapse workspace?</p>","a":[{"id":1677649,"option":"In the Activities tab","correct":false},{"id":1677650,"option":"In the SQL DW","correct":false},{"id":1677651,"option":"In the Monitor hub","correct":true},{"id":1677652,"option":"In the SQL pool","correct":false}]},{"q":"<p>Which of the following tells <em>Azure</em> Synapse how many Spark resources to use when creating a serverless Apache Spark pool?</p>","a":[{"id":1677645,"option":"The Spark pool","correct":true},{"id":1677646,"option":"The SQL pool","correct":false},{"id":1677647,"option":"The logical SQL server","correct":false},{"id":1677648,"option":"The dedicated SQL pool","correct":false}]},{"q":"<p>Which of the following resource models does <em>Azure</em> Synapse SQL provide?</p>\n\n<p><strong>Options</strong><br>\n1. Serverless resource model<br>\n2. Dedicated resource model</p>","a":[{"id":1677641,"option":"Only 1","correct":false},{"id":1677642,"option":"Only 2","correct":false},{"id":1677643,"option":"Both 1 and 2","correct":true},{"id":1677644,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>A ticket has been raised stating the the DB tables size is increasing and on-premises storage is running low. You have been asked to design a cost-effective and secure solution to store less queried DB tables on <em>Azure</em>. Which of the following can you use to achieve this objective?</p>","a":[{"id":1677637,"option":"Azure SQL database","correct":false},{"id":1677638,"option":"Azure SQL stretch database","correct":true},{"id":1677639,"option":"Azure analysis service","correct":false},{"id":1677640,"option":"Azure sql data warehouse","correct":false}]},{"q":"<p>You have a requirement wherein you have to use a user-defined extractor that is capable of reading CSV-like data into SQL.MAP&lt;string,string&gt; columns and SQL.ARRAY&lt;int&gt; columns. Which of the following User-Defined Extractor can you use to define an extractor in this scenario?</p>","a":[{"id":1677633,"option":"SampleExtractor","correct":false},{"id":1677634,"option":"DriverExtractor","correct":true},{"id":1677635,"option":"FlexExtractor","correct":false},{"id":1677636,"option":"Both DriverExtractor and FlexExtractor","correct":false}]},{"q":"<p>You are using the user-defined outputters to write data in a custom-defined format in <em>Azure</em> Data Lake. During the process, you flushed the data buffer to the file after each row iteration. Which of the following must you do along with flushing the data?<br>\n<br>\n1. Use StreamWriter object with the Disposable attribute enabled<br>\n2. Use StreamWriter object with the using keyword</p>","a":[{"id":1677629,"option":"Only1 ","correct":false},{"id":1677630,"option":"Only 2","correct":false},{"id":1677631,"option":"Both 1 and 2","correct":true},{"id":1677632,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>You have set the SqlUserDefinedOutputter attribute to true when using user-defined outputters to write data in a custom-defined format in <em>Azure</em> Data Lake. What will happen as a result of this?</p>","a":[{"id":1677625,"option":"The Outputter will only deal with atomic output files","correct":true},{"id":1677626,"option":"The Outputter will only deal with split files","correct":false},{"id":1677627,"option":"The Outputter will only deal with distributed files","correct":false},{"id":1677628,"option":"The Outputter will deal with all atomic / split / distributed files","correct":false}]},{"q":"<p>You are using the EXTRACT expression to extract data from a compressed file of size more than 4GB. What will happen once you apply the EXTRACT expression to the file during the extraction in <em>Azure</em> Data Lake?</p>","a":[{"id":1677621,"option":"The expression will compile successfully without any errors","correct":false},{"id":1677622,"option":"The EXTRACT will apply the decompression utility on the compressed file","correct":false},{"id":1677623,"option":"An error will be raised during the compilation of the job","correct":true},{"id":1677624,"option":"A limited amount of data will be fetched from the file","correct":false}]},{"q":"<p>You are using the catalog views to get the information used by U-SQL. Which of the following will the catalog views contain during the processing of the obtained information?<br>\n<strong>Options</strong><br>\n 1. Objects created as part of the same script<br>\n 2. Objects that the user submitting the query has the right to see</p>","a":[{"id":1677617,"option":"Only 1","correct":false},{"id":1677618,"option":"Only 2","correct":true},{"id":1677619,"option":"Both 1 and 2","correct":false},{"id":1677620,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>How can you achieve secure access to data sources on-premises when using the tabular mode in <em>Azure</em> analysis services?</p>","a":[{"id":1677613,"option":"by installing and configuring an On-premises data gateway","correct":true},{"id":1677614,"option":"Using Firewall protection","correct":false},{"id":1677615,"option":"by application level authorisation","correct":false},{"id":1677616,"option":"through encrypted protocol at query time","correct":false}]},{"q":"<p>You observe that when trying to store data files from the storage only metadata is stored. How will you access the actual data if you are using the DirectQuery mode?</p>","a":[{"id":1677609,"option":"using Query Processing Units (QPU)","correct":false},{"id":1677610,"option":"through encrypted protocol at query time","correct":true},{"id":1677611,"option":"using primary server","correct":false},{"id":1677612,"option":"through In-mem storage","correct":false}]},{"q":"<p>You have accessed the <em>Azure</em> Analysis Services database and you want to encrypt the data files within a blob. Which encryption technique tool can be used to do so in this scenario?</p>","a":[{"id":1677605,"option":"Azure SQL Server Side Encryption (SSSE)","correct":false},{"id":1677606,"option":"Azure RSA Encryption (RSAE)","correct":false},{"id":1677607,"option":"Azure Blob Server Side Encryption (SSE)","correct":true},{"id":1677608,"option":"Azure SQL Data Encryption (SDE)","correct":false}]},{"q":"<p>Assume that you provided user authentication in <em>Azure</em> analysis services. In the given scenario, the user identities must be members of which of these for logging in activities?</p>","a":[{"id":1677601,"option":"Azure DDoS Protection","correct":false},{"id":1677602,"option":"Azure Blob storage","correct":false},{"id":1677603,"option":"DAX limitation","correct":false},{"id":1677604,"option":"Azure Active Directory","correct":true}]},{"q":"<p>What can be used to persist storage and metadata for Analysis Services databases when working on <em>Azure</em> analysis service?</p>","a":[{"id":1677597,"option":"Azure Blob storage","correct":true},{"id":1677598,"option":"Azure Active Directory","correct":false},{"id":1677599,"option":"Azure DDoS Protection","correct":false},{"id":1677600,"option":"DAX limitation","correct":false}]},{"q":"<p>You are using the DirectQuery mode tabular model in <em>Azure</em> analysis services to work on extremely large data sets. From which of the following sources can the data be taken in the given scenario?<br>\n<strong>Options</strong><br>\na) Single SQL Server<br>\nb) SQL Server Data Warehouse<br>\nc) Azure SQL Database<br>\nd) Azure Synapse Analytics<br>\ne) Excel Sheet data file<br>\nf) Oracle and Teradata data sources</p>","a":[{"id":1677593,"option":"(a), (b) and (c)","correct":false},{"id":1677594,"option":"(a), (b), (c), (d) and (f)","correct":true},{"id":1677595,"option":"(b), (c), (d) and (e)","correct":false},{"id":1677596,"option":"(a), (c), (d) and (e)","correct":false}]},{"q":"<p>What should you do to update cached data from data sources when working with in-memory models in <em>Azure</em>?</p>","a":[{"id":1677589,"option":"use DirectQuery mode","correct":false},{"id":1677590,"option":"update the cached data using queries","correct":false},{"id":1677591,"option":"refresh the in-memory model","correct":true},{"id":1677592,"option":"use multidimensional models","correct":false}]},{"q":"<p>Which of these can be achieved by performing partitioning of a data model in <em>Azure analysis service</em>?<br>\n<strong>Options</strong><br>\n    a) Enabling of incremental loads<br>\n    b) Increase in parallelization<br>\n    c) Reduction in memory consumption<br>\n    d) Maximum use of memory</p>","a":[{"id":1677585,"option":"Both (b) and (c)","correct":false},{"id":1677586,"option":"(a), (c) and (d)","correct":false},{"id":1677587,"option":"(c) and (d)","correct":false},{"id":1677588,"option":"(a), (b) and (c)","correct":true}]},{"q":"<p>You want to use a data modeling construct in <em>Azure</em> analysis service to perform some query operations over a large amount of data from multiple data sources. Which data model would you use for the fastest query response in this scenario?</p>","a":[{"id":1677581,"option":"Tabular model in in-memory mode","correct":true},{"id":1677582,"option":"Tabular model in DirectQuery mode","correct":false},{"id":1677583,"option":"Multidimensional models","correct":false},{"id":1677584,"option":"PowerPivot in SharePoint","correct":false}]},{"q":"<p>You are using a data modeling construct which is articulated in Tabular Model Scripting Language (TMSL) and Tabular Object Model (TOM) code. Which features of this model can you use with <em>Azure Analysis Services</em>?</p>\n\n<p><strong>Option</strong><br>\na) Partitions<br>\nb) Perspectives<br>\nc) Row-level security<br>\nd) Bi-directional relationships<br>\ne) Translations</p>","a":[{"id":1677577,"option":"(a), (c) and (d)","correct":false},{"id":1677578,"option":"(a), (b), (c), (d) and (e)","correct":true},{"id":1677579,"option":"(a), (c) and (d)","correct":false},{"id":1677580,"option":"(b), (c) and (e)","correct":false}]},{"q":"<p>Which of the following statements is true about the DTU-based purchasing model in Azure SQL Database?</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li> Azure SQL Managed Instance supports a DTU-based purchasing model.</li>\n\t<li> All service tiers in the DTU-based purchase model provide flexibility of changing compute sizes with minimal downtime</li>\n</ol>","a":[{"id":1677573,"option":"Only 1","correct":false},{"id":1677574,"option":"Only 2","correct":false},{"id":1677575,"option":"Both 1 and 2","correct":true},{"id":1677576,"option":"None of these","correct":false}]},{"q":"<p>Your company wants to set up an Azure SQL database that will store sensitive Personally Identifiable Information (PII) data. The objective is to be able to track and store all the queries that are executed against the PII data. Which of the given actions will you take to achieve the desired objective?</p>\n\n<p><strong>Actions</strong></p>\n\n<ol>\n\t<li> Implementation of Transparent Data Encryption</li>\n\t<li> Create a SELECT trigger on the table in the database that will write data to a new table in the database and then a stored procedure would be executed to lookup column classifications and perform joins</li>\n</ol>","a":[{"id":1677569,"option":"Only 1","correct":false},{"id":1677570,"option":"Only 2","correct":false},{"id":1677571,"option":"Both 1 and 2","correct":false},{"id":1677572,"option":"None of these","correct":true}]},{"q":"<p>Your company has decided to use the Azure SQL Database along with elastic pools for storing customer information in the tables. Each record in the table will have a value for the column CustomerlD. Which of the following will fulfill your requirement of partitioning the data based on the values in the column CustomerlD in the given scenario?</p>","a":[{"id":1677565,"option":"Separate the data into regions by using vertical partitioning","correct":false},{"id":1677566,"option":"Separate the data into shards and use horizontal partitioning","correct":true},{"id":1677567,"option":"Separate the data into regions by using horizontal partitioning","correct":false},{"id":1677568,"option":"None of these","correct":false}]},{"q":"<p>You want to enable easy migration of on-premises SQL databases to Azure SQL databases using a fully-managed database instance. Which of the following deployment models should you use to do so?</p>","a":[{"id":1677561,"option":"Single database","correct":false},{"id":1677562,"option":"Elastic Pool","correct":false},{"id":1677563,"option":"Managed Instance","correct":true},{"id":1677564,"option":"Database pool","correct":false}]},{"q":"<p>You are encrypting an entire Azure SQL database using the Transparent Data Encryption(TDE). Which of the following encryption algorithm will be used by the TDE to encrypt the database in the given scenario?</p>","a":[{"id":1677557,"option":"Advanced Encryption Standard","correct":true},{"id":1677558,"option":"Data Encryption Standard","correct":false},{"id":1677559,"option":"Triple data encryption algorithm","correct":false},{"id":1677560,"option":"RSA Asymmetric Encryption Algorithm","correct":false}]},{"q":"<p>You are using the automated backups with geo-redundant storage for recovery in case of data center outage when working with Azure SQL Database. Which of the following will you use to recover the database while having such backups?</p>","a":[{"id":1677553,"option":"Geo-restore","correct":true},{"id":1677554,"option":"Fail-over","correct":false},{"id":1677555,"option":"Geo-replication","correct":false},{"id":1677556,"option":"Database recovery objective","correct":false}]},{"q":"<p>You want to provide access to some resources for a limited period of time in your Azure Data lake storage Gen 2 account. Which of the following will you use to grant this permission?</p>","a":[{"id":1677549,"option":"Access keys for storage account","correct":false},{"id":1677550,"option":"Azure AD users","correct":false},{"id":1677551,"option":"A shared access signature","correct":true},{"id":1677552,"option":"Role based access control","correct":false}]},{"q":"<p>Assume that you have created a Data Factory using Azure Data Factory UI. In the given context, which of these roles should you belong to in order to create and manage child resources in the Azure portal?</p>","a":[{"id":1677545,"option":"Data Factory Contributor role at the resource group level only","correct":true},{"id":1677546,"option":"Data Factory Contributor role at the resource group level or above","correct":false},{"id":1677547,"option":"The contributor role at the resource level or above","correct":false},{"id":1677548,"option":"The contributor role at the resource level only","correct":false}]},{"q":"<p>Assume that you have created a Data Factory using Azure Data Factory UI. Now, you want to create a Data Factory instance using an Azure user account. In the given scenario, the account must be a member of which of these roles in order to achieve this?</p>","a":[{"id":1677541,"option":"contributor role","correct":false},{"id":1677542,"option":"owner role","correct":false},{"id":1677543,"option":"Both 1 and 2","correct":true},{"id":1677544,"option":"None of these","correct":false}]},{"q":"<p>You want to configure an interactive cluster in Azure Databricks for auto-termination. When doing so, you want to ensure that the cluster configuration is preserved even after the cluster has been terminated. What can you do to achieve this?</p>","a":[{"id":1677537,"option":"Clone the cluster","correct":false},{"id":1677538,"option":"Pin the cluster","correct":true},{"id":1677539,"option":"Disable the termination","correct":false},{"id":1677540,"option":"Create an Azure runbook to start the cluster","correct":false}]},{"q":"<p>A company wants to make use of Azure Databricks and Azure Data Lake Storage Gen2. You have to ensure that the data in the Data Lake Storage is accessed by using a service principal from Azure Databricks. Which of the following would you implement for this requirement?</p>","a":[{"id":1677533,"option":"Use the shared access signature in Data lake storage","correct":false},{"id":1677534,"option":"Use access keys in Data lake storage","correct":false},{"id":1677535,"option":"Create an application registration in Azure AD","correct":true},{"id":1677536,"option":"Use a secret from Azure Key vault","correct":false}]},{"q":"<p>Which of the following properties does a single node cluster in Azure Databricks have?</p>\n\n<p><strong>Properties</strong></p>\n\n<ol>\n\t<li> It has 0 workers, with the driver node acting as both master and worker.</li>\n\t<li> It cannot be converted to a Standard cluster.</li>\n\t<li> It runs Spark locally with only one executor thread as logical core on the cluster.</li>\n</ol>","a":[{"id":1677525,"option":"Only 1 and 2","correct":true},{"id":1677526,"option":"Only 2 and 3","correct":false},{"id":1677527,"option":"Only 1 and 3","correct":false},{"id":1677528,"option":"All 1, 2 and 3","correct":false}]},{"q":"<p>Which of the following is the easiest way to create a DataFrame visualization in Azure Databricks?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<pre class=\"prettyprint\"><code>call ROC(&lt;dataframe-name&gt;)</code></pre>\n\n<p><strong>2.</strong></p>\n\n<pre class=\"prettyprint\"><code>call display(&lt;dataframe-name&gt;)</code></pre>\n\n<p><strong>3.</strong></p>\n\n<pre class=\"prettyprint\"><code>call residuals(&lt;dataframe-name&gt;)</code></pre>\n\n<p><strong>4.</strong></p>\n\n<pre class=\"prettyprint\"><code>call decision tree(&lt;dataframe-name&gt;)</code></pre>\n\n<p> </p>","a":[{"id":1677521,"option":"1","correct":false},{"id":1677522,"option":"2","correct":true},{"id":1677523,"option":"3","correct":false},{"id":1677524,"option":"4","correct":false}]},{"q":"<p>What is the basic requirement for using string interpolation syntax in SQL source queries when working with Azure Synapse Analytics?</p>","a":[{"id":1677517,"option":"The query string must be on one single line, with '/n'","correct":false},{"id":1677518,"option":"The query string must be on one single line, without '/n'","correct":true},{"id":1677519,"option":"The query string must be on multiple lines, with '/n'","correct":false},{"id":1677520,"option":"The query string must be on multiple lines, without '/n'","correct":false}]},{"q":"<p>You are trying to preserve metadata and ACLs using copy activity in Azure Synapse Analytics. Which of the following will allow different sets of characters in the keys of customer specified metadata when doing so?</p>","a":[{"id":1677513,"option":"Amazon S3","correct":false},{"id":1677514,"option":"Azure Blob storage","correct":false},{"id":1677515,"option":"Both 1 and 2","correct":true},{"id":1677516,"option":"None of these","correct":false}]},{"q":"<p>Which of the following creates a restore point you can leverage to recover your data warehouse to a previous state in Azure Synapse Dedicated SQL pool?</p>","a":[{"id":1677509,"option":"A data warehouse snapshot","correct":true},{"id":1677510,"option":"Restore points","correct":false},{"id":1677511,"option":"User defined restore points","correct":false},{"id":1677512,"option":"Retention snapshots","correct":false}]},{"q":"<p>Which of these are necessary steps that need to be taken when creating an Azure Synapse workspace with Azure PowerShell?</p>\n\n<p><strong>Steps</strong></p>\n\n<ol>\n\t<li> Define necessary environment variables to create resources for Azure Synapse workspace</li>\n\t<li> Create a resource group as a container for your Azure Synapse workspace</li>\n</ol>","a":[{"id":1677505,"option":"Only 1","correct":false},{"id":1677506,"option":"Only 2","correct":false},{"id":1677507,"option":"Both 1 and 2","correct":true},{"id":1677508,"option":"None of these","correct":false}]},{"q":"<p>Which of the following table action can be used to remove all rows from the target table in a SQL database when working with Azure Synapse Analytics?</p>","a":[{"id":1677497,"option":"Recreate","correct":false},{"id":1677498,"option":"Truncate","correct":true},{"id":1677499,"option":"Remove","correct":false},{"id":1677500,"option":"Retrieve","correct":false}]},{"q":"<p>Which of the following type of Pipeline-to-trigger relationship is supported by Tumbling window triggers during pipeline execution in Azure Synapse Analytics?</p>","a":[{"id":1677485,"option":"One-to-one relationship","correct":true},{"id":1677486,"option":"One-to-many relationship","correct":false},{"id":1677487,"option":"Many-to-many relationship","correct":false},{"id":1677488,"option":"Many-to-one relationship","correct":false}]},{"q":"<p>Which of these object types is not supported by serverless SQL pools in Synapse SQL?</p>","a":[{"id":1677481,"option":"Tables","correct":true},{"id":1677482,"option":"Views","correct":false},{"id":1677483,"option":"Schemas","correct":false},{"id":1677484,"option":"Procedures","correct":false}]},{"q":"<p>Assume that there are a set of tables located on several remote databases in an Azure SQL Database. You have a horizontal partitioning scenario that requires access to these tables. You are following steps to configure elastic database queries for this horizontal partitioning.<br>\nIn the given scenario, after which of these steps will you create a shard map that will represent your data tier using the elastic database client library?</p>","a":[{"id":1677477,"option":"CREATE DATABASE SCOPED CREDENTIAL","correct":true},{"id":1677478,"option":"CREATE/DROP EXTERNAL TABLE ","correct":false},{"id":1677479,"option":"CREATE MASTER KEY ","correct":false},{"id":1677480,"option":"CREATE/DROP EXTERNAL DATA SOURCE","correct":false}]},{"q":"<p>You decide to use a collection of databases with a shared set of resources managed via a logical SQL server when deploying Azure SQL Database. Which of these deployment options is being implemented in the given scenario?</p>","a":[{"id":1677473,"option":"Hyperscale","correct":false},{"id":1677474,"option":"Elastic pools","correct":true},{"id":1677475,"option":"Managed Instance","correct":false},{"id":1677476,"option":"SQL Database hosted on a virtual machine","correct":false}]},{"q":"<p>Your company needs to set up an Azure SQL data warehouse where data would be loaded weekly from an Azure SQL database instance. You have to ensure the data is encrypted at rest to meet the security requirements for the data warehouse. Which of the following would you recommend for encrypting the data in the given scenario?</p>","a":[{"id":1677469,"option":"Azure disk encryption ","correct":false},{"id":1677470,"option":"Transparent data encryption","correct":true},{"id":1677471,"option":"SSL certificates","correct":false},{"id":1677472,"option":"TLS encryption","correct":false}]},{"q":"<p>Which of the following statements regarding Azure SQL Database server is incorrect?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>Azure SQL Database Server is just a logical database server</p>\n\n<p><strong>2.</strong></p>\n\n<p>You can change common settings like logins, firewall rules, and auditing rules for all databases inside the Azure SQL database server</p>\n\n<p><strong>3.</strong></p>\n\n<p>Failover groups for all databases cannot be changed inside the server</p>\n\n<p><strong>4.</strong></p>\n\n<p>The Azure SQL database server is like a central administrative point for all databases inside it</p>","a":[{"id":1677465,"option":"1","correct":false},{"id":1677466,"option":"2","correct":false},{"id":1677467,"option":"3","correct":true},{"id":1677468,"option":"4","correct":false}]},{"q":"<p>You want to maintain control over the maintenance and patch timings of your database. Which of these Azure SQL database options can be used to meet your requirement?</p>","a":[{"id":1677461,"option":"Single database","correct":false},{"id":1677462,"option":"Elastic Pool","correct":false},{"id":1677463,"option":"Managed Instance","correct":false},{"id":1677464,"option":"SQL Server in VM","correct":true}]},{"q":"<p>You want to allow a user to create, edit, or delete any data factory in a resource group from the Azure portal. Which of these custom roles should you assign to the user in order to achieve this?</p>","a":[{"id":1677453,"option":"Assign the built-in reader role on the data factory resource","correct":false},{"id":1677454,"option":"Assign the built-in contributor role on the data factory resource","correct":false},{"id":1677455,"option":"Assign the built-in Data Factory contributor role at the data factory level","correct":false},{"id":1677456,"option":"Assign the built-in Data Factory contributor role at the resource group level","correct":true}]},{"q":"<p>You are using the TRY-CATCH block given alongside for handling errors in Data Factory. In the given context, what will the overall pipeline outcome be if the previous activity fails?</p>","a":[{"id":1677449,"option":"It succeeds if Upon Failure path succeeds","correct":true},{"id":1677450,"option":"It Fails if Upon Failure path succeeds fails","correct":false},{"id":1677451,"option":"It is skipped","correct":false},{"id":1677452,"option":"None of these","correct":false}]},{"q":"<p>Which of these can be encrypted using the Transaction Data Encryption (TDE) in Azure SQL Database?</p>","a":[{"id":1677437,"option":"Database and Transaction log files","correct":false},{"id":1677438,"option":"Associated Backup","correct":false},{"id":1677439,"option":"Both 1 and 2","correct":true},{"id":1677440,"option":"Neither 1 nor 2","correct":false}]},{"q":"<p>Which of the following service tiers would you use in Azure SQL Database for budget-oriented balanced compute and storage options?</p>","a":[{"id":1677405,"option":"Hyperscale Service Tier","correct":false},{"id":1677406,"option":"Business Critical/Premium Service Tier","correct":false},{"id":1677407,"option":"General Purpose/Standard Service Tier","correct":true},{"id":1677408,"option":"Hybrid Service Tier","correct":false}]},{"q":"<p>You accidentally deleted a Databricks notebook experiment in Python using the API given below.</p>\n\n<pre class=\"prettyprint\"><code>MlflowClient.tracking.delete_experiment()</code></pre>\n\n<p>In which of these folders will the notebook be moved to?</p>","a":[{"id":1677401,"option":"User folder","correct":false},{"id":1677402,"option":"Trash folder","correct":true},{"id":1677403,"option":"Special folder","correct":false},{"id":1677404,"option":"Workspace root folder","correct":false}]},{"q":"<p>Which of these access controls is always enabled in Azure Databricks SQL Analytics even when table access control is disabled in the Azure Databricks Workspace?</p>","a":[{"id":1677397,"option":"Alert access control","correct":false},{"id":1677398,"option":"Dashboard access control","correct":false},{"id":1677399,"option":"Data access control","correct":true},{"id":1677400,"option":"SQL endpoint access control","correct":false}]},{"q":"<p>Which of the following permission levels for a databricks cluster allows a user to attach, restart, resize a cluster?</p>","a":[{"id":1677393,"option":"Can Manage","correct":true},{"id":1677394,"option":"Can Restart","correct":false},{"id":1677395,"option":"Can Attach To","correct":false},{"id":1677396,"option":"No Permissions","correct":false}]},{"q":"<p>Which of these permission levels should be assigned to a job in databricks to view the results of a job run?</p>\n\n<p><strong>Permissions</strong></p>\n\n<ol>\n\t<li>Can View</li>\n\t<li>Can Manage Run</li>\n\t<li>Is Owner</li>\n\t<li>Can Manage</li>\n</ol>","a":[{"id":1677389,"option":"Only 1 and 2","correct":false},{"id":1677390,"option":"Only 2 and 3","correct":false},{"id":1677391,"option":"Only 1 and 3","correct":false},{"id":1677392,"option":"None of these","correct":true}]},{"q":"<p>You are using the dbutils.notebook API method given alongside to build notebook workflows in databricks. Which of these characters do the arguments parameter in the given method accept?</p>\n\n<pre class=\"prettyprint\"><code>run(path: String, timeout_seconds: int, arguments: Map): String</code></pre>\n\n<p> </p>","a":[{"id":1677385,"option":"Latin characters","correct":true},{"id":1677386,"option":"Chinese characters","correct":false},{"id":1677387,"option":"Japanese kanji characters","correct":false},{"id":1677388,"option":"Emoji characters","correct":false}]},{"q":"<p>Which of these permissions should you apply to a folder in a databricks workspace if you want to be able to change it later?</p>","a":[{"id":1677381,"option":"Read","correct":false},{"id":1677382,"option":"Edit","correct":false},{"id":1677383,"option":"Run","correct":false},{"id":1677384,"option":"Manage","correct":true}]},{"q":"<p>You are working with the Databricks workspace object \"folders\". What permission will all other users of a home directory have if the user of the home directory has Manage permission?</p>","a":[{"id":1677377,"option":"No Permissions","correct":true},{"id":1677378,"option":"Read","correct":false},{"id":1677379,"option":"Run ","correct":false},{"id":1677380,"option":"Edit","correct":false}]},{"q":"<p>You have assigned the Edit permission to a folder in a Databricks workspace.<br>\nWhich of the following actions can you take as a result of this?</p>","a":[{"id":1677373,"option":"You can list all the items","correct":false},{"id":1677374,"option":"You can view all the items","correct":false},{"id":1677375,"option":"You can clone and export items","correct":false},{"id":1677376,"option":"You can create, import, and delete items","correct":true}]},{"q":"<p>You want to install Python libraries and create an environment scoped to a notebook session in Databricks. Which of the following modes in Databricks workspace UI can be used to do so?</p>","a":[{"id":1677365,"option":"Cluster libraries","correct":false},{"id":1677366,"option":"Workspace libraries","correct":false},{"id":1677367,"option":"Notebook-scoped libraries","correct":true},{"id":1677368,"option":"Either 1 or 3","correct":false}]},{"q":"<p>You notice that multiple error messages are returned when connectivity issues occur in your sink datastore. How can you resolve this issue if you're using Azure IR in Data Factory?</p>","a":[{"id":1677361,"option":"Disable the firewall setting of the datastore","correct":true},{"id":1677362,"option":"Check your proxy and firewall settings","correct":false},{"id":1677363,"option":"Check for failures in a multiple-node self-hosted IR","correct":false},{"id":1677364,"option":"None of these","correct":false}]},{"q":"<p>Which of the following error messages will be returned in Data factory by Azure Function Activity in case of incorrect function URLs?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>Invalid HttpMethod: '%method;'.</p>\n\n<p><strong>2.</strong></p>\n\n<p>Response Content is not a valid JObject.</p>\n\n<p><strong>3.</strong></p>\n\n<p>There was an error while calling endpoint.</p>\n\n<p><strong>4.</strong></p>\n\n<p>Call to provided Azure function '%FunctionName;' failed with status-'%statusCode;' and message - '%message;'.</p>","a":[{"id":1677357,"option":"1","correct":false},{"id":1677358,"option":"2","correct":false},{"id":1677359,"option":"3","correct":true},{"id":1677360,"option":"4","correct":false}]},{"q":"<p>You have a Data Factory and an Azure function app running on a private endpoint. You are trying to run a pipeline that interacts with the function app using three different methods out of which</p>\n\n<ol>\n\t<li>One method returns the error \"Bad Request\"</li>\n\t<li>The other two methods return \"103 Error Forbidden\"</li>\n</ol>\n\n<p>Which of these resolutions can you adopt to fix this problem?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>Apply the correct monitoring filters</p>\n\n<p><strong>2.</strong></p>\n\n<p>Run your pipelines at different trigger times</p>\n\n<p><strong>3.</strong></p>\n\n<p>Create a PrivateLinkService endpoint and provide your function app's DNS</p>\n\n<p><strong>4.</strong></p>\n\n<p>Create a new integration runtime, and split your pipelines across multiple integration runtimes</p>","a":[{"id":1677353,"option":"1","correct":false},{"id":1677354,"option":"2","correct":false},{"id":1677355,"option":"3","correct":true},{"id":1677356,"option":"4","correct":false}]},{"q":"<p>Which of the following cluster sizes is not available for compute optimized Spark clusters in Data factory?</p>","a":[{"id":1677345,"option":"4 driver nodes and 4 worker nodes","correct":true},{"id":1677346,"option":"16 driver nodes and 32 worker nodes","correct":false},{"id":1677347,"option":"16 driver nodes and 16 worker nodes","correct":false},{"id":1677348,"option":"16 driver nodes and 64 worker nodes","correct":false}]},{"q":"<p>Consider the graphical node monitoring view of a Data Factory transformation given alongside. What do the green circle icons in the respective transformation represent?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>They show the number of success and failed rows</p>\n\n<p><strong>2.</strong></p>\n\n<p>They show partition stats, column counts, skewness, and kurtosis</p>\n\n<p><strong>3.</strong></p>\n\n<p>They represent the codes that were executed together on the cluster</p>\n\n<p><strong>4.</strong></p>\n\n<p>They represent the count of the number of sinks that data is flowing into</p>","a":[{"id":1677333,"option":"1","correct":false},{"id":1677334,"option":"2","correct":false},{"id":1677335,"option":"3","correct":false},{"id":1677336,"option":"4","correct":true}]},{"q":"<p>You want to generate a quick Select transformation on a column when debugging in Data factory. Which of these options should you select from the data preview toolbar to do so?</p>","a":[{"id":1677329,"option":"Typecast","correct":false},{"id":1677330,"option":"Modify","correct":false},{"id":1677331,"option":"Statistics","correct":false},{"id":1677332,"option":"Remove","correct":true}]},{"q":"<p>You want to write to a folder in the sink transformation when using the Azure Data Lake Storage Gen2 as Sink type. In the given scenario, which of these tabs lets you manage how the files get written?</p>","a":[{"id":1677321,"option":"Inspect","correct":false},{"id":1677322,"option":"Optimize","correct":false},{"id":1677323,"option":"Mapping","correct":false},{"id":1677324,"option":"Settings","correct":true}]},{"q":"<p>Which of the following options can be used if you want to copy data from Azure Data Lake Storage Gen2?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Copy from the given path specified in the dataset</li>\n\t<li>Wildcard filter against folder path or file name</li>\n\t<li>Copy the files defined in a given text file as file set</li>\n\t<li>Copy the given text specified in the files to another file</li>\n</ol>","a":[{"id":1677313,"option":"2 and 4","correct":false},{"id":1677314,"option":"3 and 4","correct":false},{"id":1677315,"option":"1, 2 and 3 ","correct":true},{"id":1677316,"option":"1, 2 and 4","correct":false}]},{"q":"<p>Assume that you have provided security to the data in Azure Data Lake Storage Gen2. Which of the technologies/utilities given alongside can you use for configuring this security?</p>\n\n<p><strong>Technologies</strong></p>\n\n<ol>\n\t<li> Hive</li>\n\t<li> Spark</li>\n\t<li> Azure Storage Explorer</li>\n</ol>","a":[{"id":1677305,"option":"1 and 3","correct":false},{"id":1677306,"option":"1","correct":false},{"id":1677307,"option":"3","correct":false},{"id":1677308,"option":"All of these","correct":true}]},{"q":"<p>At which of these levels should you set the permission for the data stored within a data lake?<br>\n<strong>Levels</strong></p>\n\n<ol>\n\t<li>Directory level</li>\n\t<li>Node level</li>\n\t<li>File level</li>\n\t<li>Storage level</li>\n</ol>","a":[{"id":1677301,"option":"3","correct":false},{"id":1677302,"option":"4","correct":false},{"id":1677303,"option":"2","correct":false},{"id":1677304,"option":"1","correct":true}]},{"q":"<p>Which of the following permissions can you get for data stored in a data lake by using Azure Data Lake Storage Gen2?</p>\n\n<p><strong>Permissions</strong></p>\n\n<ol>\n\t<li>Access control lists (ACLs)</li>\n\t<li>Portable Operating System Interface (POSIX)</li>\n\t<li>AdlCopy</li>\n\t<li>DistCp</li>\n</ol>","a":[{"id":1677297,"option":"1, 2 and 4","correct":false},{"id":1677298,"option":"1, 2 and 3","correct":false},{"id":1677299,"option":"1 and 2","correct":true},{"id":1677300,"option":"1","correct":false}]},{"q":"<p>Assume that the managed table in which you are going to insert data is specified by an identifier. In which of the following scenarios will the data be inserted into the table in the given context?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1</strong></p>\n\n<p>If the Identifier is a simple identifier</p>\n\n<p><strong>2</strong></p>\n\n<p>If the Identifier is a two-part identifier</p>\n\n<p><strong>3</strong></p>\n\n<p>If the Identifier is a three-part identifier</p>\n\n<p><strong>4</strong></p>\n\n<p>If the Identifier is a four-part identifier</p>","a":[{"id":1677281,"option":"1","correct":false},{"id":1677282,"option":"2","correct":false},{"id":1677283,"option":"3","correct":true},{"id":1677284,"option":"4","correct":false}]},{"q":"<p>Which of the given keys is Azure Data Factory <em>NOT</em> composed of?</p>\n\n<p><strong>Keys</strong></p>\n\n<ol>\n\t<li>Data Flows</li>\n\t<li>Integration Runtimes</li>\n\t<li>API</li>\n\t<li>Pipelines</li>\n</ol>","a":[{"id":1677261,"option":"Only 1","correct":false},{"id":1677262,"option":"Only 2","correct":false},{"id":1677263,"option":"Only 3","correct":true},{"id":1677264,"option":"Only 4","correct":false}]},{"q":"<p>What can be used to transform data collected in centralized data stored in the cloud?</p>","a":[{"id":1677253,"option":"ADF mapping clusters","correct":false},{"id":1677254,"option":"ADF mapping data flows","correct":true},{"id":1677255,"option":"ADF clusters","correct":false},{"id":1677256,"option":"ADF spark clusters","correct":false}]},{"q":"<p>Which of the following is <em>NOT</em><strong> </strong>true about Azure Data Factory?</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>Data Factory contains a series of interconnected systems that provide a complete end-to-end platform for data engineers</li>\n\t<li>Data Factory is a cloud-based ETL and data integration service that allows you to create data-driven workflows</li>\n</ol>","a":[{"id":1677245,"option":"Only 1","correct":false},{"id":1677246,"option":"Only 2","correct":false},{"id":1677247,"option":"Both 1 and 2","correct":false},{"id":1677248,"option":"None of these","correct":true}]},{"q":"<p>You want to reduce response times during high query workloads when working with Azure Analysis Services. Which of these tiers can you use to achieve this?</p>","a":[{"id":1677229,"option":"Standard Tier","correct":false},{"id":1677230,"option":"Basic Tier","correct":false},{"id":1677231,"option":"Developer Tier","correct":false},{"id":1677232,"option":"Scale Out","correct":true}]},{"q":"<p>You want to use Azure Analysis Service for the production of small table models with limited user concurrency. Which of these tiers can you use in this context?</p>","a":[{"id":1677213,"option":"Basic Tier","correct":true},{"id":1677214,"option":"Developer Tier","correct":false},{"id":1677215,"option":"Standard Tier","correct":false},{"id":1677216,"option":"Advanced Tier","correct":false}]},{"q":"<p>Which of these factors affects the cost of a plan/Tier in Azure Analysis Service?</p>\n\n<p><strong>Factors</strong></p>\n\n<ol>\n\t<li>Query Processing Unit(QPU)</li>\n\t<li>Memory Size</li>\n\t<li>Processing Power</li>\n</ol>","a":[{"id":1677209,"option":"Only 1 and 2","correct":false},{"id":1677210,"option":"Only 2 and 3","correct":false},{"id":1677211,"option":"Only 1 and 3","correct":false},{"id":1677212,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>Read the statements given below carefully and choose the correct option.</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>S1: Azure Analysis Services is a fully managed platform as a service (PaaS).</li>\n\t<li>S2: Azure Analysis Services secure your data in a single, trusted tabular semantic data model.</li>\n</ol>","a":[{"id":1677205,"option":"S1 is true and S2 is false","correct":false},{"id":1677206,"option":"S1 false and S2 is true ","correct":false},{"id":1677207,"option":"S1 is true and S2 is true ","correct":true},{"id":1677208,"option":"S1 is false and S2 is false ","correct":false}]},{"q":"<p>You are trying to containerize a Azure service fabric windows gaming deployment.<br>\nWhat should you do if you are unable to see the build definition template while doing so?</p>","a":[{"id":1673151,"option":"Ensure that the New YAML pipeline creation experience feature is turned off","correct":true},{"id":1673152,"option":"Ensure that New SAML pipeline creation feature is turned off","correct":false},{"id":1673153,"option":"Ensure that the New XAML pipeline creation feature is turned on","correct":false},{"id":1673154,"option":"Ensure that the New XML pipeline creation feature is turned off","correct":false}]},{"q":"<p>You have a standalone Azure service fabric cluster that needs to be reconfigured. However, before you proceed with the reconfiguration, you need to identify nodes with the IsSeedNode=”true” tag.<br>\nWhich of these queries are you likely to run in the given scenario?</p>","a":[{"id":1673147,"option":"Get-ServiceFabricClusterManifest","correct":true},{"id":1673148,"option":"Get-ServiceFabricSeedNode","correct":false},{"id":1673149,"option":"Get-ServiceFabricNodeType","correct":false},{"id":1673150,"option":"Get-ServiceFabricManifest","correct":false}]},{"q":"<p>How many consumption models are offered by Azure Synapse SQL?</p>","a":[{"id":1688761,"option":"One","correct":false},{"id":1688762,"option":"Two","correct":true},{"id":1688763,"option":"Three","correct":false},{"id":1688764,"option":"Four","correct":false}]},{"q":"<p>You are trying to save costs for resources with reserved capacity in Azure SQL Database. What should the size of the reservation be based on in the given scenario?</p>","a":[{"id":1678361,"option":"On the total amount of compute used by the existing database ","correct":true},{"id":1678362,"option":"On the purchase reservations","correct":false},{"id":1678363,"option":"On the existing databases in SQL Database","correct":false},{"id":1678364,"option":"None of these","correct":false}]},{"q":"<p>Which of the following service tiers are used by Azure SQL Database and SQL Managed Instances?</p>","a":[{"id":1678365,"option":"General Purpose","correct":false},{"id":1678366,"option":"Business Critical","correct":false},{"id":1678367,"option":"Both 1 and 2","correct":true},{"id":1678368,"option":"None of these","correct":false}]},{"q":"<p>You are performing table partitioning in Azure SQL database to make large tables more manageable and scalable. Which of these queries can you execute if you want to perform recurring partition maintenance in the given scenario?</p>","a":[{"id":1678325,"option":" ALTER PARTITION FUNCTION","correct":true},{"id":1678326,"option":"ALTER TABLE FUNCTION","correct":false},{"id":1678327,"option":"SWITCH PARTITION FUNCTION","correct":false},{"id":1678328,"option":"SWITCH OUT PARTITION","correct":false}]},{"q":"<p>A company wants to use the Azure SQL database service. Given that Business apps will be accessing the database and application data must be available in the event of a region-wide outage. Below are the key requirements that need to be met:<br>\n1. Data must be available in the secondary region if the primary region goes down.<br>\n2. The storage and compute layers for the SQL database must be integrated and replicated together.</p>\n\n<p>Which of the following would you use as the redundancy type in the given scenario?</p>","a":[{"id":1678237,"option":"SQL Sync","correct":false},{"id":1678238,"option":"Zone-Redundancy","correct":false},{"id":1678239,"option":"Geo-redundant storage","correct":true},{"id":1678240,"option":"Local-redundant storage","correct":false}]},{"q":"<p>You are using the BuiltinSqlPoolDataRequestsEnded metric to monitor the number of SQL requests that ended for the built-in serverless SQL pool in a Synapse workspace.<br>\nWhich of these dimensions of the metrics should be used to filter the results by the final state?</p>","a":[{"id":1687061,"option":"Result dimension","correct":true},{"id":1687062,"option":"Filter dimension","correct":false},{"id":1687063,"option":"State dimension","correct":false},{"id":1687064,"option":"Count dimension","correct":false}]},{"q":"<p>You observe that you cannot upload a file in your Azure Databricks workspace after creating a table using UI. Which of these clusters can cause such an issue to occur?</p>","a":[{"id":1677529,"option":"A Job cluster","correct":false},{"id":1677530,"option":"An All-purpose cluster","correct":false},{"id":1677531,"option":"A Low Concurrency cluster","correct":false},{"id":1677532,"option":"A High Concurrency cluster","correct":true}]},{"q":"<p>Your company is using an on-premise Microsoft SQL Server and want to migrate the database to Azure SQL databases. Which of the following must be used as file type for exporting the on-premise database in the given scenario?</p>","a":[{"id":1678209,"option":"VHD","correct":false},{"id":1678210,"option":"DAC","correct":false},{"id":1678211,"option":"BACPAC","correct":true},{"id":1678212,"option":"VHDX","correct":false}]},{"q":"<p>Which of the following will be used by vCore-based purchasing model in Azure SQL Database to gain cost savings?</p>","a":[{"id":1678193,"option":"Azure Hybrid Benefit Model for SQL Server","correct":true},{"id":1678194,"option":"Serverless model","correct":false},{"id":1678195,"option":"DTU-based purchasing model","correct":false},{"id":1678196,"option":"T-SQL Model","correct":false}]},{"q":"<p>Which model would you use in Azure SQL database if you want to be able to choose the following characterstics?<br>\n1. Number of vCores<br>\n2. The amount of memory<br>\n3. The amount and speed of storage</p>","a":[{"id":1678189,"option":"DTU-based purchasing model","correct":false},{"id":1678190,"option":"serverless model","correct":false},{"id":1678191,"option":"vCore-based purchasing model","correct":true},{"id":1678192,"option":"Coreless purchasing model","correct":false}]},{"q":"<p>A company wants to ensure the availability of the Databricks cluster even during the time of regional Azure Datacenter outages. Which of the following type of storage should you suggest to achieve this objective?</p>","a":[{"id":1678157,"option":"Geo-redundant storage","correct":true},{"id":1678158,"option":"Zone-redundant storage","correct":false},{"id":1678159,"option":"Locally redundant storage","correct":false},{"id":1678160,"option":"Read-access geo-redundant storage","correct":false}]},{"q":"<p>Assume that an Azure Databricks cluster fails to launch with the following error message.</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>Library installation timed out after 1800 seconds. Libraries that are not yet installed:</code></pre>\n\n<p>What can you do to fix this issue?</p>","a":[{"id":1688677,"option":"Re-run the job or restart the cluster","correct":true},{"id":1688678,"option":"Use clusters with a larger node type and smaller number of nodes","correct":false},{"id":1688679,"option":"Add a user-defined route to give the Azure Databricks control plane ssh access to the cluster instances, Blob Storage instances, and artifact resources","correct":false},{"id":1688680,"option":"Use a cluster-scoped init script instead of global or cluster-named init scripts","correct":false}]},{"q":"<p>You encounter the following error message when setting up the Azure Databricks cluster in a virtual network(VNet).</p>\n\n<p><strong>Error message</strong></p>\n\n<pre class=\"prettyprint\"><code>Timeout while placing node</code></pre>\n\n<p>What could be a possible reason behind this unexpected error?</p>","a":[{"id":1688673,"option":"The traffic from data plane to workers is blocked","correct":false},{"id":1688674,"option":"The traffic from control plane to workers is blocked","correct":false},{"id":1688675,"option":"The traffic from workers to Azure Storage endpoints is blocked","correct":true},{"id":1688676,"option":"The VNet or subnets do not exist any more","correct":false}]},{"q":"<p>What might be a possible cause for the error code: GenericIssues to pop up in your Data Factory portal during the execution of a SQL Server Integration Services (SSIS) package?</p>","a":[{"id":1688633,"option":"Transient network issue","correct":false},{"id":1688634,"option":"Test connectivity timed out","correct":false},{"id":1688635,"option":"There is a problem with your custom DNS","correct":false},{"id":1688636,"option":"The test connection encountered a general temporary problem","correct":true}]},{"q":"<p>Which of the following statements are valid with respect to configuring and using Azure Synapse Link for Azure Cosmos DB?</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li> Turning on Synapse Link does not turn on the analytical store automatically.</li>\n\t<li> Analytical store should be enabled on containers to start replicating your operation data to analytical store.</li>\n</ol>","a":[{"id":1677493,"option":"Only 1","correct":false},{"id":1677494,"option":"Only 2","correct":false},{"id":1677495,"option":"Both 1 and 2","correct":true},{"id":1677496,"option":"None of these","correct":false}]},{"q":"<p>You want to use a Azure SQL database for an application that requires only a single data source. Which of the following databases should you use in the given scenario?</p>","a":[{"id":1677457,"option":"Single database","correct":true},{"id":1677458,"option":"Elastic Pool","correct":false},{"id":1677459,"option":"Managed Instance","correct":false},{"id":1677460,"option":"Database pool","correct":false}]},{"q":"<p>You have three pipelines in Data Factory that execute at 10:00 AM, 12:00 PM, and 2:00 PM. What will the exact number of pipeline runs and pipeline run IDs be for all of the given three pipelines?</p>","a":[{"id":1677445,"option":"One pipeline run and One unique pipeline run ID","correct":false},{"id":1677446,"option":"Three pipeline runs and Three unique pipeline run IDs","correct":true},{"id":1677447,"option":"One pipeline run and Three unique pipeline run IDs","correct":false},{"id":1677448,"option":"Three pipeline runs and One unique pipeline run ID","correct":false}]},{"q":"<p>Assume that the trigger is not a tumbling window trigger. What type of relationship is shared between a pipeline and a trigger in Azure Data Factory? </p>","a":[{"id":1678125,"option":"One-to-one relationship","correct":false},{"id":1678126,"option":"One-to-many relationship","correct":false},{"id":1678127,"option":"Many-to-many relationship","correct":true},{"id":1678128,"option":"Self-joining relationship","correct":false}]},{"q":"<p>Which of the following activities is supported by Azure Data Factory?<br>\n1. Data movement activity<br>\n2. Data transformation activity<br>\n3. Control activity</p>","a":[{"id":1678105,"option":"1 and 2","correct":false},{"id":1678106,"option":"2 and 3","correct":false},{"id":1678107,"option":"1 and 3","correct":false},{"id":1678108,"option":"All of these","correct":true}]},{"q":"<p>What should you integrate Azure Data Lake Analytics with to use it for user management and permissions?</p>","a":[{"id":1688613,"option":"Active Directory","correct":true},{"id":1688614,"option":"Access Control","correct":false},{"id":1688615,"option":"YARN","correct":false},{"id":1688616,"option":"Spark","correct":false}]},{"q":"<p>How can you set up the storage account as an Azure Data Lake Storage Gen2 account if you are performing analytics on the data?</p>","a":[{"id":1688577,"option":"by setting SQL Pools","correct":false},{"id":1688578,"option":"by setting the Hierarchical Namespace option to Disabled","correct":false},{"id":1688579,"option":"by setting the Hierarchical Namespace option to Enabled","correct":true},{"id":1688580,"option":"by using Azure RBAC","correct":false}]},{"q":"<p>You are using Hadoop Compatible Access to store the data in one place and access it without moving the data between environments. Which of the compute technologies given alongside can you use to do so?</p>\n\n<p><strong>Technologies</strong></p>\n\n<ol>\n\t<li>Azure Databricks</li>\n\t<li>Azure HDInsight</li>\n\t<li>Azure Synapse Analytics</li>\n\t<li>Azure Data Analytics</li>\n</ol>","a":[{"id":1677293,"option":"1","correct":false},{"id":1677294,"option":"2, 3 and 4","correct":false},{"id":1677295,"option":"1, 2 and 4","correct":false},{"id":1677296,"option":"1, 2 and 3","correct":true}]},{"q":"<p>Which type of advanced monitoring and troubleshooting feature provided by Azure SQL Database enables you to find real-time performance insights?</p>\n\n<p><strong>Options</strong></p>\n\n<p><strong>1.</strong></p>\n\n<p>PaaS monitoring capabilities provided by SQL Server database engine</p>\n\n<p><strong>2.</strong></p>\n\n<p>PaaS monitoring capabilities provided by Azure</p>\n\n<p><strong>3.</strong></p>\n\n<p>Built-in monitoring capabilities provided by the SQL Server database engine</p>\n\n<p><strong>4.</strong></p>\n\n<p>Built-in monitoring capabilities provided by Azure</p>","a":[{"id":1677417,"option":"1","correct":false},{"id":1677418,"option":"2","correct":false},{"id":1677419,"option":"3","correct":true},{"id":1677420,"option":"4","correct":false}]},{"q":"<p>You observe that Data Lake Storage Gen2 takes advantage of the Azure Blob replication models that provide data redundancy in a single data center to a secondary region. Which of the following do you think is used to achieve this?</p>","a":[{"id":1677317,"option":"Geo-redundant storage (GRS)","correct":true},{"id":1677318,"option":"Remote-redundant storage (RRS)","correct":false},{"id":1677319,"option":"hierarchy of directories","correct":false},{"id":1677320,"option":"locally redundant storage (LRS)","correct":false}]},{"q":"<p>Which of these are valid benefits provided by Azure Data Lake storage while processing stored data?<br>\n<strong>Benefits</strong></p>\n\n<ol>\n\t<li>Less computational resources</li>\n\t<li>Reduced time</li>\n\t<li>Reduced cost</li>\n\t<li>Easier navigation</li>\n</ol>","a":[{"id":1677309,"option":"1, 2 and 4","correct":false},{"id":1677310,"option":"All the benefits","correct":true},{"id":1677311,"option":"1 and 3","correct":false},{"id":1677312,"option":"2, 3 and 4","correct":false}]},{"q":"<p>You want the data to work in the same way as it is when stored in a Hadoop Distributed File System when working on Azure Data Lake.<br>\nWhich Data lake would you use in order to achieve this?</p>","a":[{"id":1677289,"option":"Data Lake Storage Gen2","correct":true},{"id":1677290,"option":"Data Lake Storage Gen1","correct":false},{"id":1677291,"option":"dedicated SQL pools","correct":false},{"id":1677292,"option":"U-SQL jobs","correct":false}]},{"q":"Which of these actions can you take to load data into a Spark DataFrame when developing in Synapse using Azure Purview?","a":[{"id":1687005,"option":" New SQL Script","correct":false},{"id":1687006,"option":"New notebook","correct":true},{"id":1687007,"option":" New data flow","correct":false},{"id":1687008,"option":"New table","correct":false}]},{"q":"Which of the following workspace objects inherit all the permission settings of a Databricks workspace folder of which they are a part of?","a":[{"id":1686909,"option":"Notebook and Dashboard","correct":false},{"id":1686910,"option":"Library and Experiment","correct":false},{"id":1686911,"option":"Notebook and Experiment","correct":true},{"id":1686912,"option":"Dashboard and Library","correct":false}]},{"q":"<p>In which of the given scenarios can linked services be used in Azure Data Factory?</p>\n\n<p><strong>Scenarios</strong></p>\n\n<ol>\n\t<li>To represent a compute resource that can host the execution of an activity.</li>\n\t<li>To represent the unit of processing that determines when a pipeline execution needs to be kicked off.</li>\n</ol>","a":[{"id":1677277,"option":"Only 1","correct":true},{"id":1677278,"option":"Only 2","correct":false},{"id":1677279,"option":"Both 1 and 2","correct":false},{"id":1677280,"option":"None of these","correct":false}]},{"q":"<p>You have built-up a reusable library of data transformation routines and want to execute those processes in a scaled-out manner. What can you do to achieve this?</p>","a":[{"id":1677269,"option":"Use ADF clusters","correct":false},{"id":1677270,"option":"Use ADF Pipelines","correct":true},{"id":1677271,"option":"Use ADF routines","correct":false},{"id":1677272,"option":"Use ADF Copy activity ","correct":false}]},{"q":"<p>Which of these are built-in supports for monitoring pipelines in Azure Data Factory?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>Azure Monitor</li>\n\t<li>Azure Monitor logs</li>\n\t<li>PowerShell</li>\n</ol>","a":[{"id":1677257,"option":"Only 1 and 2","correct":false},{"id":1677258,"option":"Only 2 and 3","correct":false},{"id":1677259,"option":"Only 1 and 3","correct":false},{"id":1677260,"option":"All 1, 2 and 3","correct":true}]},{"q":"<p>In which following tiers can you select a plan after creating a server using Azure Analysis Service?<br>\n<strong>Tiers</strong></p>\n\n<ol>\n\t<li>Developer</li>\n\t<li>Basic</li>\n\t<li>Standard</li>\n\t<li>Advance</li>\n\t<li>Intermediate</li>\n</ol>","a":[{"id":1677241,"option":"2, 3 and 4","correct":false},{"id":1677242,"option":"1, 2 and 3","correct":true},{"id":1677243,"option":"2, 3 and 5","correct":false},{"id":1677244,"option":"1, 2 and 4","correct":false}]},{"q":"<p>Read the statements given below carefully and choose the correct option.<br>\n<strong>Statements</strong></p>\n\n<ol>\n\t<li>S1: Scale Out helps in reducing response times during high query workloads.</li>\n\t<li>S2: Client queries can be distributed among multiple query replicas in a query pool using Scale Out.</li>\n</ol>","a":[{"id":1677233,"option":"S1 is true and S2 is false","correct":false},{"id":1677234,"option":"S1 false and S2 is true ","correct":false},{"id":1677235,"option":"Both S1 and S2 are true","correct":true},{"id":1677236,"option":"Both S1 and S2 are false","correct":false}]},{"q":"<p>Which of these tiers does not support the following features?</p>\n\n<p><strong>Features</strong></p>\n\n<ol>\n\t<li>Perspectives</li>\n\t<li>Multiple partitions</li>\n\t<li>DirectQuery tabular model</li>\n</ol>","a":[{"id":1677221,"option":"Standard Tier","correct":false},{"id":1677222,"option":"Developer Tier","correct":false},{"id":1677223,"option":"Advanced Tier","correct":false},{"id":1677224,"option":"Basic Tier","correct":true}]},{"q":"You want to perform multiple runs of the same Databricks job concurrently. What should you set the \"Maximum concurrent runs\" value as in order to achieve this?","a":[{"id":1678313,"option":"Less than 0","correct":false},{"id":1678314,"option":"0","correct":false},{"id":1678315,"option":"1","correct":false},{"id":1678316,"option":"Greater than 1","correct":true}]},{"q":"What will be the consequence of increasing the number of data warehouse units (DWUs) in a Azure Synapse Dedicated SQL pool?","a":[{"id":1678269,"option":"The number of readers and writers for PolyBase load operations decreases","correct":false},{"id":1678270,"option":"The maximum number of concurrent queries and concurrency slots decreases","correct":false},{"id":1678271,"option":"The performance of the system changes linearly for scans, aggregations, and CTAS statements","correct":true},{"id":1678272,"option":"The performance of the system changes in a non-linear fashion for scans, aggregations, and CTAS statements","correct":false}]},{"q":"What benefit would you get by using the DTU-based purchasing model in Azure SQL database?","a":[{"id":1678197,"option":"A large amount and speed of storage","correct":false},{"id":1678198,"option":"A bundled measure of compute, memory, and I/O resources in three service tiers","correct":true},{"id":1678199,"option":"A desired amount of memory","correct":false},{"id":1678200,"option":"A desired number of vCores","correct":false}]},{"q":"Which of these techniques would you use for an app with a single Azure SQL database to perform manual scaling without downtime?","a":[{"id":1678177,"option":"Autoscale","correct":false},{"id":1678178,"option":"Dynamic scalability","correct":true},{"id":1678179,"option":"Auto Failover","correct":false},{"id":1678180,"option":"Azure VMs","correct":false}]},{"q":"What is the permission level assigned to a job creator in Databricks?","a":[{"id":1678145,"option":"No Permissions","correct":false},{"id":1678146,"option":"Can View","correct":false},{"id":1678147,"option":"Can Manage Run","correct":false},{"id":1678148,"option":"Is Owner","correct":true}]},{"q":"What is the number of recently terminated job clusters for which Databricks can retain cluster configuration information?","a":[{"id":1678137,"option":"10","correct":false},{"id":1678138,"option":"20","correct":false},{"id":1678139,"option":"30","correct":true},{"id":1678140,"option":"40","correct":false}]},{"q":"Which of the following alert status reveals that Databricks SQL Analytics does not have enough data to evaluate a certain alert criteria?","a":[{"id":1678129,"option":"OK","correct":false},{"id":1678130,"option":"DEFAULT","correct":false},{"id":1678131,"option":"UNKNOWN","correct":true},{"id":1678132,"option":"TRIGGERED","correct":false}]},{"q":"You have created a Azure Databricks workspace in portal. What happens when the workspace deployment fails?","a":[{"id":1678085,"option":"The workspace is still created in a failed state.","correct":true},{"id":1678086,"option":"The workspace is deleted","correct":false},{"id":1678087,"option":"An error is thrown","correct":false},{"id":1678088,"option":"The workspace is created in a resource group","correct":false}]},{"q":"Which of these Markdown image syntaxes can be used to include static images within text boxes on your SQL Analytics dashboards?","a":[{"id":1678081,"option":"![alt-text](<image-url>)","correct":true},{"id":1678082,"option":"[alt-text](<image-url>)","correct":false},{"id":1678083,"option":"![alt-text<image-url>]","correct":false},{"id":1678084,"option":"!(alt-text)<image-url>","correct":false}]},{"q":"Which of these can be used to delete a model database from the primary server?","a":[{"id":1678037,"option":"Sync-AzAnalysisServicesInstance","correct":true},{"id":1678038,"option":"Test-AzAnalysisServicesServer","correct":false},{"id":1678039,"option":"AzContext","correct":false},{"id":1678040,"option":"AzureRmContext","correct":false}]},{"q":"What is the effective Data Integration Units (DIU) for staged storage-to-Synapse when using PolyBase with Azure Integration Runtime?","a":[{"id":1677501,"option":"0","correct":false},{"id":1677502,"option":"1","correct":false},{"id":1677503,"option":"2","correct":true},{"id":1677504,"option":"4","correct":false}]},{"q":"Which of these wildcard patterns represents a recursive directory nesting in Azure Synapse Analytics?","a":[{"id":1677489,"option":"**","correct":true},{"id":1677490,"option":"?","correct":false},{"id":1677491,"option":"( )","correct":false},{"id":1677492,"option":"{ }","correct":false}]},{"q":"Which of the following Automatic Tuning options in Azure SQL Database are invalid?","a":[{"id":1677441,"option":"FORCE PLAN","correct":false},{"id":1677442,"option":"CREATE INDEX","correct":false},{"id":1677443,"option":"DROP INDEX","correct":false},{"id":1677444,"option":"ALTER INDEX","correct":true}]},{"q":"Where are the audit logs stored when auditing Azure SQL Database?","a":[{"id":1677433,"option":"Azure storage account","correct":false},{"id":1677434,"option":"Log Analytics workspace","correct":false},{"id":1677435,"option":"Event Hubs","correct":false},{"id":1677436,"option":"All 1, 2 and 3","correct":true}]},{"q":"What can be used to analyze resource consumption over time if you run Query Store in Azure SQL Database?","a":[{"id":1677425,"option":"Query Performance Insight","correct":true},{"id":1677426,"option":"Management Studio","correct":false},{"id":1677427,"option":"Data Studio","correct":false},{"id":1677428,"option":"Query Studio","correct":false}]},{"q":"Which of these data encryptions does Azure SQL Database use for any data at rest?","a":[{"id":1677429,"option":"Transparent data encryption","correct":true},{"id":1677430,"option":" Transport layer security","correct":false},{"id":1677431,"option":"Advanced Encryption Standard","correct":false},{"id":1677432,"option":"Triple DES","correct":false}]},{"q":"Which of these built-in Azure SQL Server monitoring features allows you to record the performance of your queries in real time?","a":[{"id":1677421,"option":"\nAzure AD","correct":false},{"id":1677422,"option":"\nAzure Synapse Analytics","correct":false},{"id":1677423,"option":"\nAzure SQL VMs (IaaS)","correct":false},{"id":1677424,"option":"Query Store","correct":true}]},{"q":"What is the benefit of using hyperscale service tier when working with Azure SQL Databases?","a":[{"id":1677413,"option":"Autoscale storage and scale compute fluidly","correct":true},{"id":1677414,"option":"Common workloads usability","correct":false},{"id":1677415,"option":"High transaction rate in applications","correct":false},{"id":1677416,"option":"Storage options","correct":false}]},{"q":"Which of these service tiers in Azure SQL Database is designed for OLTP applications with high transaction rate and lowest-latency I/O?","a":[{"id":1677409,"option":"Hybrid Service Tier","correct":false},{"id":1677410,"option":"General Purpose/Standard Service Tier","correct":false},{"id":1677411,"option":"Business Critical/Premium Service Tier","correct":true},{"id":1677412,"option":"Hyperscale Service Tier","correct":false}]},{"q":"Which of these cluster types will you use to create Databricks jobs that are important to complete?","a":[{"id":1677369,"option":"New Job Cluster","correct":true},{"id":1677370,"option":"Existing Job Cluster","correct":false},{"id":1677371,"option":"New All-Purpose Cluster","correct":false},{"id":1677372,"option":"Existing All-Purpose Cluster","correct":false}]},{"q":"Which of the following is the fastest partitioning option for writing files for evenly distributed data in Data factory?","a":[{"id":1677349,"option":"Set Partitioning","correct":false},{"id":1677350,"option":"Use current partitioning","correct":true},{"id":1677351,"option":"Single partition","correct":false},{"id":1677352,"option":"Multiple partition","correct":false}]},{"q":"Which of these Spark clusters is not recommended by Azure Data Factory for most production workloads?","a":[{"id":1677341,"option":"Specific purpose clusters","correct":false},{"id":1677342,"option":"General purpose clusters","correct":false},{"id":1677343,"option":"Memory optimized cluster","correct":false},{"id":1677344,"option":"Compute optimized cluster","correct":true}]},{"q":"Which of the following partitioning options can be used for distributing data equally across several partitions in a Data factory transformation?","a":[{"id":1677337,"option":"Round robin","correct":true},{"id":1677338,"option":"Hash","correct":false},{"id":1677339,"option":"Dynamic range","correct":false},{"id":1677340,"option":"Fixed range","correct":false}]},{"q":"Which of the following Azure Data Factory artifact names should be unique within a data factory?","a":[{"id":1677325,"option":"Data factory","correct":false},{"id":1677326,"option":"Resource Group","correct":false},{"id":1677327,"option":"Integration Runtime","correct":true},{"id":1677328,"option":"Data flow transformations","correct":false}]},{"q":"How does Azure Data Lake help you with quickly identifying insights into your data?","a":[{"id":1677285,"option":"using U-SQL jobs","correct":false},{"id":1677286,"option":"combines a file system with a storage platform","correct":true},{"id":1677287,"option":"integrates with Visual Studio","correct":false},{"id":1677288,"option":"Using appropriate sorting algorithms","correct":false}]},{"q":"Which of the following represents a processing step in a Azure Data Factory Pipeline?","a":[{"id":1677273,"option":"Activities","correct":true},{"id":1677274,"option":"Datasets","correct":false},{"id":1677275,"option":"Linked services","correct":false},{"id":1677276,"option":"Dataflows","correct":false}]},{"q":"How can the activities in a Azure Data Factory pipeline operate?","a":[{"id":1677265,"option":"Sequentially","correct":false},{"id":1677266,"option":"Independently","correct":false},{"id":1677267,"option":"Both 1 and 2","correct":true},{"id":1677268,"option":"None of these","correct":false}]},{"q":"Which of the following can be used to move data from cloud source data stores to a centralised data store in the cloud using Data Factory?","a":[{"id":1677249,"option":"Copy Activity","correct":true},{"id":1677250,"option":"Azure Blob","correct":false},{"id":1677251,"option":"Data pipeline","correct":false},{"id":1677252,"option":"Azure Data Lake ","correct":false}]},{"q":"How many query pools can you create including the primary server by using scale out in Azure Analysis Services?","a":[{"id":1677237,"option":"5","correct":false},{"id":1677238,"option":"8","correct":true},{"id":1677239,"option":"7","correct":false},{"id":1677240,"option":"6","correct":false}]},{"q":"Which of the following tier does not support Service Legal Agreement (SLA)?","a":[{"id":1677225,"option":"Standard Tier","correct":false},{"id":1677226,"option":"Basic Tier","correct":true},{"id":1677227,"option":"Developer Tier","correct":false},{"id":1677228,"option":"Advanced Tier","correct":false}]},{"q":"Which of the following combinations of tiers does not support query replica scale-out feature?","a":[{"id":1677217,"option":"Standard Tier","correct":true},{"id":1677218,"option":"Developer Tier","correct":false},{"id":1677219,"option":"Advanced Tier","correct":false},{"id":1677220,"option":"Basic Tier","correct":false}]},{"q":"<p>In Azure, If your source database contains one or more temporal tables, your database migration fails during the “Full data load” operation and the following message will be displayed:</p>\n\n<pre class=\"prettyprint\"><code>{\n    \"resourceId\":\"/subscriptions//resourceGroups/migrateready/providers/Microsoft.DataMigration/services/\", \"errorType\":\"Database migration error\",\n    \"errorEvents\":\"[\"Capture functionalities could not be set. RetCode: SQL_ERROR SqlState: 42000 NativeError: 13570 Message: [Microsoft][SQL Server Native Client 11.0][SQL Server]The use of replication is not supported with system-versioned temporal table '[Application. Cities]' Line: 1 Column: -1 \"]\"\n}</code></pre>\n\n<p>    <br>\n    Which of the following SQL commands is used to find the temporal tables in your source schema?</p>","a":[{"id":1152721,"option":"select name,temporal_type,* from sys.migrationtables where temporal_type <>0","correct":false},{"id":1152722,"option":"select name,temporal_type,temporal_type_desc,*from sys.tables where temporal_type<>0","correct":true},{"id":1152723,"option":"select name,temporal_type,temporal_type_desc,*from sys.tables where tempotal_type<>1","correct":false},{"id":1152724,"option":"select name,temporal_type,* from sys.migrationtables where temporal_type<>1","correct":false}]},{"q":"<p>In which of the following scenarios is it appropriate to use a SQL Server in an Azure Virtual Machine:</p>\n\n<ol>\n\t<li>You need control on SQL Server and Windows.</li>\n\t<li>You need full compatibility with SQL Server On-premises and want to move existing applications to Azure.</li>\n</ol>\n\n<p> </p>\n\n<p> </p>","a":[{"id":915163,"option":"1","correct":false},{"id":915164,"option":"2","correct":false},{"id":915165,"option":"Both of these","correct":true},{"id":915166,"option":"None of these","correct":false}]},{"q":"<p>In Azure Data Lake Analytics, you are working with Azure PowerShell to manage this Data Lake Analytics. You have logged into Azure with your subscription ID by implementing the following PowerShell cmdlet. Now, you have observed that this cmdlet always prompts for the user credentials. You wanted to avoid these prompts. If you are required to load the login session information from the path <strong>D:\\profile.json</strong>, then which of these cmdlets can be implemented to perform this action in this scenario:</p>\n\n<p><strong>PowerShell cmdlet used to login into Azure</strong></p>\n\n<pre class=\"prettyprint\"><code>Connect-AzAccount -SubscriptionId $subId</code></pre>\n\n<p><strong>cmdlets</strong></p>\n\n<ol>\n\t<li>Connect-AzAccount -path D:\\profile.json</li>\n\t<li>Save-AzAccounts -path D:\\profile.json</li>\n\t<li>Select-AzAccounts -path D:\\profile.json</li>\n</ol>\n\n<p> </p>","a":[{"id":1444026,"option":"1","correct":false},{"id":1444027,"option":"2","correct":false},{"id":1444028,"option":"3","correct":true},{"id":1444029,"option":"None of these","correct":false}]},{"q":"<p>In Azure, you are working with the Azure Cosmos DB resource model. If you have created two Azure Cosmos containers with the same name but different casing, then which of the following situations can be occurred in this scenario:</p>\n\n<ol>\n\t<li>The Confusion of telemetry can occur.</li>\n\t<li>The collision of actions on containers can occur.</li>\n\t<li>Errors in the system-defined properties can occur.</li>\n</ol>","a":[{"id":1444022,"option":"1 and 2","correct":true},{"id":1444023,"option":"2 and 3","correct":false},{"id":1444024,"option":"1 and 3","correct":false},{"id":1444025,"option":"All of these","correct":false}]},{"q":"<p>In Azure DevOps, you are configuring your Continuous Delivery (CD) pipeline for your Jenkins Continuous Integration (CI) Server. If you are required. If  you are required to integrate your Jenkins Server with the Azure Pipelines, then which of the following actions can be performed in this scenario:</p>\n\n<ol>\n\t<li>Run CI jobs in the Jenkins server separately.</li>\n\t<li>Include a docker file in your CD pipeline to avoid Azure pipeline concurrency.</li>\n\t<li>Wrap a Jenkins CI job inside the Azure pipeline</li>\n</ol>","a":[{"id":1161879,"option":"1 and 2","correct":false},{"id":1161880,"option":"2 and 3","correct":false},{"id":1161881,"option":"1 and 3","correct":true},{"id":1161882,"option":"All of these","correct":false}]},{"q":"<p>In Azure DevOps, you want to implement configuration management by using the Ansible tool.</p>\n\n<p>Which of the following agents contain the build dependencies that are used to perform tests on your web applications?</p>","a":[{"id":1161887,"option":"Build ","correct":true},{"id":1161888,"option":"Deployment ","correct":false},{"id":1161889,"option":"Repository ","correct":false},{"id":1161890,"option":"Executable ","correct":false}]},{"q":"<p>In Azure DevOps, if you are required to implement a continuous orchestration process by using the Kubernetes container-orchestration system, then which of the following orchestration services is used to upgrade your Kubernetes cluster?</p>","a":[{"id":1161875,"option":"Azure Container Registry (ACR)","correct":false},{"id":1161876,"option":"Azure Kubernetes Services (AKS)","correct":true},{"id":1161877,"option":"Azure Kubernetes Orchestrator (AKO)","correct":false},{"id":1161878,"option":"None of these","correct":false}]},{"q":"<p>In Azure DevOps, which of the following statements about Azure Pipelines are correct:</p>\n\n<ol>\n\t<li>It is a fully-integrated continuous integration service and a partially-integrated continuous delivery service.</li>\n\t<li>It can be deployed to major cloud services including Azure services.</li>\n\t<li>It is used to configure and automate the build, delivery tools, and environment in the YAML language.</li>\n</ol>","a":[{"id":1161907,"option":"1 and 2","correct":false},{"id":1161908,"option":"2 and 3","correct":true},{"id":1161909,"option":"1 and 3","correct":false},{"id":1161910,"option":"All of these","correct":false}]},{"q":"<p>In Azure DevOps, you are working on the docker container.</p>\n\n<p>Which of the following properties will be enabled if you combine Azure DevOps and Azure integrations with the docker container:</p>\n\n<ol>\n\t<li>You can build custom docker images by using an Azure DevOps hosted Linux agent.</li>\n\t<li>You can push and store the docker images in a private repository.</li>\n\t<li>You can deploy and run the images inside the container.</li>\n</ol>","a":[{"id":1161895,"option":"1 and 2","correct":false},{"id":1161896,"option":"2 and 3","correct":false},{"id":1161897,"option":"1 and 3","correct":false},{"id":1161898,"option":"All of these","correct":true}]},{"q":"<p>In TFS, you are working with Azure Repos. If you are working with the Git distributed version control, then which of the following statements about the isolation of the code with the forks are correct:</p>\n\n<ol>\n\t<li>A fork is a complete copy of a repository, including all files, commits.</li>\n\t<li>The new fork acts as if someone cloned the original repository and then pushed it to a new, empty repository.</li>\n\t<li>After a fork has been created, new files, folders, and branches are not shared between the repositories unless a pull request carries these files along.</li>\n</ol>","a":[{"id":1156891,"option":"1 and 2","correct":false},{"id":1156892,"option":"2 and 3","correct":false},{"id":1156893,"option":"1 and 3","correct":false},{"id":1156894,"option":"All of these","correct":true}]},{"q":"<p>In TFS, you are managing the process templates. If you are required to upload a process template, then which of the following conditions must be valid:</p>\n\n<ol>\n\t<li>Process template names must be unique and must contain less than or equal to 256 Unicode characters.</li>\n\t<li>Process template folders must contain at least one executable (.exe) files in them.</li>\n\t<li>The total size of the process template must less than or equal to 2GB.</li>\n</ol>\n\n<p> </p>","a":[{"id":1156923,"option":"1 and 2","correct":false},{"id":1156924,"option":"2 and 3","correct":false},{"id":1156925,"option":"1 and 3","correct":true},{"id":1156926,"option":"All of these","correct":false}]},{"q":"<p>In Azure DevOps Server, you are working on the Azure test plans. Which of the following manual tests are performed by the development teams, including developers, testers, UX teams, product owners by exploring the software systems without using test plans or test suites?</p>","a":[{"id":1156903,"option":"Planned manual testing","correct":false},{"id":1156904,"option":"User Acceptance testing","correct":false},{"id":1156905,"option":"Exploratory testing","correct":true},{"id":1156906,"option":"Stakeholder feedback","correct":false}]},{"q":"<p>In Azure DevOps Server, which of the following statements about the Azure Pipelines are correct:</p>\n\n<ol>\n\t<li>It is a cloud service that you can use to automatically build and test your code project and make it available to other users.</li>\n\t<li>It is used to connect the Azure repositories with the SQL Server instance to make all the duplicate elements are removed from the data resources.</li>\n\t<li>It combines continuous integration (CI) and continuous delivery (CD) to test and build the user's code consistently.</li>\n</ol>\n\n<p> </p>","a":[{"id":1156899,"option":"1 and 2","correct":false},{"id":1156900,"option":"2 and 3","correct":false},{"id":1156901,"option":"1 and 3","correct":true},{"id":1156902,"option":"All of these","correct":false}]},{"q":"<p>In Azure, you are working on the Table Service Rest API from the storage services to work on the tables and the data that is contained in the tables. If you are required to retrieve statistics related to replication for the Table service by using the <strong>Get Table Service Stats </strong>operation, then which of the replication must be enabled for the storage account in this scenario?</p>\n\n<p> </p>","a":[{"id":738799,"option":"locally redundant replication","correct":false},{"id":738800,"option":"geo-redundant replication","correct":false},{"id":738801,"option":"zone-redundant  replication","correct":false},{"id":738802,"option":" read-access geo-redundant replication","correct":true}]},{"q":"<p>In Azure functions, you are working on triggers and binding definitions. Determine the direction value of a trigger that you are required to include in the JSON file of your function.</p>","a":[{"id":1340971,"option":"in","correct":true},{"id":1340972,"option":"out","correct":false},{"id":1340973,"option":"inout","correct":false},{"id":1340974,"option":"It depends on the input and the output bindings.","correct":false}]},{"q":"<p>You have the access to Azure Virtual Network resources from the Azure Logic Apps by using the Integration Service Environments (ISEs). Which of the following statements represent the advantages of using an ISE in your Azure Logic Apps:</p>\n\n<ol>\n\t<li>If an ISE connector exists for an on-premises system or data source, you can connect directly without having to use the on-premises data gateway.</li>\n\t<li>For each ISE, you can automatically set up additional firewall openings at the destination systems.</li>\n\t<li>Logic apps that you create and run in an ISE can still use connectors that run in the multi-tenant Logic Apps service when an ISE-specific connector is not available.</li>\n</ol>","a":[{"id":1340931,"option":"1 and 2","correct":false},{"id":1340932,"option":"2 and 3","correct":false},{"id":1340933,"option":"1 and 3","correct":true},{"id":1340934,"option":"All of these","correct":false}]},{"q":"<p>In Azure, you are creating a hybrid mode instance with the Azure portal &amp; Azure Database Migration Service. If you are required to communicate with Azure Database Migration Service in the cloud, then which of the following IDs should be created that the on-premises hybrid worker can use?</p>","a":[{"id":1152681,"option":"App registration ID","correct":true},{"id":1152682,"option":"Service registration ID","correct":false},{"id":1152683,"option":"Role registration ID","correct":false},{"id":1152684,"option":"Server registration ID","correct":false}]},{"q":"<p>In Azure, a custom role for the APP ID is required for the Azure Database Migration Service (DMS) at the subscription level. Which of these text files can store the following JSON code to create the roles by using the New-AzureRmRoleDefinition (AzureRM) command:<br>\n    </p>\n\n<pre class=\"prettyprint\"><code> {\n            \"Name\": \"DMS Role - App ID - Sub\",\n            \"IsCustom\": true,\n            \"Description\": \"DMS App ID access at subscription level to complete MI migrations\",\n            \"Actions\": [ \"Microsoft.Sql/locations/managedDatabaseRestoreAzureAsyncOperation/*\"],\n            \"NotActions\": [ ],\n            \"AssignableScopes\": [ \"/subscriptions/\" ]\n}</code></pre>\n\n<p><code>    </code></p>\n\n<p> </p>\n\n<ol>\n\t<li>    Azure CLI</li>\n\t<li>    AzureDMSRM</li>\n\t<li>    AZ Powershell cmdlets</li>\n</ol>","a":[{"id":1152713,"option":"1 and 2 ","correct":false},{"id":1152714,"option":"2 and 3","correct":false},{"id":1152715,"option":"1 and 3","correct":true},{"id":1152716,"option":"All of these","correct":false}]},{"q":"In MS SQL, you are migrating SQL Server to Azure SQL Database Offline by using Data Migration Service. If you are assessing the source SQL Server’s database that is migrating to a pooled database in Azure SQL Database, then which of the following assessment report types is selected by default in this scenario:<br>&nbsp;&nbsp;&nbsp;&nbsp;1. Check database compatibility<br>&nbsp;&nbsp;&nbsp;&nbsp;2. Check feature parity","a":[{"id":1189619,"option":"1","correct":false},{"id":1189620,"option":"2","correct":false},{"id":1189621,"option":"Both of these","correct":true},{"id":1189622,"option":"None of these","correct":false}]},{"q":"In Azure DevOps, which of the following statements about the advantages of using Team Foundation Version Control are correct:<br>&nbsp;&nbsp;&nbsp;&nbsp;1. It is used to automate the testing and the code analysis of a project.<br>&nbsp;&nbsp;&nbsp;&nbsp;2. It is used to remove cache files automatically from the project.<br>&nbsp;&nbsp;&nbsp;&nbsp;3. It is used to create a workflow for the project.","a":[{"id":1161923,"option":"1 and 2","correct":false},{"id":1161924,"option":"2 and 3","correct":false},{"id":1161925,"option":"1 and 3","correct":true},{"id":1161926,"option":"All of these","correct":false}]},{"q":"Which of the following statements about Azure DevOps Repos are correct:<br>&nbsp;&nbsp;&nbsp;&nbsp;1. They are a set of repositories that allow users to version control and manage the project code.<br>&nbsp;&nbsp;&nbsp;&nbsp;2. They are used to monitor code, solutions, builds, commits, pushes, pull requests, and branching information about projects.<br>&nbsp;&nbsp;&nbsp;&nbsp;3. They cannot control solutions to the project by using their repos links.","a":[{"id":1161915,"option":"1 and 2","correct":true},{"id":1161916,"option":"2 and 3","correct":false},{"id":1161917,"option":"1 and 3","correct":false},{"id":1161918,"option":"All of these","correct":false}]},{"q":"In Azure DevOps, which of the following statements about Azure Boards are correct?","a":[{"id":1161919,"option":"It provides a set of properties such as native support for Scrum and Kanban, customizable dashboards, and integrated reporting.","correct":true},{"id":1161920,"option":"It provides various CI/CD pipeline services for all public repositories.","correct":false},{"id":1161921,"option":"It provides image content for Docker containers.","correct":false},{"id":1161922,"option":"All of these","correct":false}]},{"q":"In Azure DevOps, which of the following properties is used to provide a workspace for planning tests, execution, and analysis of your team project?","a":[{"id":1161903,"option":"Test Suite","correct":false},{"id":1161904,"option":"Test Hub","correct":true},{"id":1161905,"option":"Test Plan","correct":false},{"id":1161906,"option":"Test Center","correct":false}]},{"q":"In Azure DevOps, you are working on continuous testing using Selenium. You have created a unit testing folder to perform UI tests. If you are required to include your Internet Explorer browser driver to execute the tests, then which of the following packages must be included in the NuGet Package Manager workspace?","a":[{"id":1161883,"option":"Selenium.IEDriver.WebDriver","correct":false},{"id":1161884,"option":"Selenium.IE.WebDriver","correct":false},{"id":1161885,"option":"Selenium.WebDriver.IEDriver","correct":true},{"id":1161886,"option":"Selenium.WebDriver.IE","correct":false}]},{"q":"In Azure DevOps, you are implementing configuration management by using the Puppet tool. If you are required to deploy your application with the Puppet tool in Azure, then which of the following sequence of steps are correct:<br>&nbsp;&nbsp;&nbsp;&nbsp;1. Run the Puppet configuration on a node.<br>&nbsp;&nbsp;&nbsp;&nbsp;2. Provision a Puppet master and a node in Azure<br>&nbsp;&nbsp;&nbsp;&nbsp;3. Install a Puppet agent on the node<br>&nbsp;&nbsp;&nbsp;&nbsp;4. Configure the Puppet production environment<br>&nbsp;&nbsp;&nbsp;&nbsp;5. Test the production environment configuration system<br>&nbsp;&nbsp;&nbsp;&nbsp;6. Create a Puppet program","a":[{"id":1161891,"option":"1 -> 2 -> 3 -> 4 -> 5 -> 6","correct":false},{"id":1161892,"option":"2 -> 3 -> 4 -> 5 -> 6 -> 1","correct":true},{"id":1161893,"option":"3 -> 4 -> 5 -> 6 -> 1 -> 2","correct":false},{"id":1161894,"option":"4 -> 5 -> 6 -> 1 -> 2 -> 3","correct":false}]},{"q":"In TFS, which of the following authorization tokens is used to authenticate Azure DevOps?","a":[{"id":1156915,"option":"OAuth","correct":false},{"id":1156916,"option":"Basic Authorization","correct":false},{"id":1156917,"option":"Personal Access Token (PAT)","correct":true},{"id":1156918,"option":"Alternate Password","correct":false}]},{"q":"In TFS, which of the following workflow models is followed by Team Foundation Version Control?","a":[{"id":1156883,"option":"Server","correct":false},{"id":1156884,"option":"Local ","correct":false},{"id":1156885,"option":"Both of these","correct":true},{"id":1156886,"option":"None of these","correct":false}]},{"q":"Which of the following are the advantages of using Team Foundation Server?","a":[{"id":1156875,"option":"It provides a set of collaboration tools that work with your existing IDEs or editors.","correct":false},{"id":1156876,"option":"It provides support to AGILE methodologies.","correct":false},{"id":1156877,"option":"It provides support for multiple languages and IDEs.","correct":false},{"id":1156878,"option":"All of these","correct":true}]},{"q":"<p>Which of the following illustrates the state of the Azure VM after downloading the image from Docker Hub correctly:</p>\n\n<ol>\n\t<li><img alt=\"\" height=\"180\" src=\"https://he-s3.s3.amazonaws.com/media/uploads/318e2e31-580a-4b3c-9447-9d2ef14520ca.jpg\" width=\"256\"></li>\n\t<li><img alt=\"\" height=\"182\" src=\"https://he-s3.s3.amazonaws.com/media/uploads/bb0f0860-eac3-4b34-b5eb-c3bc5693e6fd.jpg\" width=\"247\"></li>\n\t<li><img alt=\"\" height=\"167\" src=\"https://he-s3.s3.amazonaws.com/media/uploads/7bbd524c-7afe-482e-9e3f-da94e695329a.jpg\" width=\"224\"></li>\n\t<li><img alt=\"\" height=\"177\" src=\"https://he-s3.s3.amazonaws.com/media/uploads/bab38c73-c0b3-4d45-bfba-b730b218b925.jpg\" width=\"241\"></li>\n</ol>","a":[{"id":916011,"option":"1","correct":true},{"id":916012,"option":"2","correct":false},{"id":916013,"option":"3","correct":false},{"id":916014,"option":"4","correct":false}]},{"q":"<p>If you are creating an Azure Analysis Service server for deploying a tabular model with the help of SQL Server Data Tools<em> (</em>SSDT), then which of the following <strong>cmdlet </strong>commands will allow you to specify the administrator parameter?</p>","a":[{"id":915999,"option":"New-AzAnalysisServicesServer","correct":true},{"id":916000,"option":"Set-AzAnalysisServicesServer ","correct":false},{"id":916001,"option":"Admin-AzAnalysisServicesServer ","correct":false},{"id":916002,"option":"Configure-AzAnalysisServicesServer ","correct":false}]},{"q":"<p>You are designing an Azure Web App where all users are required to authenticate using the <strong>Active Directory Domain Services</strong> (AD DS) credentials. Which of the following actions enables Single Sign-On to the application for the domain-authenticated users:</p>\n\n<ol>\n\t<li>Use Forms authentication to generate claims</li>\n\t<li>Use Active Directory Federation Services (AD FS) to generate claims</li>\n\t<li>Use the Azure AD Authentication library in the web application </li>\n</ol>","a":[{"id":915995,"option":"1 and 2","correct":false},{"id":915996,"option":"2 and 3","correct":true},{"id":915997,"option":"1 and 3\r\n","correct":false},{"id":915998,"option":"All of these","correct":false}]},{"q":"<p>You have an Azure subscription. You are required to use an Azure Active Directory Domain Services (AD DS) domain with Virtual machines to deploy VM. If you have to join the Virtual Machines to the domains, then which of the following prerequisItes is not required?</p>","a":[{"id":915975,"option":"Ensure that you always use a brand new VNet to host the Azure AD Service.","correct":true},{"id":915976,"option":"Use a dedicated subnet for Azure AD Domain Services.","correct":false},{"id":915977,"option":"Ensure that the subnet you have selected has at least 3 to 5 available address space.","correct":false},{"id":915978,"option":"Do not select the Gateway subnet for deploying Azure AD Domain Services.","correct":false}]},{"q":"<p>You have an existing On-premises Active Directory environment that is synchronized using <strong>DirSync</strong>. If you want to transition the <strong>DirSync </strong>deployment to Azure Active Directory (AD) connect, then which of the following transition paths can be used:</p>\n\n<ol>\n\t<li>Install a new on-premises domain controller</li>\n\t<li>Create a new Azure AD instance</li>\n\t<li>Upgrade the on-premises Active Directory Domain Service (AD DS) forest functional level to Windows Server 2016.</li>\n\t<li>Deploy Azure AD Connect in parallel</li>\n</ol>","a":[{"id":915971,"option":"1\r\n","correct":false},{"id":915972,"option":"2","correct":false},{"id":915973,"option":"3","correct":false},{"id":915974,"option":"4","correct":true}]},{"q":"<p>You have deployed an Azure web app named VotingApp that is available by using HTTP or HTTPS. You want to ensure that a web administrator receives an email notification if the average response time for the app exceeds 50 milliseconds. Which of the following tasks must be performed to ensure the same:</p>\n\n<ol>\n\t<li>Create a rule</li>\n\t<li>Enable Application logging</li>\n\t<li>Create an HTTP monitoring endpoint</li>\n</ol>","a":[{"id":915967,"option":"1","correct":false},{"id":915968,"option":"2","correct":false},{"id":915969,"option":"3","correct":false},{"id":915970,"option":"1 and 3","correct":true}]},{"q":"<p>In Azure, if you require the following features for an application, then which of these features can be used:</p>\n\n<p><strong>Features</strong></p>\n\n<ol>\n\t<li> Ability to monitor page views for the application</li>\n\t<li> Ability for trace log integration</li>\n\t<li> Ability to integrate a monitoring solution in Visual Studio</li>\n</ol>","a":[{"id":915959,"option":"Log Analytics","correct":false},{"id":915960,"option":"Azure Logs","correct":false},{"id":915961,"option":"Azure Advisor","correct":false},{"id":915962,"option":"Application insights","correct":true}]},{"q":"<p>You want to migrate all apps from On-premises to Azure using lift and shift migration. You do not have enough bandwidth to use VPN connectivity. Which of the following options can be used without the need for an identity provider?<br>\n </p>","a":[{"id":915916,"option":"ADFS server\r\n","correct":false},{"id":915917,"option":"Azure AD connect","correct":false},{"id":915918,"option":"Azure AD Domain services","correct":true},{"id":915919,"option":"DC on Azure Vms","correct":false}]},{"q":"<p>If you are moving your company's infrastructure to Azure, then which of the following services can be used to sync users from on-premises to Azure Active Directory (AD)?</p>","a":[{"id":915892,"option":"Azure Active Directory\r\n","correct":false},{"id":915893,"option":"Azure AD Connect","correct":true},{"id":915894,"option":"ADFS","correct":false},{"id":915895,"option":"Site-Site VPN","correct":false}]},{"q":"<p>In Azure Active Directory, which of the following application resources allows you to perform the unattended resource and service-level operations?</p>","a":[{"id":915852,"option":"Service principal","correct":true},{"id":915853,"option":"Cube","correct":false},{"id":915854,"option":"Tabular model","correct":false},{"id":915855,"option":"Dimensions","correct":false}]},{"q":"<p>You are concerned that someone may try to insert malicious statements into your SQL database in Azure. Which of the following types of threat detection events must be enabled in this scenario?</p>","a":[{"id":738903,"option":"Detect breaches","correct":false},{"id":738904,"option":"Identity insertions","correct":false},{"id":738905,"option":"Monitor entries","correct":false},{"id":738906,"option":"SQL injection","correct":true}]},{"q":"<p>In Azure, you are using the <strong>Set-AzureRmVMSourceImage</strong> command with the <strong>-Version</strong> parameter. You are required to ensure that you get the current image. Which of the following values will you use with the <strong>-Version</strong> parameter?<br>\n </p>","a":[{"id":738895,"option":"sync","correct":false},{"id":738896,"option":"today","correct":false},{"id":738897,"option":"update","correct":false},{"id":738898,"option":"latest","correct":true}]},{"q":"<p>Your organization uses on-premises Active Directory Domain Services (AD DS), Azure, and Office 365. Which of the following tools can integrate the identities between technologies in this infrastructure?</p>","a":[{"id":738879,"option":"DirSync","correct":false},{"id":738880,"option":"Azure AD Sync","correct":false},{"id":738881,"option":"Azure AD Connect","correct":true},{"id":738882,"option":"Active Directory Federation Services (AD FS)","correct":false}]},{"q":"<p>If you are required to elucidate how Azure AD is different from on-premises <strong>Active Directory Domain Services</strong> (AD DS), then which of the following information regarding Azure AD you must not use while describing the difference?</p>","a":[{"id":738871,"option":"It has predefined OUs and GPOs","correct":true},{"id":738872,"option":"It uses the REST API over HTTP and HTTPS","correct":false},{"id":738873,"option":"It relies on federation to extend the scope","correct":false},{"id":738874,"option":"It uses HTTP and HTTPS protocols such as SAML","correct":false}]},{"q":"<p>You are required to implement Azure Storage Queues into your web application. This implementation allows you to send messages across workers. Which of the following statements can be used to check whether all of your messages have been successfully added to the queue?</p>","a":[{"id":738779,"option":"_queue.FetchAll();\r\nAssert.AreEqual(count, _queue.ApproximateMessageCount);","correct":false},{"id":738780,"option":"_queue.FetchAttributes();\r\nAssert.AreEqual(count, _queue.MessageCount);","correct":false},{"id":738781,"option":"_queue.FetchAttributes();\r\nAssert.AreEqual(count, _queue.ApproximateMessageCount);","correct":true},{"id":738782,"option":"_queue.FetchAll();\r\nAssert.AreEqual(count, _queue.MessageCount);","correct":false}]},{"q":"<p>You are an Azure administrator who creates Authentication messages for new users. The process involves capturing a voice clip of the user and snipping it cleanly to factor in the authentication while keeping the clip below a certain time limit <span class=\"mathjax-latex\">\\(x\\)</span>.</p>\n\n<p>Assuming that the default configurations for Multifactor Authentication in Windows Azure, determine the appropriate value of <span class=\"mathjax-latex\">\\(x\\)</span>?</p>","a":[{"id":738727,"option":"10 seconds","correct":false},{"id":738728,"option":"20 seconds","correct":true},{"id":738729,"option":"30 seconds","correct":false},{"id":738730,"option":"60 seconds","correct":false}]},{"q":"<p>You have hosted an environment in Azure that consists of Azure Virtual Machines. You are required to guarantee that the Virtual Machine environment has <strong>99.95%</strong> uptime. Also, your task is to ensure that the Virtual Machine does not go offline due to the installation of updates that require a reboot. Which of the following techniques will you use to perform the task?</p>","a":[{"id":738643,"option":"Ensure autoscaling process for the VM","correct":false},{"id":738644,"option":"Place the VM in the availability set ","correct":true},{"id":738645,"option":"Place the VM behind an application load balancer","correct":false},{"id":738646,"option":"Place the VM in different subnets","correct":false}]},{"q":"<p>You are designing an Azure application that processes images. The maximum size of an image is 10 MB.<br>\nThe application includes a web role that allows users to upload images and a worker role with multiple instances that process the images. The web role communicates with the worker role by using an Azure queue service.</p>\n\n<p>Which of the following techniques will you use to store images such that it minimized storage transactions?<br>\n </p>","a":[{"id":738843,"option":"Store images in Azure Blob service. Store references to the images in the queue","correct":true},{"id":738844,"option":"Store images in the queue","correct":false},{"id":738845,"option":"Store images in OneDrive attached to the worker role instances. Store references to the images in the queue","correct":false},{"id":738846,"option":"Store images in local storage on the web role instance. Store references to the images in the queue","correct":false}]},{"q":"<p>In Azure DevOps, you are working on Azure Pipelines. If you are required to run pipeline tests, then which of the following outcomes in this scenario are correct:</p>\n\n<ol>\n\t<li>Successful</li>\n\t<li>Passed</li>\n\t<li>Aborted</li>\n\t<li>Not impacted</li>\n</ol>","a":[{"id":1161911,"option":"1, 2, and 3","correct":false},{"id":1161912,"option":"2, 3, and 4","correct":true},{"id":1161913,"option":"1, 2, and 4","correct":false},{"id":1161914,"option":"1, 3, and 4","correct":false}]},{"q":"<p>In Azure, you are implementing the disaster recovery by using service backup and recover the data in Azure API management. If you are required to utilize the Azure Resource Manager that must be authenticated with Azure Active Directory, then which of the following actions must be performed in this scenario:</p>\n\n<ol>\n\t<li>Add an application to the Azure Active Directory tenant.</li>\n\t<li>Set permissions for the application that you added.</li>\n\t<li>Get the token for authenticating requests to Azure Resource Manager.</li>\n</ol>","a":[{"id":915963,"option":"1 and 2","correct":false},{"id":915964,"option":"2 and 3","correct":false},{"id":915965,"option":"1 and 3","correct":false},{"id":915966,"option":"All of these","correct":true}]},{"q":"<p>In TFS, which of the following the building blocks of the work item tracking system as well as other sub-systems you access through Azure Boards or an on-premises Azure DevOps Server?</p>","a":[{"id":1156895,"option":"TFVC","correct":false},{"id":1156896,"option":"Process template","correct":true},{"id":1156897,"option":"Access key","correct":false},{"id":1156898,"option":"None of these","correct":false}]},{"q":"<p>In TFS, if you are required to customize the process template, then which of the following tools support in performing this action:</p>\n\n<ol>\n\t<li>XML editor</li>\n\t<li>TFS Team Project Manager tool</li>\n\t<li>Process Editor tool</li>\n</ol>","a":[{"id":1156867,"option":"1 and 2","correct":false},{"id":1156868,"option":"2 and 3","correct":false},{"id":1156869,"option":"1 and 3","correct":false},{"id":1156870,"option":"All of these","correct":true}]},{"q":"<p>In Azure, you are working on the storage systems. If you have orphaned the file-snapshots by deleting the backup file without using the <strong>sys.sp_delete_backup</strong> system stored procedure, then which of the following system functions is used to identify these orphaned file-snapshots?</p>","a":[{"id":738899,"option":"sys.delete_db_backup_file_snapshots","correct":false},{"id":738900,"option":"sys.fn_db_backup_file_snapshots","correct":true},{"id":738901,"option":"delete_db_backup_file_snapshots","correct":false},{"id":738902,"option":"None of these","correct":false}]},{"q":"<p>In Azure, you want to secure the access and data in your Azure Logic App.</p>\n\n<p>You limit the access to the triggers that receive inbound calls in your app by generating a Shared Access Signatures (SAS) access such that only authorized clients can call your logic apps.</p>\n\n<p>Which of the following statements about the query parameters in the endpoint’s URL are correct in this scenario:</p>\n\n<ol>\n\t<li>The parameter <strong>sp</strong> specifies the permissions for the permitted HTTP methods to use.</li>\n\t<li>The parameter <strong>sn</strong> specifies the type of permission that a grouped user has on the permitted HTTP methods.</li>\n\t<li>The parameter <strong>sv</strong> specifies the SAS version to be used to generate the signature.</li>\n\t<li>The parameter <strong>sig</strong> specifies the signature to be used for authenticating access to the trigger.</li>\n</ol>\n\n<p> </p>","a":[{"id":1340943,"option":"1, 2, and 3","correct":false},{"id":1340944,"option":"2, 3, and 4","correct":false},{"id":1340945,"option":"1, 2, and 4","correct":false},{"id":1340946,"option":"1, 3, and 4","correct":true}]},{"q":"In MS SQL, you are migrating SQL Server to Azure SQL Database Offline by using Data Migration Service. If you are running multiple named SQL Server instances by using dynamic ports, then which of the following ports must be granted access to the key through the firewall in this scenario?","a":[{"id":1189611,"option":"TCP port 1434","correct":false},{"id":1189612,"option":"UDP port 1434","correct":true},{"id":1189613,"option":"TCP port 1432","correct":false},{"id":1189614,"option":"UDP port 1433","correct":false}]},{"q":"In TFS, which of the following components of Azure DevOps Server is a storage space for repositories that provide unlimited cloud-hosted private Git repositories?","a":[{"id":1156911,"option":"Azure Repos","correct":true},{"id":1156912,"option":"Azure Pipelines","correct":false},{"id":1156913,"option":"Azure Artifacts","correct":false},{"id":1156914,"option":"Azure Test Plans","correct":false}]},{"q":"In TFS, which of the following version control systems is a centralized version control system?","a":[{"id":1156887,"option":"Git","correct":false},{"id":1156888,"option":"TFVC","correct":true},{"id":1156889,"option":"Both of these","correct":false},{"id":1156890,"option":"None of these","correct":false}]},{"q":"In TFS, which of the following components of Azure DevOps Server enables you to create, host, and also share packages among your team?","a":[{"id":1156907,"option":"Azure Boards","correct":false},{"id":1156908,"option":"Azure Repos","correct":false},{"id":1156909,"option":"Azure Test Plans","correct":false},{"id":1156910,"option":"None of these","correct":true}]},{"q":"<p>You are connecting the Azure Analysis Services server to On-Premise data using an On-Premise gateway. Also, you have installed the On-Premise gateway as a local service. Which of the following steps must be performed immediately after this:</p>\n\n<ol>\n\t<li>Register the local service with the Gateway Cloud Service through Azure Service Bus.</li>\n\t<li>Create a gateway resource Gateway Cloud Service</li>\n\t<li>Connect the Azure analysis service to gateway resource</li>\n\t<li>Set up a two-way connection with port 443</li>\n</ol>","a":[{"id":916007,"option":"1","correct":true},{"id":916008,"option":"2","correct":false},{"id":916009,"option":"3","correct":false},{"id":916010,"option":"4","correct":false}]},{"q":"<p>You publish an application named <strong>MyApp</strong> to Azure Active Directory (AD) and grant access to web APIs through <strong>OAuth 2.0</strong>. If MyApp is generating numerous user consent prompts, then which of the following steps can reduce it?</p>","a":[{"id":915987,"option":"Enable Multi-resource refresh tokens","correct":true},{"id":915988,"option":"Enable WS-federation access tokens","correct":false},{"id":915989,"option":"Configure the Open Web Interface for .NET","correct":false},{"id":915990,"option":"Configure SAML 2.0","correct":false}]},{"q":"<p>You have migrated all applications to the Azure cloud. All global users are getting authenticated against Azure AD as per the assigned role-based access control (RBAC) method. If you need to audit admin access for all the users and configure time bound access for admin users, then which of the following components will allow you to achieve the same?</p>\n\n<p> </p>","a":[{"id":915908,"option":"Managed service Identity\r\n","correct":false},{"id":915909,"option":"Privileged Identity management","correct":true},{"id":915910,"option":"Security Center","correct":false},{"id":915911,"option":"Azure AD Identity Protection","correct":false}]},{"q":"<p>In Azure, which of the following security measures must be adopted to secure and protect the container ecosystem:</p>\n\n<ol>\n\t<li>Use vulnerability management as part of your container development life cycle</li>\n\t<li>Scan for vulnerabilities after pushing images to the registry</li>\n\t<li>Ensure that only approved images are used in your environment</li>\n</ol>","a":[{"id":915880,"option":"1","correct":false},{"id":915881,"option":"2","correct":false},{"id":915882,"option":"3","correct":false},{"id":915883,"option":"1 and 3","correct":true}]},{"q":"<p>In Azure Analysis services, if the minimum requirement is of 50 QPUs and 30 GB memory for a big data analysis, then which of the following tiers is the most cost-effective:</p>\n\n<ol>\n\t<li>Developer</li>\n\t<li>Basic </li>\n\t<li>Standard </li>\n\t<li>Advanced</li>\n</ol>","a":[{"id":915840,"option":"1","correct":false},{"id":915841,"option":"2","correct":false},{"id":915842,"option":"3","correct":true},{"id":915843,"option":"4","correct":false}]},{"q":"<p>You have a firewall between your MFA servers and you are required to configure the instances to communicate on a static port that has been opened specifically for this purpose.</p>\n\n<p>Which of the following DWORD registry values will you need to set to the static port that was accessed in order to achieve this objective?</p>","a":[{"id":738927,"option":"RPC_ncan_ip_tcp_port","correct":false},{"id":738928,"option":"Pfsvc_ncan_ip_tcp_port","correct":true},{"id":738929,"option":"Pfsvc_ncan_Static_port","correct":false},{"id":738930,"option":"Pfsvc_ncan_RPC_port","correct":false}]},{"q":"<p>Which of the following features is not available in the basic subscription that is provided by Azure?</p>","a":[{"id":738907,"option":"Self-Service Password Change for cloud users","correct":false},{"id":738908,"option":"Group-based Access management or provisioning","correct":false},{"id":738909,"option":"Application Proxy","correct":false},{"id":738910,"option":"Multi-Factor Authentication","correct":true}]},{"q":"<p>You have a set of database and web servers that are hosted in Azure. You are required to implement availability sets. Which of the following is the best design practice that must be followed while implementation?<br>\n </p>","a":[{"id":738863,"option":"Put both the web and database servers in one availability set","correct":false},{"id":738864,"option":"Place one set of web and database servers in one availability set and another set in a separate availability set","correct":false},{"id":738865,"option":"Place the web servers in one availability set and the database servers in another availability set ","correct":true},{"id":738866,"option":"Place the web servers in one availability set and the database servers can be managed by Azure","correct":false}]},{"q":"<p>You are working on an Azure project that makes use of storage queues to send messages across workers. You are required to make a REST call to an Azure queue from your application program and process it as required.</p>\n\n<p>Which of the following commands can be used to perform this task?</p>","a":[{"id":738855,"option":"HttpWebRequest req = (HttpWebRequest)WebRequest.Create(string.Format(CultureInfo.InvariantCulture,\r\n\"\"https://{0}.queue.core.windows.net/{1}\"\",\r\nStorageAccount, queuename));","correct":true},{"id":738856,"option":"HttpWebRequest req = (HttpWebRequest)WebRequest.Start(string.Format(CultureInfo.InvariantCulture,\r\n\"\"https://{0}.queue.core.windows.net/{1}\"\",\r\nStorageAccount, queuename));","correct":false},{"id":738857,"option":"HttpWebRequest req = new (HttpWebRequest)(string.Format(CultureInfo.InvariantCulture,\r\n\"\"https://{0}.queue.core.windows.net/{1}\"\",\r\nStorageAccount, queuename));","correct":false},{"id":738858,"option":"HttpWebRequest req = (HttpWebRequest)WebRequest.Create(string.Format(CultureInfo.InvariantCulture,\r\n\"\"https://{0}.queue.core.windows.net/{1}\"\",\r\nServiceAccount, queuename));","correct":false}]},{"q":"<p>Which of the following PowerShell scripts can be used to enable the protection in non-encrypted Resource Manager VMs in Azure after defining the backup protection policy?</p>","a":[{"id":738823,"option":"PS C:\\> $pol=Get-AzureRmRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\"\r\nPS C:\\> Enable-AzureRmRecoveryServicesBackupProtection -Policy $pol -Name \"V2VM\" -ResourceGroupName \"RGName1\"","correct":true},{"id":738824,"option":"PS C:\\> $pol=Get-AzureRmRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\"\r\nPS C:\\> Enable-AzureRmRecoveryServicesBackupProtection -Policy $pol -Name \"E2VM\" -ResourceGroupName \"RGName1\"\r\n","correct":false},{"id":738825,"option":"PS C:\\> $pol=Get-AzureRecoveryServicesBackupProtectionPolicy -Name \"NewPolicy\"\r\nPS C:\\> Enable-AzureRecoveryServicesBackupProtection  $pol -Name \"V2VM\" -ResourceGroupName \"RGName1\"","correct":false},{"id":738826,"option":"None of these","correct":false}]},{"q":"<p>You are planning to run an application on Azure virtual machines (VMs). The VMs are backed up using Azure Backup.</p>\n\n<p>The application maintains its state in three binary files that are stored on disk. Any changes in the application state require that all the three files must be updated on the disk. If only one or two of the files are updated on disk, then the whole task that is performed gets lost and the system is in an inconsistent state.</p>\n\n<p>You are required to ensure that when a backup occurs, the application’s data is always in a consistent state. Which of the following techniques will you use to fulfill this requirement?<br>\n </p>","a":[{"id":738807,"option":"Disable caching for the VMs virtual hard disks","correct":false},{"id":738808,"option":"Implement the Volume Shadow Copy Service (VSS) API in the application","correct":true},{"id":738809,"option":"Use Premium Storage for the VMs virtual hard disks","correct":false},{"id":738810,"option":"Store the application files on an Azure File Service network share","correct":false}]},{"q":"<p>You are using Azure PowerShell to create and add entries to an Azure queue. Which of the following statements can be used to get a reference to an existing queue if it exists and create a new storage queue if it doesn't exist?</p>","a":[{"id":738783,"option":"$queue = Get-AzureStorageQueue -name sampleName -Context $storeAuthContext\r\nif(~!$queue){ \r\n    ---------code to create the queue\r\n}\r\n","correct":false},{"id":738784,"option":"$queue = Get-AzureStorageQueue -name sampleName -Context $storeAuthContext\r\nif(-not $queue){ \r\n    ------code to create the queue\r\n}","correct":true},{"id":738785,"option":"$queue = Get-Queue -name sampleName -Context $storeAuthContext\r\nif(-not $queue){ \r\n    ------code to create the queue\r\n}","correct":false},{"id":738786,"option":"None of these","correct":false}]},{"q":"<p>In Azure, which of the following roles will you assign to a user who can exclusively maintain virtual machines?</p>","a":[{"id":738663,"option":"Virtual Machine Contributor ","correct":true},{"id":738664,"option":"Classic Network Contributor","correct":false},{"id":738665,"option":"Contributor","correct":false},{"id":738666,"option":"Administrator","correct":false}]},{"q":"<p>You are creating an Azure database for the MySQL server. You have entered a unique server name in the setting panel Server name. Which of the following statements about the server name is correct:</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>It is used to identify the Azure database for the MySQL server.</li>\n\t<li>It should contain only upper-case letters with a range of 3 to 63 characters.</li>\n\t<li>It should contain only lower-case letters with a range of 3 to 63 characters.</li>\n\t<li>It should contain both the upper-case and the lower-case letters with a range of 6 to 126 characters.</li>\n</ol>","a":[{"id":1152733,"option":"1 and 2","correct":false},{"id":1152734,"option":"1 and 3","correct":true},{"id":1152735,"option":"3 and 4","correct":false},{"id":1152736,"option":"2 and 4","correct":false}]},{"q":"<p>Your company deploys Microsoft SQL Server on an Azure <strong>Standard_DS3</strong> Virtual Machine. Now if you are asked to modify the disk caching policy, then which of the following Azure PowerShell cmdlet will you run?<br>\n </p>","a":[{"id":738639,"option":"Set-AzureRmVmOperatingSystem","correct":false},{"id":738640,"option":"Set-AzureRmVmDataDisk ","correct":true},{"id":738641,"option":"Update-Disk","correct":false},{"id":738642,"option":"Update-AzureDisk","correct":false}]},{"q":"<p>In Azure DevOps, you are running tests on Azure Pipelines for a test runner. If you are required to slice a test suite, then which of the following YAML variables can be used to indicate the total number of slices that are available in the test run?</p>","a":[{"id":1161927,"option":"System.JobPositionInPhase","correct":false},{"id":1161928,"option":"System.TotalJobsInPhase","correct":true},{"id":1161929,"option":"System.TotalSlicesInPhase","correct":false},{"id":1161930,"option":"None of these","correct":false}]},{"q":"<p>In Azure DevOps, you are required to implement continuous monitoring by using the Nagios monitoring tool. </p>\n\n<p>Which of the following commands is used to enable Apache and Nagios services after installing the Nagios’ core and plugins?</p>","a":[{"id":1161931,"option":"chkconfig httpd on && chkconfig nagios on","correct":true},{"id":1161932,"option":"install chkconfig -m httpd on && chkconfig nagios on","correct":false},{"id":1161933,"option":"install chkconfig -a httpd enable-nagios on","correct":false},{"id":1161934,"option":"install chkconfig -t httpd on && chkconfig enable-nagios","correct":false}]},{"q":"<p>In Azure, you are working with the VPN gateway from Infrastructure as a Service (IaaS).</p>\n\n<p>Which of the following statements about <strong>virtual network gateway</strong> are correct:</p>\n\n<ol>\n\t<li>This gateway contains two or more virtual machines (VM) that are deployed to specific subnets called gateway subnets.</li>\n\t<li>These gateway VMs contain routing tables and run specific gateway services.</li>\n\t<li>These gateway VMs are created when the users create the virtual network gateway. Also, these VMs that are the parts of this gateway can directly be configured by the users.</li>\n\t<li>These gateway VMs contain the routing tables and run specific gateway services.</li>\n</ol>","a":[{"id":1341003,"option":"1, 2, and 3","correct":false},{"id":1341004,"option":"2, 3, and 4","correct":false},{"id":1341005,"option":"1, 2, and 4","correct":true},{"id":1341006,"option":"1, 3, and 4","correct":false}]},{"q":"<p>In Azure, which of the following permissions is required for the credentials that are used to connect to the target Azure SQL database instance on the Azure SQL databases?</p>","a":[{"id":1152673,"option":"CONTROL SERVER","correct":false},{"id":1152674,"option":"CONTROL DATABASE","correct":true},{"id":1152675,"option":"FIREWALL RULE","correct":false},{"id":1152676,"option":"None of these","correct":false}]},{"q":"<p>Which of the following categories identifies partially supported or unsupported features that reflect compatibility issues that might block migrating on-premises SQL Server database(s) to Azure SQL Database?</p>","a":[{"id":1152689,"option":"Compatibility issues","correct":true},{"id":1152690,"option":"SQL Server feature parity","correct":false},{"id":1152691,"option":"Both of these","correct":false},{"id":1152692,"option":"None of these","correct":false}]},{"q":"<p>You are enabled to create a managed instance and configure a virtual machine in your Azure SQL Database. Which of the following files is used to restore the new managed instance?</p>","a":[{"id":1152737,"option":".bacpac","correct":false},{"id":1152738,"option":".dacpac","correct":false},{"id":1152739,"option":".bak","correct":true},{"id":1152740,"option":".restre","correct":false}]},{"q":"If your windows firewall is opened to allow Azure Database Migration Service (DMS) to access the source SQL Server, then what is the default port number?","a":[{"id":1152669,"option":"TCP port 9354","correct":false},{"id":1152670,"option":"TCP port 1433","correct":true},{"id":1152671,"option":"TCP port 1453","correct":false},{"id":1152672,"option":"TCP port 1445","correct":false}]},{"q":"<p>In Azure, which of the following statements about performing the scale-out operation is correct:</p>\n\n<ol>\n\t<li>Perform a synchronization before the scale-out operation to avoid redundant hydration of the added replica.</li>\n\t<li>You must first process data on the primary server, then perform synchronization and perform the scale-out operation.</li>\n</ol>\n\n<p> </p>\n\n<p> </p>","a":[{"id":915943,"option":"1","correct":false},{"id":915944,"option":"2","correct":false},{"id":915945,"option":"1 and 2","correct":true},{"id":915946,"option":"None of these","correct":false}]},{"q":"<p>In Azure Analysis Services, if you want to specify an HTTPS endpoint to serve as an alias, then which of the following statements is correct:</p>\n\n<ol>\n\t<li>The endpoint must support HTTPS over port 443.</li>\n\t<li>The relevant port should not be specified in the URL.</li>\n</ol>\n\n<p> </p>\n\n<p> </p>","a":[{"id":915928,"option":"1","correct":false},{"id":915929,"option":"2","correct":false},{"id":915930,"option":"Both of these","correct":true},{"id":915931,"option":"None of these","correct":false}]},{"q":"<p>You have developed a Web and Mobile application on the Azure cloud. The app is accessible by all consumers across the globe. Which of the following options can be used to implement a secure identity solution allowing users to log in to the app from any of the social networking platforms instead of creating a new account?</p>","a":[{"id":915900,"option":"Azure B2B\r\n","correct":false},{"id":915901,"option":"AzureB2C","correct":true},{"id":915902,"option":"Graphapl","correct":false},{"id":915903,"option":"SAMLtoken","correct":false}]},{"q":"<p>In Azure Active Directory, which of the following options will allow you to use the <strong>Single Sign-On </strong>(SSO) mode for a SaaS application:</p>\n\n<ol>\n\t<li>Automatic user provisioning</li>\n\t<li>Federation-based</li>\n\t<li>Active Directory Federation Service (AD FS)</li>\n</ol>","a":[{"id":915828,"option":"1","correct":false},{"id":915829,"option":"2","correct":false},{"id":915830,"option":"3","correct":false},{"id":915831,"option":"1 and 2","correct":true}]},{"q":"<p>You have deployed your application and integrated it with the <strong>Azure Active Directory </strong>service. Which of the following application settings can be used to uniquely identify a web application:</p>\n\n<ol>\n\t<li>Sign-on URL</li>\n\t<li>Reply URL</li>\n\t<li>Application ID URI</li>\n</ol>\n\n<p> </p>","a":[{"id":915824,"option":"3","correct":true},{"id":915825,"option":"2 and 3","correct":false},{"id":915826,"option":"1 and 2","correct":false},{"id":915827,"option":"All of the above","correct":false}]},{"q":"<p>In Azure Active Directory, which of the following types of DNS records can be added to your domain registrar to verify a custom domain:</p>\n\n<ol>\n\t<li>CNAME (Alias)</li>\n\t<li>TXT (Text)</li>\n\t<li>MX (Mail Exchanger)</li>\n</ol>","a":[{"id":915816,"option":"1 and 2","correct":false},{"id":915817,"option":"2 and 3","correct":true},{"id":915818,"option":"3","correct":false},{"id":915819,"option":"All of these","correct":false}]},{"q":"<p>You are planning to create a new Azure SQL Database on an existing SQL Server by using the Azure portal. Which of the following settings you must configure for the new database:</p>\n\n<ol>\n\t<li>Collation</li>\n\t<li>Pricing tier</li>\n\t<li>Resource group</li>\n\t<li>Username and password</li>\n\t<li>Firewall rule for allowing Azure services to access the database<br>\n\t </li>\n</ol>","a":[{"id":738891,"option":"1","correct":false},{"id":738892,"option":"2","correct":false},{"id":738893,"option":"3 and 4","correct":false},{"id":738894,"option":"1 and 2","correct":true}]},{"q":"<p>You are using Windows Azure queues in your web application. Your manager asks you to add a condition such that whenever the current queue length drops below a certain threshold, you are able to add a set of messages into the storage queue.</p>\n\n<p>Which of the following commands can be used to get the current message count in the queue while performing the operation?</p>","a":[{"id":738851,"option":"CloudQueue q = queueClient.GetQueueUrl(QUEUE_NAME);\r\nq.FetchAttributes();\r\nqCnt = q.ApproximateMessageCount;","correct":false},{"id":738852,"option":"CloudQueue q = queueClient.GetQueueReference(QUEUE_NAME);\r\nq.FetchAttributes();\r\nqCnt = q.ApproximateMessageCount;","correct":true},{"id":738853,"option":"CloudQueue q = queueClient.GetQueueUrl(QUEUE_NAME);\r\nq.FetchAttributes();\r\nqCnt = q. RetrieveApproximateMessageCount;","correct":false},{"id":738854,"option":"CloudQueue q = queueClient.GetQueueReference(QUEUE_NAME);\r\nq.FetchAttributes();\r\nqCnt = q. RetrieveApproximateMessageCount;","correct":false}]},{"q":"<p>You are administering an Azure Storage account named <strong>contosostorage</strong>. The account has queue containers with logging enabled. You are required to view all log files that are generated during the month of July 2014. Which of the following URLs will you use to access the list?<br>\n </p>","a":[{"id":738835,"option":"http://contosostorage.queue.core.windows.net/$logs? restype=container&comp:=list&prefix-queue/2014/07","correct":false},{"id":738836,"option":"http://contosostorage.queue.core.windows.net/$files? restype-container&compslist&prefix-queue/2014/07","correct":false},{"id":738837,"option":"http://contosostorage.blob.core windows.net/$files? restype=container&compz=list&prefix-blob/2014/07","correct":false},{"id":738838,"option":"http://contosostorage blob.core.windows.net/$logs? restype=container&comp:=list&prefix-blob/2014/07 ","correct":true}]},{"q":"<p>You are required to provide a cost-effective, low latency solution that allows you to establish a secure connection from on-premises networks to Azure SQL DB. Which of the following services can be used for this purpose?</p>","a":[{"id":738819,"option":"Azure express route ","correct":false},{"id":738820,"option":"VNet Service Endpoints","correct":true},{"id":738821,"option":"Forced tunnelling","correct":false},{"id":738822,"option":"Reflective proxy tunnels","correct":false}]},{"q":"<p>You have created two virtual machines named VM1 and VM2 in Azure. Both virtual machines are instances in a cloud service named <strong>Cloud</strong>.<br>\nYou are required to ensure that any virtual hard disks that the VMs use are not replicated between datacenters.</p>\n\n<p>Which of the following settings you must modify to perform this task?</p>","a":[{"id":738679,"option":"Azure subscription","correct":false},{"id":738680,"option":"Virtual machine","correct":false},{"id":738681,"option":"Cloud services","correct":false},{"id":738682,"option":"Storage account ","correct":true}]},{"q":"<p>In Azure, you have developed a set of PowerShell scripts that will run when you deploy new virtual machines (VMs). You are required to ensure that the scripts are executed on new VMs. Which of the following is an optimal method to achieve this task with the least amount of administrative effort?</p>","a":[{"id":738655,"option":"Create a SetupComplete.cmd batch file to call the scripts after the VM is started","correct":false},{"id":738656,"option":"Create a new virtual hard disk (VHD) that contains the scripts","correct":false},{"id":738657,"option":"Load the scripts to a common file that is accessible to all the VMs","correct":false},{"id":738658,"option":"Set the VM to execute a custom script extension","correct":true}]},{"q":"<p>You are creating a <strong>firewall </strong>blocked alert for your <strong>SQL Server database</strong>. You are using the Azure portal. You are required to specify a time span to monitor the<strong> metric data</strong>. Which of the following parameters will you configure to perform this task?</p>","a":[{"id":738883,"option":"Threshold","correct":false},{"id":738884,"option":"Condition","correct":false},{"id":738885,"option":"Duration","correct":false},{"id":738886,"option":"Period","correct":true}]},{"q":"<p>In Azure DevOps, which of the following statements represents the advantages of using Azure Artifacts:</p>\n\n<ol>\n\t<li>Maven, npm, and NuGet Package feeds can be created and shared from public and private sources.</li>\n\t<li>A fully integrated management can be added to CI/CD pipelines with a single click.</li>\n\t<li>Docker images from the Jenkins server can be imported into the public repositories.</li>\n</ol>","a":[{"id":1161899,"option":"1 and 2","correct":true},{"id":1161900,"option":"2 and 3","correct":false},{"id":1161901,"option":"1 and 3","correct":false},{"id":1161902,"option":"All of these","correct":false}]},{"q":"<p>In Azure, which of the following statements about <strong>connectors</strong> for the Azure Logic Apps service are correct:</p>\n\n<ol>\n\t<li>They provide quick access from Azure Logic Apps to events, data, and actions across other apps, services, systems, protocols, and platforms.</li>\n\t<li>The users can expand the capabilities of their cloud and on-premises apps to perform tasks with their data.</li>\n\t<li>There are three types of connectors namely built-in triggers and actions connectors, managed connectors, and instance connectors.</li>\n</ol>\n\n<p> </p>","a":[{"id":1340907,"option":"1 and 2","correct":true},{"id":1340908,"option":"2 and 3","correct":false},{"id":1340909,"option":"1 and 3","correct":false},{"id":1340910,"option":"All of these","correct":false}]},{"q":"<p>In Azure, if you have a trigger in the data, then the trigger will enforce data integrity in the target ahead of the replicated data from the source. Which of the following SQL queries must be executed to disable triggers in the target database?</p>","a":[{"id":1152697,"option":"SELECT SchemaName, GROUP_CONCAT(DropQuery SEPARATOR ';\\n') as DropQuery, GROUP_CONCAT(AddQuery SEPARATOR ';\\n') as AddQuery","correct":false},{"id":1152698,"option":"SELECT SchemaName as DropQuery Group_CONCAT(AddQuery SEPARATOR ‘;\\n’) as AddQuery","correct":false},{"id":1152699,"option":"SELECT Concat(‘DROP TRIGGER’, Trigger_Name, ‘;’) FROM information_schema.TRIGGERS WHERE TRIGGER_SCHEMA = ‘your_schema’;","correct":true},{"id":1152700,"option":"SELECT Concat(Trigger_Name, ‘;’) FROM information_schema.TRIGGERS WHERE TRIGGER_SCHEMA=’your_schema’;","correct":false}]},{"q":"<p>In Azure, if you are required to perform the migration cutover while migration Oracle database to the Azure database for a PostgreSQL-Single server online, which of the following actions must be performed?</p>\n\n<ol>\n\t<li>    Resume all the incoming transactions that are reaching to the source database.</li>\n\t<li>    Wait until all the pending transactions have been applied to the target database.</li>\n\t<li>    By default database objects in the PostgreSQL database should be case insensitive.</li>\n\t<li>    Reconnect your application to the new Azure target database.</li>\n</ol>","a":[{"id":1152705,"option":"1, 2, and 3","correct":false},{"id":1152706,"option":"2, 3, and 4","correct":true},{"id":1152707,"option":"1, 2, and 4","correct":false},{"id":1152708,"option":"1, 3, and 4","correct":false}]},{"q":"<p>If you are required to migrate your SQL workloads to Azure while maintaining complete SQL Server compatibility and operating system-level access, then which of the following database services is used to perform this action?</p>","a":[{"id":1152729,"option":"Azure Synapse Analytics","correct":false},{"id":1152730,"option":"Azure SQL Database","correct":false},{"id":1152731,"option":"SQL Server on virtual machines","correct":true},{"id":1152732,"option":"Azure Consume DB","correct":false}]},{"q":"<p>In Azure, which of the following statements about the SQL Database service is correct:</p>\n\n<ol>\n\t<li>It enables the user to define and scale performance within two different purchasing models such as the vCore-based purchasing model and the DTU-based purchasing model.</li>\n\t<li>It is a fully managed service that has built-in high availability, backups, and other common maintenance operations.</li>\n</ol>\n\n<p> </p>","a":[{"id":1152725,"option":"1","correct":false},{"id":1152726,"option":"2","correct":false},{"id":1152727,"option":"Both of these","correct":true},{"id":1152728,"option":"None of these","correct":false}]},{"q":"In MS SQL, you are configuring the replication between two managed instances. If you are required to create a publisher database in your Azure storage account, then which of the following must be connected to the Transact-SQL extension to perform this taks?","a":[{"id":1189623,"option":"Managed instance","correct":true},{"id":1189624,"option":"SQL Session ID","correct":false},{"id":1189625,"option":"Azure storage session ID","correct":false},{"id":1189626,"option":"None of these","correct":false}]},{"q":"Which of the following statements are functions of the Team Foundation Version Control?","a":[{"id":1156879,"option":"Checking files in and out","correct":false},{"id":1156880,"option":"Deciding who can access the version control data","correct":false},{"id":1156881,"option":"Branching and merging tasks with different versions of the source code","correct":true},{"id":1156882,"option":"All of these","correct":false}]},{"q":"Which of the following statements about the Azure Database Migration Service(DMS) are correct:<br>&nbsp;&nbsp;&nbsp;&nbsp;It provides customers with the least comprehensive service solution.<br>&nbsp;&nbsp;&nbsp;&nbsp;It uses the Data Migration Assistant (DMA) to generate the assessment reports that provide recommendations to guide through the changes required prior to performing a migration.<br>&nbsp;&nbsp;&nbsp;&nbsp;It is a fully managed service designed to enable seamless migrations from multiple database sources to Azure Data platforms with minimal downtime (online migrations).","a":[{"id":1152685,"option":"1 and 2","correct":false},{"id":1152686,"option":"2 and 3","correct":true},{"id":1152687,"option":"1 and 3","correct":false},{"id":1152688,"option":"All of these","correct":false}]},{"q":"In Azure, if the HackerEarth2020 is the name of the Azure SQL database, then which of the following SQL queries is used to enable Change Data Capture (CDC) on an RDS SQL Server?","a":[{"id":1152693,"option":"exec msdb.dbo.rds enable cdc ‘HackerEarth2020’","correct":false},{"id":1152694,"option":"exec msdb.dbo.rds.db enable cdc ‘HackerEarth2020’","correct":false},{"id":1152695,"option":"exec msdb.dbo.rds cdc enable db ‘HackerEarth2020’","correct":true},{"id":1152696,"option":"exec msdb.dbo.rds db enable cdc ‘HackerEarth2020’","correct":false}]},{"q":"<p>You use Azure AD Connect to synchronize On-premises and Azure identity and also use the Active Directory Federation Services (ADFS) for external users. If you are required to ensure that Azure Connect Health is able to analyze all ADFS audit logs, then which of the following options can be used to fulfill the requirement:</p>\n\n<ol>\n\t<li>On Azure AD Connect Server, enable security auditing</li>\n\t<li>On ADFS Server enable security auditing</li>\n\t<li>On ADFS Server set audit policy to verbose</li>\n</ol>","a":[{"id":915979,"option":"1\r\n","correct":false},{"id":915980,"option":"2","correct":false},{"id":915981,"option":"3","correct":false},{"id":915982,"option":"1 and 3","correct":true}]},{"q":"<p>In Azure, which of the following <strong>server metrics</strong> can be used to count the number of jobs in the queue of the command thread pool?</p>","a":[{"id":915951,"option":"CommandPool","correct":false},{"id":915952,"option":"CommandPoolJobQueueLength","correct":true},{"id":915953,"option":"CommandPoolJob","correct":false},{"id":915954,"option":"CommandPoolJobQueue","correct":false}]},{"q":"<p>In Azure Analysis Services query pool, you are performing the scale-out operations. If you are required to perform a subsequent scale-out operation to increase the number of replicas from 3 to 6, then which of the following statements is correct:</p>\n\n<ol>\n\t<li>Data on the primary server should be processed first</li>\n\t<li>Synchronization should be performed after the scale-out operation </li>\n\t<li>Concurrent synchronization and scale-out operations should not be implemented</li>\n</ol>","a":[{"id":915932,"option":"1","correct":false},{"id":915933,"option":"2","correct":false},{"id":915934,"option":"3","correct":false},{"id":915935,"option":"1 and 3","correct":true}]},{"q":"<p>Which of the following is the correct method to install the docker on an Azure Virtual Machines (VM):</p>\n\n<ol>\n\t<li>Azure Resource Manager (ARM) templates</li>\n\t<li>PowerShell</li>\n\t<li>Azure Portal</li>\n</ol>\n\n<p> </p>","a":[{"id":915884,"option":"1","correct":false},{"id":915885,"option":"2","correct":false},{"id":915886,"option":"3","correct":false},{"id":915887,"option":"All of these","correct":true}]},{"q":"<p>In an Azure App Services hosted application, if both the agent-based monitoring and manual SDK based instrumentation is detected, then which of the following conditions is most likely to occur :</p>\n\n<ol>\n\t<li>Agent-based monitoring is preferred</li>\n\t<li>Manual instrumentation settings are preferred</li>\n\t<li>First detected method is given the preference</li>\n</ol>\n\n<p> </p>","a":[{"id":915868,"option":"1","correct":false},{"id":915869,"option":"2","correct":true},{"id":915870,"option":"3","correct":false},{"id":915871,"option":"None of these","correct":false}]},{"q":"<p>You are using Azure services to back up your data. You are required to apply a filter on the files that need to be backed up onto Azure. Which of the following PowerShell scripts can be used to exclude the files in the following folder from being backed up to Azure:</p>\n\n<p><strong>“C:\\windows”, “C:\\temp”</strong></p>","a":[{"id":738923,"option":"PS C:\\> $exclusions = New-FileSpec  @(\"\"C:\\windows\"\", \"\"C:\\temp\"\") \r\nPS C:\\> Add-FileSpec -Policy $newpolicy -FileSpec $exclusions","correct":false},{"id":738924,"option":"PS C:\\> $exclusions = New-OBFileSpec -FileSpec %%(\"\"C:\\windows\"\", \"\"C:\\temp\"\")%% -Exclude\r\nPS C:\\> Add-OBFileSpec -Policy $newpolicy -FileSpec $Exclude","correct":false},{"id":738925,"option":"PS C:\\> $exclusions = New-OBFileSpec -FileSpec %%(\"\"C:\\windows\"\", \"\"C:\\temp\"\")%% -Explode\r\nPS C:\\> Add-OBFileSpec -Policy $newpolicy -FileSpec $exclusions","correct":false},{"id":738926,"option":"PS C:\\> $exclusions = New-OBFileSpec -FileSpec @(\"\"C:\\windows\"\", \"\"C:\\temp\"\") -Exclude\r\nPS C:\\> Add-OBFileSpec -Policy $newpolicy -FileSpec $exclusions\r\n","correct":true}]},{"q":"<p>Your company has hosted virtual machines on-premise as well as in Azure cloud. The connection from on-premise to Azure cloud is established using the <strong>ExpressRoute </strong>circuit. The company wants to build a failsafe to be able to continue the regular operations in cases when the express route fails.</p>\n\n<p>Which of the following techniques is the most cost-effective backup strategy to perform this operation?</p>","a":[{"id":738915,"option":"Set up a second ExpressRoute connection","correct":false},{"id":738916,"option":"Set up a VPN connection","correct":true},{"id":738917,"option":"Increase the bandwidth of the on-premises connection","correct":false},{"id":738918,"option":"Increase the bandwidth of the ExpressRoute connection","correct":false}]},{"q":"<p>In Azure, which of the following techniques is not valid to demonstrate the use case for Dynamic Management Views?</p>","a":[{"id":738875,"option":"Calculating the database size","correct":false},{"id":738876,"option":"Identifying the most common users","correct":true},{"id":738877,"option":"Monitoring the number of connections","correct":false},{"id":738878,"option":"Identifying queries that are using the most amount of CPU resources","correct":false}]},{"q":"<p>A website displays text, pictures, video files, and audio files. The website processes requests from countries and regions all over the world. Now, you plan to migrate the website to the Azure platform. The requirements of the website are as follows:</p>\n\n<ol>\n\t<li>Encode, store, and stream audio and video at scale</li>\n\t<li>Load-balance communications with the website instance that is closest to the user's location</li>\n\t<li>Deliver content with high-bandwidth and low latency</li>\n</ol>\n\n<p>Which of the following technologies will you use for Load balanced communication?</p>","a":[{"id":738847,"option":"Traffic manager","correct":true},{"id":738848,"option":"MediaServices","correct":false},{"id":738849,"option":"ServiceBus","correct":false},{"id":738850,"option":"Azure Content Delivery Network","correct":false}]},{"q":"<p>Your company currently uses an instance of Dynamics CRM Online. They are required to have a trigger that can update their Azure SQL database whenever a record is inserted in the Dynamics CRM system.</p>\n\n<p>Which of the following services can be used to effectively perform the task?<br>\n </p>","a":[{"id":738831,"option":"Azure Functions","correct":false},{"id":738832,"option":"Azure Web Apps","correct":false},{"id":738833,"option":"Azure Containers","correct":false},{"id":738834,"option":"Azure logic Apps","correct":true}]},{"q":"<p>You are designing an Azure application. The application includes services that are hosted in different geographic locations. These service locations may change over time. You are required to minimize the cost of communication between services and recommend an approach for data transmission between your application and Azure services.</p>\n\n<p>Which of the following solutions will you use to complete the task and it minimizes the required administrative effort?<br>\n </p>","a":[{"id":738795,"option":"Azure Table storage","correct":false},{"id":738796,"option":"Service Bus ","correct":true},{"id":738797,"option":"Service Management API","correct":false},{"id":738798,"option":"Azure queue storage","correct":false}]},{"q":"<p>In Azure functions, you are working with durable functions. You have executed multiple functions in parallel and then waited for all the executed functions to be finished. </p>\n\n<p>Which of the following application patterns of the durable functions is performed in this scenario?</p>","a":[{"id":1340959,"option":"Function chaining application pattern ","correct":false},{"id":1340960,"option":"Fan out/fan in application pattern","correct":true},{"id":1340961,"option":"Monitoring application pattern","correct":false},{"id":1340962,"option":"Async HTTP APIs pattern","correct":false}]},{"q":"<p>In Azure, you have performed some actions on your Logic App. You want to schedule your actions such that you want to wait to run your next action until a specified date and time.</p>\n\n<p>Which of the following actions can be used to perform this task?</p>","a":[{"id":1340919,"option":"Delay action","correct":false},{"id":1340920,"option":"Delay by action\r\n","correct":false},{"id":1340921,"option":"Delay date action","correct":false},{"id":1340922,"option":"None of these","correct":true}]},{"q":"In MS SQL, you are migrating SQL Server to Azure SQL Database Offline by using Data Migration Service. If you have disabled your Windows firewall to allow Azure Database Migration Service to access the source SQL server, then determine the default TCP port number in this scenario?","a":[{"id":1189615,"option":"1431","correct":false},{"id":1189616,"option":"1432","correct":false},{"id":1189617,"option":"1433","correct":true},{"id":1189618,"option":"1434","correct":false}]},{"q":"In MS SQL, you are working on the security features for your SQL Database. You have created a new key in Key Vault. If you are required to delete this key from Key Vault, then which of the following PowerShell parameters are required to restore this key to Key Vault by using the Restore-AzKeyVaultzKey cmdlet command:<br>&nbsp;&nbsp;&nbsp;&nbsp;1. VaultName<br>&nbsp;&nbsp;&nbsp;&nbsp;2. InputFile<br>&nbsp;&nbsp;&nbsp;&nbsp;3. KeyID","a":[{"id":1189607,"option":"1 and 2","correct":false},{"id":1189608,"option":"2 and 3","correct":false},{"id":1189609,"option":"All of these","correct":false},{"id":1189610,"option":"Invalid cmdlet command","correct":true}]},{"q":"Work item types and Test Client OMs are a part of TFS Software Development Kits. Which of the following sets of APIs contains these components?","a":[{"id":1156871,"option":"SOAP","correct":true},{"id":1156872,"option":"RESTful","correct":false},{"id":1156873,"option":"Both of these","correct":false},{"id":1156874,"option":"None of these","correct":false}]},{"q":"In Azure, if you are required to cancel or delete any Database Migration Service(DMS) while migrating the PostgreSQL online database to the Azure Database, which of the following sequences of steps is correct:<br>&nbsp;&nbsp;&nbsp;&nbsp;Delete DMS service<br>&nbsp;&nbsp;&nbsp;&nbsp;Delete the task<br>&nbsp;&nbsp;&nbsp;&nbsp;Delete the project<br>&nbsp;&nbsp;&nbsp;&nbsp;Cancel any running task","a":[{"id":1152701,"option":"2 -> 1 -> 4 -> 3","correct":false},{"id":1152702,"option":"3 -> 4 -> 2 -> 1","correct":false},{"id":1152703,"option":"1 -> 3 -> 2 -> 4","correct":false},{"id":1152704,"option":"4 -> 2 -> 3 -> 1","correct":true}]},{"q":"<p>While configuring the Azure Analysis server, which of the following ways to automate alias to server name for high availability is correct:</p>\n\n<ol>\n\t<li>By coding an endpoint health check on the primary server</li>\n\t<li>By setting up alternate paths for alias connection</li>\n\t<li>By running redundant server sync with the primary server at continuous intervals</li>\n</ol>\n\n<p> </p>","a":[{"id":916003,"option":"1","correct":true},{"id":916004,"option":"2","correct":false},{"id":916005,"option":"3","correct":false},{"id":916006,"option":"None of these","correct":false}]},{"q":"<p>You have an Azure subscription and you create an Azure Active Directory (AD) tenant named <strong>Tenant</strong>. You want to integrate Tenant and the On-premise Active Directory. You create a user account that can be used to synchronize changes from the directory. If the solution uses the principle of least privilege<strong>,</strong> then which of the following organizational roles can you assign to the user account?</p>","a":[{"id":915983,"option":"Service administrator","correct":true},{"id":915984,"option":"Global administrator","correct":false},{"id":915985,"option":"Password administrator","correct":false},{"id":915986,"option":"User administrator","correct":false}]},{"q":"<p>You are designing a web application which has the following key requirements<br>\nIn Azure, if you are designing a web application with the following key requirements, then which of these features support them?</p>\n\n<p><strong>Key requirements</strong></p>\n\n<ol>\n\t<li>Near real-time data exploration</li>\n\t<li>Anomaly detection</li>\n</ol>","a":[{"id":915955,"option":"Azure Data Lake\r\n","correct":false},{"id":915956,"option":"Azure TimeSeries Insights","correct":true},{"id":915957,"option":"Azure Service Bus","correct":false},{"id":915958,"option":"Application Insights","correct":false}]},{"q":"<p>You want to set up diagnostics logging for the Azure Analysis Service server. If you are required to log all extended events (xEvents), then which of the following options must be enabled?</p>","a":[{"id":915936,"option":"Service","correct":false},{"id":915937,"option":"Engine","correct":true},{"id":915938,"option":"Metrics","correct":false},{"id":915939,"option":"Archive","correct":false}]},{"q":"<p>In Azure Analysis Services, which of the following protocol formats can be used at the endpoints to connect with the server from a client application using an alias?</p>","a":[{"id":915924,"option":"link:// protocol format","correct":true},{"id":915925,"option":"ssh:// and hash key protocol format","correct":false},{"id":915926,"option":"path:// and route key format","correct":false},{"id":915927,"option":"Id and public key format","correct":false}]},{"q":"<p>In Azure, you have a solution that uses <strong>multi-factor authentication</strong> (MFA) for users outside the office. The billing model is set to the 'pre-authentication' method. Now, you have acquired new staffs and you have to add them to the Azure Active Directory (AD). Which of the following options allows you to change the billing model from pre-authentication to the per-user method:</p>\n\n<ol>\n\t<li>Use Azure CLI to change the current billing model.</li>\n\t<li>Use the Azure portal to change the current billing model.</li>\n\t<li>Create a new multi-factor authentication resource and reconfigure the billing model. </li>\n\t<li>Create a new multi-factor authentication resource with a backup from the current multi-factor authentication resource data.</li>\n</ol>\n\n<p> </p>","a":[{"id":915920,"option":"1\r\n","correct":false},{"id":915921,"option":"2","correct":false},{"id":915922,"option":"3","correct":true},{"id":915923,"option":"4","correct":false}]},{"q":"<p>You have hosted multiple Virtual Machines and other website workloads in an Azure environment. If you are required to assign administrative roles to the group of users called ADMIN, then which of the following permissions is most appropriate for this group? </p>","a":[{"id":915904,"option":"Reader","correct":false},{"id":915905,"option":"Contributor","correct":false},{"id":915906,"option":"Co-administrator","correct":false},{"id":915907,"option":"Owner","correct":true}]},{"q":"<p>If your company has the following requirements to use Azure<strong> </strong>Active Directory (AD), then which of these cost-effective editions of the directory is recommended:</p>\n\n<p><strong>Requirements</strong></p>\n\n<ol>\n\t<li>SSO for users</li>\n\t<li>Standard Security Reports</li>\n\t<li>Self Service password reset</li>\n</ol>","a":[{"id":915896,"option":"Azure AD Basic\r\n\r\n","correct":true},{"id":915897,"option":"Azure AD Free\r\n","correct":false},{"id":915898,"option":"Azure AD Premium(P1)","correct":false},{"id":915899,"option":"Azure AD Premium(P2)","correct":false}]},{"q":"<p>In Azure, which of the following options can be used to manage a <strong>Kubernetes</strong> cluster in a container service?</p>\n\n<p> </p>","a":[{"id":915876,"option":"Azure CLI","correct":false},{"id":915877,"option":"Kubectl Tool","correct":true},{"id":915878,"option":"Azure PowerShell","correct":false},{"id":915879,"option":"Azure Container Registry","correct":false}]},{"q":"<p>If you have applied the Application Insight service to your .NET web application by using the runtime route, then which of the following features cannot be enabled: </p>\n\n<ol>\n\t<li>Dependency diagnostics</li>\n\t<li>API for custom telemetry</li>\n\t<li>Page view and user data</li>\n</ol>","a":[{"id":915864,"option":"1 and 2","correct":false},{"id":915865,"option":"2 and 3","correct":true},{"id":915866,"option":"1 and 3","correct":false},{"id":915867,"option":"All of these","correct":false}]},{"q":"<p>The Azure Application Insights SDK is sending telemetry data about your application to the cloud. What is the default duration in which data can be stored in the cloud?</p>","a":[{"id":915860,"option":"30 days","correct":false},{"id":915861,"option":"45 days","correct":false},{"id":915862,"option":"60 days ","correct":false},{"id":915863,"option":"90 days","correct":true}]},{"q":"<p>In Azure, which of the following features can be monitored by the <strong>Application Monitor Insights</strong> that helps you to understand the performance and usage of an application:</p>\n\n<ol>\n\t<li>AJAX calls</li>\n\t<li>User and session counts</li>\n\t<li>Request rates, response times, and failure rates</li>\n</ol>","a":[{"id":915856,"option":"1 and 2","correct":false},{"id":915857,"option":"2 and 3","correct":false},{"id":915858,"option":"1 and 3","correct":false},{"id":915859,"option":"All of these","correct":true}]},{"q":"<p>In Azure, which of the following statements about <strong>Query replicas</strong> is correct:</p>\n\n<ol>\n\t<li>Query replicas can be spread outside your server's region</li>\n\t<li>Query replicas are billed at the same rate as your server</li>\n</ol>\n\n<p> </p>","a":[{"id":915844,"option":"1","correct":false},{"id":915845,"option":"2","correct":true},{"id":915846,"option":"Both of these","correct":false},{"id":915847,"option":"None of these","correct":false}]},{"q":"<p>In Azure Analysis services, if you want to process a <strong>Tabular Model Scripting Language</strong> (TMSL) script to process a tabular model, then which of the following considerations is correct:</p>\n\n<ol>\n\t<li>Impact analysis cannot be performed on tabular models</li>\n\t<li>Process Defrag is not exposed in Tabular mode</li>\n\t<li>Process Index option should always be used in tabular mode</li>\n</ol>","a":[{"id":915836,"option":"1","correct":false},{"id":915837,"option":"2","correct":false},{"id":915838,"option":"3","correct":false},{"id":915839,"option":"1 and 2","correct":true}]},{"q":"<p>In Azure Active Directory, which of the following options is not a supported endpoint and cannot be utilized by a user:</p>\n\n<ol>\n\t<li>WS-Federation</li>\n\t<li>SAML-P</li>\n\t<li>OAuth 2</li>\n\t<li>Federation metadata document</li>\n</ol>","a":[{"id":915832,"option":"1","correct":false},{"id":915833,"option":"2","correct":false},{"id":915834,"option":"3","correct":false},{"id":915835,"option":"4","correct":true}]},{"q":"You are required to synchronise a model and use return status codes. What should be the code assigned for 'failed' ? ","a":[{"id":915940,"option":"1","correct":false},{"id":915941,"option":"2","correct":false},{"id":915942,"option":"3","correct":true}]},{"q":"<p>You are using Windows backup services to back up your data to the Azure system. If you have misplaced the encryption key that is used to encrypt the backup data, then which of the following techniques will you use to recover your data?</p>","a":[{"id":738919,"option":"Windows cannot recover the backup data since the key is present only in the customer premises  ","correct":true},{"id":738920,"option":"Windows can recover the backup data since the key is present in Azure","correct":false},{"id":738921,"option":"You cannot recover the backup data since the key present in Azure mismatches with your key  ","correct":false},{"id":738922,"option":"You can recover the backup data since the key is present in Azure","correct":false}]},{"q":"<p>You are working as an Azure administrator in your organization. You are required to monitor a virtual network named VNet1 and it consists of a subnet called Subnet1. You notice a high volume of traffic amongst the VMs present in Subnet1 during the process.</p>\n\n<p>Which of the following is the most suitable approach to isolate the traffic with the least amount of time and impact on the users?<br>\n </p>","a":[{"id":738911,"option":"Create a new subnet and move the VM to a new subnet","correct":false},{"id":738912,"option":"Create a new VNet and move the VM to a new network","correct":true},{"id":738913,"option":"Create site-to-site virtual network to DC","correct":false},{"id":738914,"option":"Create an availability set for the existing VMs","correct":false}]},{"q":"<p>Which of the following can be used to identify I/O issues in an Azure SQL database?</p>\n\n<ol>\n\t<li>Availability of IO in the page latch wait name</li>\n\t<li>Availability of IO in the transaction log</li>\n</ol>","a":[{"id":738859,"option":"1","correct":true},{"id":738860,"option":"2","correct":false},{"id":738861,"option":"Both of these","correct":false},{"id":738862,"option":"None of these","correct":false}]},{"q":"<p>Which of the following statements about the Azure Backup Services is correct:</p>\n\n<ol>\n\t<li>A user schedules a backup at 6 PM and specifies retention policies at a different time.</li>\n\t<li>If a backup is retained for a long duration, then it takes more time to recover than a new data point<br>\n\t </li>\n</ol>","a":[{"id":738827,"option":"1","correct":false},{"id":738828,"option":"2","correct":false},{"id":738829,"option":"Both of these","correct":false},{"id":738830,"option":"None of these","correct":true}]},{"q":"<p>You manage an application that is deployed to virtual machines (VMs) on an Azure virtual network named <strong>corpVnet1</strong>. You plan to hire several remote employees who can require access to the application on <strong>corpVnet1</strong>.</p>\n\n<p>You are required to ensure that new employees can access <strong>corpVnet1</strong>. If you want to achieve this goal by using the most cost-effective solution, then which of the following actions you must perform:</p>\n\n<ol>\n\t<li>Create a VPN subnet.</li>\n\t<li>Enable point-to-site connectivity for corpVnet1</li>\n\t<li>Create a gateway subnet<br>\n\t </li>\n</ol>","a":[{"id":738811,"option":"1 and 2","correct":false},{"id":738812,"option":"1 and 3","correct":false},{"id":738813,"option":"2 and 3","correct":true},{"id":738814,"option":"All of these","correct":false}]},{"q":"<p>Your organization is currently using a pair of hardware load balancers behind a single published endpoint to load balance traffic. The customer data is hosted on a SQL Server 2012 database. The customer user accounts are stored in an AD DS instance.</p>\n\n<p>Which of the following data encryption protocol will you use for SQL data security at the Presentation layer?</p>","a":[{"id":738791,"option":"SQL transparent data encryption","correct":false},{"id":738792,"option":"SQL always encrypted","correct":true},{"id":738793,"option":"SQL dynamic data masking","correct":false},{"id":738794,"option":"Row level security","correct":false}]},{"q":"<p>You are developing a hybrid solution for a video editing company. Videos are currently edited on-premises and stored in the <strong>Server Message Block</strong> (SMB) protocol shares. However, due to legal regulations, all videos must be stored on-premises in the near future.<br>\nYou have decided to distribute videos by using Azure media services in the given context. Now, you are required to recommend a storage solution for the videos.</p>\n\n<p>Which of the following is an appropriate storage solution that can be recommended here?<br>\n </p>","a":[{"id":738787,"option":"Azure Blob storage","correct":false},{"id":738788,"option":"Azure Table Storage","correct":false},{"id":738789,"option":"Azure Cosmos DB","correct":false},{"id":738790,"option":"Azure StorSimple","correct":true}]},{"q":"<p>You are the administrator of an Azure subscription for your company.</p>\n\n<p>The Management asks you to configure Azure permissions for a user who is your Azure Active Directory (Azure AD). The user must be able to perform all actions on virtual machines (VMs). However, the user must not be allowed to create and manage availability sets for the VMs.</p>\n\n<p>You are required to implement the required permissions with the least administrative effort. Which of the following techniques will you use to assign the required permissions to the users?</p>","a":[{"id":738667,"option":"Use Windows PowerShell to assign the Classic Virtual Machine Contributor role to the user","correct":false},{"id":738668,"option":"Use Windows PowerShell to create a custom role from the Virtual Machine Contributor role and then use NotActions to customize the role permissions","correct":true},{"id":738669,"option":"Implement a custom role through the Azure Portal and customize the role by adding the appropriate permissions","correct":false},{"id":738670,"option":"Assign the Virtual Machine Contributor role to the user","correct":false}]},{"q":"<p>You have planned to deploy an application that requires four Azure Virtual Machines (VMs). If all the VMs use <strong>Azure Resource Management</strong> mode, then which of the following techniques will you use to minimize the time taken by the VMs to communicate with each other?</p>","a":[{"id":738659,"option":"Create a multi-site virtual network","correct":false},{"id":738660,"option":"Create a regional virtual network","correct":true},{"id":738661,"option":"Create a site-to-site virtual network","correct":false},{"id":738662,"option":"Add the VMs to the same affinity group","correct":false}]},{"q":"<p>The CosmosClient initialization is used to provide the client-side logical representation for an Azure cosmos database service. If the following Java code is used to configure and execute requests against the service, then which of these Java commands is used to create a CosmosDatabase method:<br>\n    </p>\n\n<pre class=\"prettyprint\"><code>    client = new CosmosClientBuilder()\n                    .setEndpoint(AccountSettings.HOST)\n                    .setKey(AccountSettings.MASTER_KEY)\n                    .setConnectionPolicy(defaultPolicy)\n                    .setConsistencyLevel(ConsistencyLevel.EVENTUAL)\n                    .buildClient();\n    </code></pre>","a":[{"id":1152741,"option":"database = client.create.CosmosDatabaseIfNotExists(databaseName).getDatabase();","correct":false},{"id":1152742,"option":"database = client.createDatabaseIfNotExists(databaseName).getDatabase(); ","correct":true},{"id":1152743,"option":"database = client.createDatabaseIfNotExits(databaseName).setDatabase();","correct":false},{"id":1152744,"option":"database = client.create.CosmosDatabaseIfNotExists(databaseName).setDatabase();","correct":false}]},{"q":"<p>In Azure, you are configuring a scale-out on a server for the first time. The models on your primary server are automatically synchronized with replicas in the query pool. Determine the number of times the automatic synchronization can occur.</p>","a":[{"id":915947,"option":"Once ","correct":true},{"id":915948,"option":"Twice","correct":false},{"id":915949,"option":"Thrice","correct":false},{"id":915950,"option":"No limit","correct":false}]},{"q":"<p>You are developing an application hosted on Azure. You learned that the credentials required to authenticate against Azure AD are saved in the application code. Which of the following options will allow you to overcome this security risk?</p>","a":[{"id":915912,"option":"Azure AD Identity Protection\r\n","correct":false},{"id":915913,"option":"Azure Key vault","correct":false},{"id":915914,"option":"Secure Token","correct":false},{"id":915915,"option":"Managed Service identity","correct":true}]},{"q":"<p>In Azure, which of the following application setting definitions ensures that the essential features are enabled for optimal performance?</p>","a":[{"id":915872,"option":"ApplicationInsightsAgent_EXTENSION_VERSION","correct":false},{"id":915873,"option":"XDT_MicrosoftApplicationInsights_BaseExtensions","correct":false},{"id":915874,"option":"XDT_MicrosoftApplicationInsights_Mode","correct":false},{"id":915875,"option":"XDT_MicrosoftApplicationInsights_Mode","correct":true}]},{"q":"<p>Your organisation uses Azure websites for public-facing customer websites. The company has a mobile app that requires customer sign in by using your company's customer account. Customers must be able to sign on to the websites and mobile app by using a Microsoft, Facebook, or Google account. All transactions must be secured in-transit regardless of device.<br>\nYou need to configure the websites and mobile app to work with external identity providers. Which of the following won't be one of the actions that should be performed in the given context?</p>\n\n<p> </p>\n\n<p>A mobile app requires customer sign in by using your company's customer account that uses Azure for hosting public-facing customer websites. Customers must be able to sign on to the websites and mobile app by using a Microsoft, Facebook, or Google account and all transactions must be secured in-transit regardless of the device. If you have to configure the websites and mobile app to work with external identity providers, then which of the following actions must be performed:</p>\n\n<p> </p>","a":[{"id":915991,"option":" Request a certificate from a domain registrar for the website URL, and enable TLS/SSL.","correct":false},{"id":915992,"option":"Configure IPsec for the websites and the mobile app.","correct":true},{"id":915993,"option":"Configure OAuth2 to connect to an external authentication provider.","correct":false},{"id":915994,"option":"Build an app by using MVC 5 that is hosted in Azure to provide a framework for the underlying authentication.","correct":false}]},{"q":"<p>You want to use the Azure Analysis Services with the help of a relevant cmdlet to execute a TMSL script.<br>\nIn the given context, select the appropriate cmdlet to be used for this purpose?</p>\n\n<p> </p>\n\n<p>Which of the following <strong>cmdlet </strong>commands will allow you to use the Azure Analysis Services to execute a TMSL script?</p>\n\n<p> </p>","a":[{"id":915848,"option":"Invoke-ASCmd","correct":true},{"id":915849,"option":"Invoke-DSCmd","correct":false},{"id":915850,"option":"Invoke-TMSL","correct":false},{"id":915851,"option":"Invoke-SQLTM","correct":false}]},{"q":"<p>You are required to configure the Multifactor Authentication in such a way that you receive an OTP on your registered mobile number from the MFA server every time you log in. Assume that a load balancer is being used to manage the load on incoming traffic for the Azure server. In order to achieve this functionality, which of the following types of sessions will you must configure and set up?</p>","a":[{"id":738839,"option":"Encrypted directory session","correct":false},{"id":738840,"option":"Sticky sessions","correct":true},{"id":738841,"option":"Active directory session","correct":false},{"id":738842,"option":"Persistent sessions","correct":false}]},{"q":"<p>You are required to verify a custom domain for an Azure Active Directory. Which of the following types of DNS records will you add to your domain registrar to complete this task:</p>\n\n<ol>\n\t<li>CNAME (Alias)</li>\n\t<li>TXT (Text)</li>\n\t<li>MX (Mail Exchanger)</li>\n</ol>","a":[{"id":738735,"option":"1 and 2","correct":false},{"id":738736,"option":"2 and 3","correct":true},{"id":738737,"option":"3","correct":false},{"id":738738,"option":"All of these","correct":false}]},{"q":"<p>In Azure, you are analyzing an MFA server that contains some crucial data. You are required to back up the data within the MFA server such that the data is not lost in the process. If you are able to do so by copying the data within a particular file <span class=\"mathjax-latex\">\\(x\\)</span>, then which of the following correctly represents <span class=\"mathjax-latex\">\\(x\\)</span>?</p>\n\n<p> </p>","a":[{"id":738731,"option":"Phonefactor.pfdata","correct":true},{"id":738732,"option":"Factor.pfdata","correct":false},{"id":738733,"option":"Internal.pfdata","correct":false},{"id":738734,"option":"Storage.pfdata","correct":false}]},{"q":"<p>You manage a cloud service that supports features hosted by two instances of an Azure virtual machine (VM). You discover that<br>\noccasional outages cause your service to fail. You are required to minimize the impact of outages on your cloud service.</p>\n\n<p>Which of the following techniques will you use to perform this task:</p>\n\n<ol>\n\t<li>Configure Load Balancing on the VMs</li>\n\t<li>Redeploy the VMs to belong to an Affinity Group</li>\n\t<li>Configure the VMs to belong to an availability set</li>\n</ol>","a":[{"id":738723,"option":"1 and 2","correct":false},{"id":738724,"option":"2 and 3","correct":false},{"id":738725,"option":"1 and 3","correct":false},{"id":738726,"option":"None of these","correct":true}]},{"q":"<p>In Azure, which of the following services must be implemented to configure the availability of Virtual Machines that are migrating to Azure?</p>","a":[{"id":738647,"option":"Update domain","correct":true},{"id":738648,"option":"Fault domain","correct":false},{"id":738649,"option":"Cloud services","correct":false},{"id":738650,"option":"Traffic manager","correct":false}]}]