[{"q":"<p>In Kafka, you have created the following topic and it has one partition.&nbsp;Which of these&nbsp;statements about this scenario are correct?</p>\n\n<p><strong>Topic:</strong></p>\n\n<pre class=\"prettyprint\"><code>&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-sample-topic\nTopic:my-sample-topic        PartitionCount:1        ReplicationFactor:3        Configs:\nTopic: my-sample-topic        Partition: 0      Leader: 1      Replicas: 1,2,0    Isr:1,2,0</code></pre>\n\n<p><strong>Statements:</strong></p>\n\n<ol>\n\t<li>Leader&nbsp;is the node that is responsible for all the reads and writes for the given partition, where each node is the leader for a randomly selected portion of the partitions.</li>\n\t<li>Replicas&nbsp;is the list of nodes that replicate the log for this partition irrespective of whether they are the leader or they are currently active.&nbsp;</li>\n\t<li>Isr&nbsp;is the set of in-sync&nbsp;replicas that represents a set of the replicas that are currently inactive.</li>\n</ol>","a":[{"id":1166763,"option":"1 and 2","correct":true},{"id":1166764,"option":"2 and 3","correct":false},{"id":1166765,"option":"1 and 3","correct":false},{"id":1166766,"option":"All of these","correct":false}]},{"q":"<p>You are using Partitioning in Redis&nbsp;for&nbsp;larger databases which allows using the sum of the memory of many computers to avoid the problem of less memory a single computer can support. Which of the following is a type of implementation of partitioning?</p>","a":[{"id":1733714,"option":"Hash Partitioning","correct":false},{"id":1733715,"option":"Range Partitioning","correct":false},{"id":1733716,"option":"Server-side Partitioning","correct":false},{"id":1733717,"option":"Client-side Partitioning","correct":true}]},{"q":"<p>You are using Partitioning in Redis&nbsp;for&nbsp;larger databases that allow using the sum of the memory of many computers to avoid the problem of less memory a single computer can support. Suppose you want to separate the customer, employees, and vendors into different nodes. Given below are the steps that you can follow in order to do so. Arrange them&nbsp;in the correct order.</p>\n\n<p><strong>Steps</strong></p>\n\n<ol>\n\t<li>Assign cluster slots to specific nodes in the cluster.</li>\n\t<li>Set up a Redis Cluster.</li>\n\t<li>Write data for each customer, where the hset function sets the key using the hash slot function.</li>\n</ol>","a":[{"id":1733710,"option":"1, 2, 3","correct":false},{"id":1733711,"option":"2, 1, 3","correct":true},{"id":1733712,"option":"3, 1, 2","correct":false},{"id":1733713,"option":"3, 2, 1","correct":false}]},{"q":"<p>You are using Partitioning in Redis&nbsp;for&nbsp;larger databases which allows using the sum of the memory of many computers to avoid the problem of less memory a single computer can support. You have created a function that is used to convert the key into a number and then the data is stored in different- different Redis instances. Which of the following type of partitioning could you use to do so?</p>","a":[{"id":1733365,"option":"Range","correct":false},{"id":1733366,"option":"Hash","correct":true},{"id":1733367,"option":"List","correct":false},{"id":1733368,"option":"Key","correct":false}]},{"q":"<p>You are using Partitioning in Redis&nbsp;for&nbsp;larger databases which allows using the sum of the memory of many computers to avoid the problem of less memory a single computer can support. Which among the following is not true for Redis&nbsp;Partitioning?</p>\n\n<p><strong>Options</strong></p>\n\n<ol>\n\t<li>You can't perform the intersection between two sets if they are stored in keys that are mapped to different Redis instances</li>\n\t<li>Redis transactions involving multiple keys can not be used.</li>\n\t<li>Operations involving multiple keys are usually not supported.&nbsp;</li>\n\t<li>Redis Cluster supports mostly transparent rebalancing of data with the ability to add and remove nodes at runtime, but other systems like client-side partitioning and proxies support this feature.</li>\n</ol>","a":[{"id":1727150,"option":"1","correct":false},{"id":1727151,"option":"2","correct":false},{"id":1727152,"option":"3","correct":false},{"id":1727153,"option":"4","correct":true}]},{"q":"<p>In Spark Streaming integrated with Kafka, if you increase the number of topic-specific partitions in <strong>KafkaUtils.createStream()</strong>, then<strong> </strong>which of the following occurs:</p>\n\n<ol>\n\t<li>The number of threads using which the topics are consumed within multiple receivers increases.</li>\n\t<li>The number of threads using which the topics are consumed within a single receiver increases.</li>\n\t<li>The parallelism property of Spark does not increase while processing data.</li>\n\t<li>The parallelism property of Spark decreases significantly while processing data.</li>\n</ol>","a":[{"id":1166929,"option":"1 and 2","correct":false},{"id":1166930,"option":"2 and 3","correct":true},{"id":1166931,"option":"3 and 4","correct":false},{"id":1166932,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, what does the following code allow you to do:</p>\n\n<pre class=\"prettyprint\"><code>Ranges = []\n\n def storeRanges(rdd):\n     global Ranges\n     Ranges = rdd.Ranges()\n     return rdd\n\n def printRanges(rdd):\n     for o in Ranges:\n         print \"%s %s %s %s\" % (o.topic, o.partition, o.fromOffset, o.untilOffset)\n\n directKafkaStream \\\n     .transform(storeRanges) \\\n     .foreachRDD(printRanges)</code></pre>\n\n<p><strong>Tasks</strong></p>\n\n<ol>\n\t<li>Access the Kafka offsets consumed in each batch.</li>\n\t<li>If you want Zookeeper-based Kafka monitoring tools to show the progress of the streaming application then it updates Zookeeper.</li>\n\t<li>Create multiple-input Kafka streams and merge them.</li>\n\t<li>Increase the number of topic-specific partitions.</li>\n</ol>","a":[{"id":1167960,"option":"1 and 2","correct":true},{"id":1167961,"option":"2 and 3","correct":false},{"id":1167962,"option":"3 and 4","correct":false},{"id":1167963,"option":"1 and 4","correct":false}]},{"q":"<p>In Spark streaming integrated with Kafka, which of the following artifacts is used to change this setting to handle more than (64 * number of executors) Kafka partitions?</p>","a":[{"id":1166925,"option":"spark.streaming.kafka.consumer.cache.maxCapacity","correct":true},{"id":1166926,"option":"spark-streaming-kafka-0-10_2.11","correct":false},{"id":1166927,"option":"LocationStrategies.PreferConsistent","correct":false},{"id":1166928,"option":"spark.streaming.kafka.consumer.cache.enabled","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, your application uses the consumer group ID <strong>terran </strong>to read from the <strong>zerg.hydra </strong>Kafka topic, which contains 10 partitions. You have configured your application such that it can consume the topic with only one thread. However, during runtime, you increase the number of threads from 1 to 14.</p>\n\n<p>Which of the following statements about the consumption of the threads in this scenario is correct:</p>\n\n<ol>\n\t<li>All the 14 threads are consumed from a single partition—each consumed in two rebalancing events.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads stay idle.</li>\n\t<li>Once the rebalancing is complete, 10 of the 14 threads are consumed—each from a single partition—and the remaining 4 threads are consumed in the next rebalancing event.</li>\n\t<li>Once the rebalancing is complete, 7 of the 14 threads are consumed—each from a single partition—and the remaining 7 threads stay idle.</li>\n</ol>","a":[{"id":1167798,"option":"1","correct":false},{"id":1167799,"option":"2","correct":true},{"id":1167800,"option":"3","correct":false},{"id":1167801,"option":"4","correct":false}]},{"q":"<p>In Spark Streaming integrated with Kafka, RDD actions inside the DStream output operations force the processing of the data that is received. If you used the <strong>dstream.foreachRDD()</strong> artifact without any RDD actions inside them, then which of the following statements about the given scenario is correct:</p>\n\n<ol>\n\t<li>The system receives the data and discards it, and therefore, there is no execution.</li>\n\t<li>The system does not receive any data.</li>\n\t<li>The system receives and processes the data and the execution runs successfully.</li>\n\t<li>The system receives no data, and therefore, there is no execution.</li>\n</ol>","a":[{"id":1167810,"option":"1","correct":true},{"id":1167811,"option":"2","correct":false},{"id":1167812,"option":"3","correct":false},{"id":1167813,"option":"4","correct":false}]},{"q":"<p>In Informatica, the PowerCenter Integration Services creates a default partition type at each partition point. In your data, you need to sort items by item ID, but we do not know how many items have a particular ID number. For this, you are implementing Hash partitioning to perform this action. Now, if the PowerCenter Integration Service uses all grouped or sorted ports as a compound partition key, then which of the following types of hash partitioning techniques you must implement in this scenario?</p>","a":[{"id":1754579,"option":"Hash user keys partitioning","correct":false},{"id":1754580,"option":"Hash auto keys partitioning","correct":true},{"id":1754581,"option":"Hash domain keys partitioning","correct":false},{"id":1754582,"option":"Hash data keys partitioning","correct":false}]},{"q":"<p>In Informatica, the PowerCenter Integration Service distributes blocks of data to one or more partitions. If you require each partition process to be rows based on the number and size of the blocks, then which of the following types of partitions should be implemented to perform this action in this scenario?</p>","a":[{"id":1754575,"option":"Key range partitioning","correct":false},{"id":1754576,"option":"Pass-through partitioning","correct":false},{"id":1754577,"option":"Round robin partitioning","correct":true},{"id":1754578,"option":"Database partitioning","correct":false}]},{"q":"<p>In Informatica, the PowerCenter Integration Services creates a default partition type at each partition point. Which of the following statements about Informatical Partitions are correct:</p>\n\n<p><strong>Statements</strong></p>\n\n<ol>\n\t<li>We cannot create a partition key for round-robin, hash auto-keys, and pass-through partition.</li>\n\t<li>When we use a pass-through partition, then Informatica tries to make multiple connection requests to the database server. </li>\n\t<li>We cannot use the native database options as partition alternatives to increase the degree of parallelism of query processing.</li>\n\t<li>If we have a bitmap index upon the target and by using the pass-through partition, then we need to update the target table. </li>\n</ol>","a":[{"id":1754571,"option":"1, 2, and 3","correct":false},{"id":1754572,"option":"2, 3, and 4","correct":false},{"id":1754573,"option":"1, 2, and 4","correct":true},{"id":1754574,"option":"1, 3, and 4","correct":false}]}]